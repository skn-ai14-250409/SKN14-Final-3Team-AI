# -*- coding: utf-8 -*-
"""batchAPI_KB_QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQWoiPAFJ0UUzrlQD4VIFqrDsaruDYIt
"""

# (ì£¼ì˜) Colab ì „ìš© ì„¤ì¹˜ ëª…ë ¹ ì œê±°ë¨. íŒ¨í‚¤ì§€ëŠ” í™˜ê²½ì— ë¯¸ë¦¬ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

import os
import json
import jsonlines
import getpass
import tiktoken
from openai import OpenAI
# Colab ì „ìš© ëª¨ë“ˆ ì œê±°: .py ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” ë¶ˆí•„ìš”
import pdfplumber
from datetime import datetime

# OpenAI API í‚¤ë¥¼ ì•ˆì „í•˜ê²Œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key: ")

# OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

print("âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° API í‚¤ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

"""--- ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ ---

`{'pdf': <pdfplumber.pdf.PDF object at 0x7b38f0a31730>, 'label': 0}`
Hugging Face datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ë©´ì„œ PDF íŒŒì¼ì„ ì—´ì–´ì£¼ëŠ” ì²« ë‹¨ê³„ë¥¼ ë¯¸ë¦¬ ì²˜ë¦¬í•´ì¤€ ìƒíƒœ
"""

from datasets import load_dataset

# Hugging Face Hubì—ì„œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
# ë§Œì•½ ë°ì´í„°ì…‹ì´ ë¹„ê³µê°œ(private)ë¼ë©´, Hugging Face ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ê·¸ëŸ´ ê²½ìš°, from huggingface_hub import notebook_login; notebook_login() ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.
try:
    dataset = load_dataset("sumilee/SKN14-Final-3Team-Data")
    print("âœ… ë°ì´í„°ì…‹ ë¡œë“œì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤!")
    print("\n--- ë°ì´í„°ì…‹ ì •ë³´ ---")
    print(dataset)

    # ë°ì´í„°ì…‹ì˜ ì²« ë²ˆì§¸ ìƒ˜í”Œì„ í™•ì¸í•˜ì—¬ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    print("\n--- ì²« ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ ---")
    # 'train' ìŠ¤í”Œë¦¿ì´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ë§Œì•½ ìŠ¤í”Œë¦¿ ì´ë¦„ì´ ë‹¤ë¥´ë‹¤ë©´ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
    sample = dataset['train'][0]
    print(sample)

except Exception as e:
    print(f"ğŸš¨ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    print("\në°ì´í„°ì…‹ ì´ë¦„ì´ ì •í™•í•œì§€, ê³µê°œ ìƒíƒœì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    print("ë¹„ê³µê°œ ë°ì´í„°ì…‹ì˜ ê²½ìš°, Colab í™˜ê²½ì— Hugging Face ë¡œê·¸ì¸ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

from tqdm.auto import tqdm

def extract_text_from_pdf_object(pdf_obj):
    """pdfplumber.pdf.PDF ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    full_text = ""
    # .pagesëŠ” pdf ê°ì²´ì˜ ëª¨ë“  í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    for page in pdf_obj.pages:
        # ê° í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ full_textì— ì¶”ê°€í•©ë‹ˆë‹¤.
        page_text = page.extract_text()
        if page_text:
            full_text += page_text + "\n"
    return full_text

def chunk_text(text, chunk_size=2000, chunk_overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ í† í° ê¸°ì¤€ìœ¼ë¡œ ì§€ì •ëœ í¬ê¸°ì™€ ê²¹ì¹¨ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    # gpt-4, gpt-3.5-turbo, gpt-4o ë“± ìµœì‹  ëª¨ë¸ì— ì‚¬ìš©ë˜ëŠ” ì¸ì½”ë”ì…ë‹ˆë‹¤.
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)

    chunks = []
    # (chunk_size - chunk_overlap) ë§Œí¼ ê±´ë„ˆë›°ë©´ì„œ í† í°ì„ ìë¦…ë‹ˆë‹¤.
    for i in range(0, len(tokens), chunk_size - chunk_overlap):
        chunk_tokens = tokens[i:i + chunk_size]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)
    return chunks

# 1. ëª¨ë“  PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ì²­í‚¹í•˜ì—¬ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
all_text_chunks = []
print(f"ğŸ“„ ì´ {len(dataset['train'])}ê°œì˜ PDF íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

# tqdmì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ ìƒí™©ì„ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.
for item in tqdm(dataset['train'], desc="PDF ì²˜ë¦¬ ì¤‘"):
    try:
        pdf_obj = item['pdf']
        full_text = extract_text_from_pdf_object(pdf_obj)
        text_chunks = chunk_text(full_text)
        all_text_chunks.extend(text_chunks)
    except Exception as e:
        # ì˜¤ë¥˜ê°€ ë°œìƒí•œ PDFëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.
        print(f"â—ï¸ íŠ¹ì • PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê±´ë„ˆëœë‹ˆë‹¤): {e}")

print(f"âœ… ëª¨ë“  PDF ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(all_text_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")


# 2. ìƒì„±ëœ ëª¨ë“  ì²­í¬ì— ëŒ€í•œ Batch API ì…ë ¥ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
batch_input_filename = "batch_input_all_pdfs.jsonl"
system_prompt = """
You are a helpful assistant specialized in finance for KB Financial Group.
Your task is to generate a high-quality question and a detailed answer based *only* on the provided context.
The question should be what a KB employee or a financial expert might ask.
The output format must be a single JSON object with two keys: "question" and "answer".
The entire response, including keys and values, must be in Korean.
"""
# ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì´ ì¢‹ì€ ìµœì‹  ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
model_name = "gpt-4o-mini"

print(f"\nğŸ“ ì´ {len(all_text_chunks)}ê°œì˜ ì²­í¬ì— ëŒ€í•œ Batch API ìš”ì²­ íŒŒì¼ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

with jsonlines.open(batch_input_filename, mode='w') as writer:
    for i, chunk in enumerate(all_text_chunks):
        json_request = {
            "custom_id": f"request_{i+1}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": model_name,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Context:\n---\n{chunk}\n---\nPlease generate a question and answer pair based on the context above."}
                ],
                "response_format": {"type": "json_object"},
                "temperature": 0.5,
                "max_tokens": 1024
            }
        }
        writer.write(json_request)

print(f"âœ… Batch API ì…ë ¥ íŒŒì¼ '{batch_input_filename}'ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 1. íŒŒì¼ ì—…ë¡œë“œ
# 'batch_qa_generation_input.jsonl' íŒŒì¼ì„ 'ì½ê¸°(rb)' ëª¨ë“œë¡œ ì—´ì–´ OpenAIì— ì „ì†¡í•©ë‹ˆë‹¤.
print("ğŸ“‚ Batch íŒŒì¼ì„ OpenAIì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
batch_file = client.files.create(
  file=open(batch_input_filename, "rb"),
  purpose="batch"
)
print(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ! (File ID: {batch_file.id})")


# 2. Batch Job ìƒì„±
# ìœ„ì—ì„œ ì—…ë¡œë“œí•œ íŒŒì¼ì˜ IDë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìƒì„±í•©ë‹ˆë‹¤.
print("\nğŸš€ Batch Job ìƒì„±ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
batch_job = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    # 24ì‹œê°„ ì•ˆì— ì‘ì—…ì„ ì™„ë£Œí•´ë‹¬ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
    completion_window="24h",
    metadata={
      "description": "sLLM QA Generation for KB Financial Group"
    }
)
print(f"âœ… Batch Job ìƒì„± ì™„ë£Œ! (Job ID: {batch_job.id})")
print("\n---")
print("â„¹ï¸ ì´ì œ OpenAI ì„œë²„ì—ì„œ ì‘ì—…ì´ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
print("ì‘ì—…ì´ ì™„ë£Œë˜ê¸°ê¹Œì§€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤.")

import json

try:
    # Cell 5ì—ì„œ ìƒì„±ëœ batch_job ë³€ìˆ˜ì˜ IDë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    job_id = batch_job.id
    retrieved_job = client.batches.retrieve(job_id)
    print(f"ğŸ”„ Batch Job (ID: {job_id})ì˜ í˜„ì¬ ìƒíƒœ: {retrieved_job.status}")

    # ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    if retrieved_job.status == 'completed':
        print("\nâœ… Batch Jobì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê²°ê³¼ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤...")

        output_file_id = retrieved_job.output_file_id
        result_content_bytes = client.files.content(output_file_id).read()
        results_str = result_content_bytes.decode('utf-8')

        final_results = []
        for line in results_str.strip().split('\n'):
            try:
                response_data = json.loads(line)
                content_str = response_data['response']['body']['choices'][0]['message']['content']
                qa_pair = json.loads(content_str)

                # Unsloth ìŠ¤í¬ë¦½íŠ¸ì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
                final_results.append({
                    "question": qa_pair.get("question", "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨"),
                    "answer_B": qa_pair.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")
                })
            except (json.JSONDecodeError, KeyError, IndexError) as e:
                # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë¼ì¸ì€ ê±´ë„ˆëœë‹ˆë‹¤.
                print(f"â—ï¸ê²°ê³¼ ë¼ì¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ê±´ë„ˆëœë‹ˆë‹¤): {e}")

        if final_results:
            print(f"\n--- âœ¨ ì´ {len(final_results)}ê°œì˜ QA ë°ì´í„° ìƒì„± ì™„ë£Œ! âœ¨ ---")
            print("--- ìµœì¢… ë°ì´í„° ìƒ˜í”Œ ---")
            print(json.dumps(final_results[0], indent=2, ensure_ascii=False))

            final_dataset_filename = 'kb_sllm_qa_dataset.jsonl'
            with open(final_dataset_filename, 'w', encoding='utf-8') as f:
                for item in final_results:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            print(f"\nâœ… ìµœì¢… ë°ì´í„°ì…‹ì´ '{final_dataset_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    elif retrieved_job.status in ['failed', 'expired', 'cancelled']:
        print(f"âŒ ì‘ì—…ì´ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ìƒíƒœ: {retrieved_job.status})")
    else:
        # ì•„ì§ ì‘ì—…ì´ ì§„í–‰ ì¤‘ì¸ ê²½ìš°
        print("â³ ì‘ì—…ì´ ì•„ì§ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì—¬ í™•ì¸í•´ì£¼ì„¸ìš”.")

except NameError:
    print("â—ï¸'batch_job' ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ì „ ë‹¨ê³„ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
except Exception as e:
    print(f"ğŸš¨ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")



import json

file_path = "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl"

print(f"--- íŒŒì¼ '{file_path}' ë‚´ìš© (ì²˜ìŒ 10ì¤„) ---")
try:
    with open(file_path, 'r', encoding='utf-8') as f:
        # íŒŒì¼ì˜ ì²˜ìŒ 10ì¤„ë§Œ ì½ì–´ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.
        for i, line in enumerate(f):
            if i >= 10:
                break
            try:
                # ê° ì¤„ì€ JSON í˜•ì‹ì´ë¯€ë¡œ íŒŒì‹±í•˜ì—¬ ì˜ˆì˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
                json_obj = json.loads(line)
                print(json.dumps(json_obj, indent=2, ensure_ascii=False))
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {e} - ì¤„ ë‚´ìš©: {line.strip()}")
            print("-" * 20) # ì¤„ êµ¬ë¶„ì„ ìœ„í•œ êµ¬ë¶„ì„ 
except FileNotFoundError:
    print(f"ğŸš¨ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {file_path}")
except Exception as e:
    print(f"ğŸš¨ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")



file_path = "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl"

try:
    with open(file_path, 'r', encoding='utf-8') as f:
        line_count = sum(1 for line in f)

    print(f"--- íŒŒì¼ '{file_path}' ì •ë³´ ---")
    print(f"âœ… ì´ ì¤„ ìˆ˜: {line_count} ì¤„")

except FileNotFoundError:
    print(f"ğŸš¨ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {file_path}")
except Exception as e:
    print(f"ğŸš¨ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

"""# Task
Create a Q&A dataset from the file "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl" and upload it to Hugging Face.

## ë°ì´í„° ë¡œë“œ ë° q&a ì¶”ì¶œ

### Subtask:
Batch API ê²°ê³¼ íŒŒì¼(`batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl`)ì„ ì½ì–´ì™€ ê° ì¤„ì—ì„œ ì§ˆë¬¸(`question`)ê³¼ ë‹µë³€(`answer`)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì¶”ì¶œëœ ë°ì´í„°ëŠ” Hugging Face `Dataset` ê°ì²´ ìƒì„±ì— ì í•©í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤.

**Reasoning**:
Read the batch output file, extract the question and answer from each line, format them as a list of dictionaries with 'question' and 'answer_B' keys, and store them in the `qa_pairs` list.
"""

import json

file_path = "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl"
qa_pairs = []

print(f"--- íŒŒì¼ '{file_path}'ì—ì„œ QA ë°ì´í„° ì¶”ì¶œ ì‹œì‘ ---")
try:
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            try:
                # ê° ì¤„ì€ JSON í˜•ì‹ì´ë¯€ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
                response_data = json.loads(line)

                # response -> body -> choices[0] -> message -> content ê²½ë¡œì˜ ë¬¸ìì—´ ì¶”ì¶œ
                content_str = response_data.get('response', {}).get('body', {}).get('choices', [{}])[0].get('message', {}).get('content')

                if content_str:
                    # content_strì´ ë‹¤ì‹œ JSON í˜•ì‹ìœ¼ë¡œ ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.
                    qa_pair = json.loads(content_str)

                    # ì§ˆë¬¸ê³¼ ë‹µë³€ ì¶”ì¶œ ë° í¬ë§· ë³€ê²½
                    question = qa_pair.get("question", "ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨")
                    answer = qa_pair.get("answer", "ë‹µë³€ ìƒì„± ì‹¤íŒ¨")

                    # Unsloth í˜•ì‹ì— ë§ê²Œ answer_B í‚¤ ì‚¬ìš©
                    qa_pairs.append({
                        "question": question,
                        "answer_B": answer
                    })
                else:
                    print(f"âš ï¸ ì¤„ {i+1}: 'content' í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•´ë‹¹ ì¤„ ê±´ë„ˆëœë‹ˆë‹¤.")

            except json.JSONDecodeError as e:
                print(f"âš ï¸ ì¤„ {i+1}: JSON íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {e} - ì¤„ ë‚´ìš©: {line.strip()} - í•´ë‹¹ ì¤„ ê±´ë„ˆëœë‹ˆë‹¤.")
            except KeyError as e:
                print(f"âš ï¸ ì¤„ {i+1}: ì˜ˆìƒì¹˜ ëª»í•œ í‚¤ ì˜¤ë¥˜ ë°œìƒ: {e} - í•´ë‹¹ ì¤„ ê±´ë„ˆëœë‹ˆë‹¤.")
            except IndexError as e:
                print(f"âš ï¸ ì¤„ {i+1}: ì˜ˆìƒì¹˜ ëª»í•œ ì¸ë±ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {e} - í•´ë‹¹ ì¤„ ê±´ë„ˆëœë‹ˆë‹¤.")
            except Exception as e:
                 print(f"âš ï¸ ì¤„ {i+1}: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e} - í•´ë‹¹ ì¤„ ê±´ë„ˆëœë‹ˆë‹¤.")


    print(f"\nâœ… ë°ì´í„° ì¶”ì¶œ ì™„ë£Œ! ì´ {len(qa_pairs)}ê°œì˜ QA ìŒì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

except FileNotFoundError:
    print(f"ğŸš¨ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {file_path}")
except Exception as e:
    print(f"ğŸš¨ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

"""## Hugging face ë°ì´í„°ì…‹ ìƒì„±

### Subtask:
ì¶”ì¶œëœ Q&A ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `Dataset` ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**Reasoning**:
Use the extracted Q&A list to create a Dataset object using the datasets library.
"""

from datasets import Dataset

# Create a Dataset object from the list of QA pairs
qa_dataset = Dataset.from_list(qa_pairs)

# Print information about the created dataset
print("--- ìƒì„±ëœ ë°ì´í„°ì…‹ ì •ë³´ ---")
print(qa_dataset)

print("\n--- ë°ì´í„°ì…‹ ì²« ë²ˆì§¸ ìƒ˜í”Œ ---")
print(qa_dataset[0])

"""## ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì¤€ë¹„

### Subtask:
Hugging Face Hubì— ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•˜ê¸° ìœ„í•´ ì¸ì¦ ì ˆì°¨ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**Reasoning**:
Authenticate with Hugging Face Hub to upload the dataset.
"""

from huggingface_hub import notebook_login

notebook_login()

"""## ë°ì´í„°ì…‹ ì—…ë¡œë“œ

### Subtask:
ìƒì„±ëœ `Dataset` ê°ì²´ë¥¼ Hugging Face Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ ì´ë¦„ê³¼ ê³µê°œ/ë¹„ê³µê°œ ì„¤ì •ì„ ì§€ì •í•©ë‹ˆë‹¤.

**Reasoning**:
Upload the created Dataset object to the Hugging Face Hub as instructed.

## ë°ì´í„°ì…‹ ì—…ë¡œë“œ

### Subtask:
ìƒì„±ëœ `Dataset` ê°ì²´ë¥¼ Hugging Face Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤. ë°ì´í„°ì…‹ ì´ë¦„ê³¼ ê³µê°œ/ë¹„ê³µê°œ ì„¤ì •ì„ ì§€ì •í•©ë‹ˆë‹¤.
"""

# Replace "your-username" with your actual Hugging Face username
repo_name = "sssssungjae/kb-sllm-qa-dataset"

print(f"Uploading dataset to Hugging Face Hub: {repo_name}")

# Use the push_to_hub method to upload the dataset
# Ensure you have authenticated with a token that has write access to "your-username"
qa_dataset.push_to_hub(
    repo_name,
    private=True, # Set to False if you want the dataset to be public
    commit_description="Initial upload of KB Financial Group QA dataset generated by sLLM"
)

print("âœ… Dataset upload initiated. Check your Hugging Face profile for status.")
print(f"Please replace 'your-username' in the code with your actual Hugging Face username and re-run this cell.")
