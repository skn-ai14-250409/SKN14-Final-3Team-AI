# pip install unsloth
# pip install transformers==4.55.4
# pip install wandb
# export HF_TOKEN="ìì‹ ì˜ API ë„£ê¸°!"
# export WANDB_API_KEY="ìì‹ ì˜ API ë„£ê¸°!"
# train_full_model.py

import os
import torch
from datasets import load_dataset
from unsloth import FastLanguageModel
from trl import SFTTrainer, SFTConfig
from huggingface_hub import login
from transformers import AutoTokenizer

# ===================================================================================
# 1. ì„¤ì • (CONFIGURATION)
# ===================================================================================
# Hugging Face Hub ì‚¬ìš©ì ì´ë¦„ ë˜ëŠ” ì¡°ì§ ì´ë¦„
HF_USERNAME = "sssssungjae" # <--- ğŸ™‹â€â™‚ï¸ ì—¬ê¸°ì— ë³¸ì¸ì˜ Hugging Face IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.

# ğŸŒŸ WandB ì„¤ì •
WANDB_PROJECT_NAME = "qwen2.5-7b-finance-full-finetune" # <--- ğŸ™‹â€â™‚ï¸ WandBì— í‘œì‹œë  í”„ë¡œì íŠ¸ ì´ë¦„

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •
BASE_MODEL_NAME = "unsloth/Qwen2.5-7B" # <--- ğŸŒŸ ëª¨ë¸ ë³€ê²½
MAX_SEQ_LENGTH = 1024

# ğŸš« LoRA ì„¤ì •ì€ Full-Finetuning ì‹œ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤.

# ë°ì´í„°ì…‹ ì„¤ì •
DATASET_REPO_NAME = "sssssungjae/finance-kb-mixed-dataset-final"
DATASET_SPLIT = "train"
DATASET_TEXT_FIELD = "text"  # ì‹¤ì œ ì»¬ëŸ¼ëª…ê³¼ ë‹¤ë¥´ë©´ ìˆ˜ì •í•˜ì„¸ìš”.

# í‰ê°€ ë°ì´í„°ì…‹ ì„¤ì • (ì˜µì…˜)
# í™˜ê²½ë³€ìˆ˜ë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥: EVAL_DATASET_REPO, EVAL_DATASET_SPLIT
EVAL_DATASET_REPO_NAME = os.environ.get("EVAL_DATASET_REPO", "")
EVAL_DATASET_SPLIT = os.environ.get("EVAL_DATASET_SPLIT", "validation")

# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
TRAINING_EPOCHS = 1
BATCH_SIZE = 8
GRADIENT_ACCUMULATION = 2
LEARNING_RATE = 2e-5
# H100ì—ì„œëŠ” 8-bit ì˜µí‹°ë§ˆì´ì €ë³´ë‹¤ fused AdamWê°€ ì¼ë°˜ì ìœ¼ë¡œ ë” ë¹ ë¦…ë‹ˆë‹¤.
OPTIMIZER = "adamw_torch_fused"

# ì €ì¥ ë° ì—…ë¡œë“œ ì„¤ì •
LOCAL_FULL_MODEL_PATH = "full_model"
LOCAL_GGUF_PATH = "model_gguf"
HUB_FULL_REPO = f"{HF_USERNAME}/qwen2_5-7b-finance-full"
HUB_GGUF_REPO = f"{HF_USERNAME}/qwen2_5-7b-finance-full-gguf"

# ===================================================================================
# 2. ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ (í•„ìš” ì‹œ í™•ì¥ ê°€ëŠ¥)
# ===================================================================================


# ===================================================================================
# 3. ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
# ===================================================================================
def train_model(train_dataset, tokenizer, eval_dataset=None):
    """Full-Finetuningìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. (H100 ìµœì í™”)"""
    print("\n step 2: Full-Finetuning ëª¨ë¸ ë¡œë”© ë° í•™ìŠµ ì‹œì‘...")

    # ğŸŒŸ Full-Finetuningìœ¼ë¡œ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, _ = FastLanguageModel.from_pretrained(
        model_name=BASE_MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH,
        load_in_4bit=False,
        full_finetuning=True,  # <--- ğŸŒŸ Full-Finetuning í™œì„±í™”
        torch_dtype=torch.bfloat16,
    )

    # í•™ìŠµ ìµœì í™” ì„¤ì • (H100)
    torch.backends.cuda.matmul.allow_tf32 = True
    if hasattr(torch, "set_float32_matmul_precision"):
        torch.set_float32_matmul_precision("high")

    # special tokens ì¶”ê°€ ì‹œ ì„ë² ë”© í¬ê¸° ì¬ì¡°ì • ë° ìºì‹œ ë¹„í™œì„±í™”
    if hasattr(tokenizer, "vocab") or hasattr(tokenizer, "get_vocab"):
        try:
            model.resize_token_embeddings(len(tokenizer))
        except Exception:
            pass
    if hasattr(model, "config"):
        model.config.use_cache = False

    # ğŸš« LoRA ì–´ëŒ‘í„°ë¥¼ ì¶”ê°€í•˜ëŠ” get_peft_model í•¨ìˆ˜ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

    # SFTTrainer ì„¤ì •
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,  # ì˜µì…˜: í‰ê°€ ë°ì´í„°ì…‹
        # ì‚¬ì „ í† í¬ë‚˜ì´ì¦ˆëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ í•„ë“œ ì§€ì • ë¶ˆí•„ìš”
        dataset_text_field=None,
        max_seq_length=MAX_SEQ_LENGTH,
        args=SFTConfig(
            per_device_train_batch_size=BATCH_SIZE,
            gradient_accumulation_steps=GRADIENT_ACCUMULATION,
            warmup_steps=100,
            num_train_epochs=TRAINING_EPOCHS,
            learning_rate=LEARNING_RATE,
            logging_steps=10,
            optim=OPTIMIZER,
            weight_decay=0.01,
            lr_scheduler_type="cosine",
            seed=3407,
            output_dir="outputs",
            report_to="wandb",
            bf16=True,
            fp16=False,
            tf32=True,
            gradient_checkpointing=True,
            gradient_checkpointing_kwargs={"use_reentrant": False},
            dataloader_num_workers=4,
            # í‰ê°€/ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
            eval_strategy=("steps" if eval_dataset is not None else "no"),
            eval_steps=100,
            save_strategy="steps",
            save_steps=100,
            load_best_model_at_end=(eval_dataset is not None),
            metric_for_best_model="eval_loss",
            greater_is_better=False,
        ),
    )

    # ğŸŒŸ WandB í”„ë¡œì íŠ¸ ì´ë¦„ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["WANDB_PROJECT"] = WANDB_PROJECT_NAME

    # í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")
    return model, tokenizer

# ===================================================================================
# 4. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ í•¨ìˆ˜
# ===================================================================================
def save_and_upload_models(model, tokenizer):
    """í•™ìŠµëœ ì „ì²´ ëª¨ë¸ì„ ì €ì¥í•˜ê³  Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    print("\n step 3: ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ ì‹œì‘...")

    try:
        login(token=os.environ.get("HF_TOKEN"))
    except Exception as e:
        print("Hugging Face ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. export HF_TOKEN='ë‚´í† í°' ëª…ë ¹ì–´ë¡œ í† í°ì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print(f"ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
        return

    # 1. Full-Finetuningëœ ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ (16ë¹„íŠ¸)
    print("\n(1/2) Full-Finetuning ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ ì¤‘...")
    model.save_pretrained(LOCAL_FULL_MODEL_PATH) # ë¡œì»¬ì— ì €ì¥
    tokenizer.save_pretrained(LOCAL_FULL_MODEL_PATH)
    model.push_to_hub(HUB_FULL_REPO, token=True) # Hubì— ì—…ë¡œë“œ
    tokenizer.push_to_hub(HUB_FULL_REPO, token=True)
    print(f"âœ… ì „ì²´ ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ: {HUB_FULL_REPO}")

    # 2. GGUF ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ (ëª¨ë¸/ë²„ì „ì— ë”°ë¼ ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ)
    try:
        print("\n(2/2) GGUF ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ ì¤‘...")
        model.save_pretrained_gguf(
            LOCAL_GGUF_PATH,
            tokenizer,
            quantization_method=["q4_k_m"],
        )
        model.push_to_hub_gguf(
            HUB_GGUF_REPO,
            tokenizer,
            quantization_method=["q4_k_m"],
            token=True,
        )
        print(f"âœ… GGUF ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ: {HUB_GGUF_REPO}")
    except Exception as e:
        print(f"âš ï¸ GGUF ë‚´ë³´ë‚´ê¸°/ì—…ë¡œë“œê°€ ì§€ì›ë˜ì§€ ì•Šê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

# ===================================================================================
# 5. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ===================================================================================
if __name__ == "__main__":
    # WandB í”„ë¡œì íŠ¸ ì´ë¦„ ë¨¼ì € ì„¤ì •
    os.environ["WANDB_PROJECT"] = WANDB_PROJECT_NAME

    # í† í¬ë‚˜ì´ì €: Instruct í…œí”Œë¦¿ì„ Base í† í¬ë‚˜ì´ì €ì— ì´ì‹
    print("  step 0: ë² ì´ìŠ¤ ëª¨ë¸ í† í¬ë‚˜ì´ì €ì— ì±„íŒ… í…œí”Œë¦¿ ì„¤ì • ì¤‘...")
    instruct_tokenizer = AutoTokenizer.from_pretrained("unsloth/Qwen2.5-7B-Instruct")
    base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)
    base_tokenizer.chat_template = instruct_tokenizer.chat_template
    base_tokenizer.add_special_tokens(
        instruct_tokenizer.special_tokens_map,
        replace_additional_special_tokens=True,
    )
    print("âœ… ì±„íŒ… í…œí”Œë¦¿ ì„¤ì • ì™„ë£Œ.")
    
    # 1. ë°ì´í„° ì¤€ë¹„ (train + optional eval)
    print("\n  step 1: í—ˆê¹…í˜ì´ìŠ¤ Hubì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ë¡œë”©...")
    final_dataset = load_dataset(DATASET_REPO_NAME, split=DATASET_SPLIT)
    if DATASET_TEXT_FIELD not in final_dataset.column_names:
        raise ValueError(
            f"ë°ì´í„°ì…‹ì— '{DATASET_TEXT_FIELD}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {final_dataset.column_names}"
        )
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ. í¬ê¸°: {len(final_dataset)}")

    # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë”© (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë™ì¼ ì €ì¥ì†Œì˜ eval/validation ì‹œë„)
    eval_dataset = None
    if EVAL_DATASET_REPO_NAME:
        try:
            print("  step 1b: í‰ê°€ ë°ì´í„°ì…‹ ë¡œë”©...")
            eval_dataset = load_dataset(EVAL_DATASET_REPO_NAME, split=EVAL_DATASET_SPLIT)
            if DATASET_TEXT_FIELD not in eval_dataset.column_names:
                raise ValueError(
                    f"í‰ê°€ ë°ì´í„°ì…‹ì— '{DATASET_TEXT_FIELD}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {eval_dataset.column_names}"
                )
            print(f"âœ… í‰ê°€ ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ. í¬ê¸°: {len(eval_dataset)}")
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨, í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤: {e}")
    else:
        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ë‹¤ë©´ ë™ì¼ ì €ì¥ì†Œì—ì„œ ê³µí†µ eval ìŠ¤í”Œë¦¿ëª… ìˆœì°¨ ì‹œë„
        for cand_split in ("validation", "eval", "valid"):
            try:
                print(f"  step 1b: ë™ì¼ ì €ì¥ì†Œì—ì„œ í‰ê°€ ìŠ¤í”Œë¦¿('{cand_split}') ë¡œë”© ì‹œë„...")
                cand_eval = load_dataset(DATASET_REPO_NAME, split=cand_split)
                if DATASET_TEXT_FIELD not in cand_eval.column_names:
                    raise ValueError(
                        f"í‰ê°€ ë°ì´í„°ì…‹ì— '{DATASET_TEXT_FIELD}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. í˜„ì¬ ì»¬ëŸ¼: {cand_eval.column_names}"
                    )
                eval_dataset = cand_eval
                print(f"âœ… í‰ê°€ ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ. ìŠ¤í”Œë¦¿: '{cand_split}', í¬ê¸°: {len(eval_dataset)}")
                break
            except Exception as e:
                print(f"  â†ªï¸ '{cand_split}' ìŠ¤í”Œë¦¿ ë¡œë”© ì‹¤íŒ¨: {e}")
        if eval_dataset is None:
            print(" í‰ê°€ ìŠ¤í”Œë¦¿ì„ ì°¾ì§€ ëª»í•´ í‰ê°€ë¥¼ ìƒëµí•©ë‹ˆë‹¤. (í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì • ê°€ëŠ¥)")
    
    # 1c. ì‚¬ì „ í† í¬ë‚˜ì´ì¦ˆ: input_ids / attention_mask ìƒì„±
    print("  step 1c: ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì¦ˆ ì¤‘ (input_ids/attention_mask ìƒì„±)...")
    def _tokenize_batch(batch):
        texts = batch[DATASET_TEXT_FIELD]
        if isinstance(texts, str):
            texts = [texts]
        enc = base_tokenizer(
            texts,
            max_length=MAX_SEQ_LENGTH,
            truncation=True,
            padding=False,
            add_special_tokens=True,
            return_attention_mask=True,
        )
        return {"input_ids": enc["input_ids"], "attention_mask": enc["attention_mask"]}

    final_dataset = final_dataset.map(_tokenize_batch, batched=True, desc="Tokenizing train")
    if eval_dataset is not None:
        try:
            eval_dataset = eval_dataset.map(_tokenize_batch, batched=True, desc="Tokenizing eval")
        except Exception as e:
            print(f"âš ï¸ í‰ê°€ ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì¦ˆ ì¤‘ ì˜¤ë¥˜: {e}. í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            eval_dataset = None
    
    # 2. ëª¨ë¸ í•™ìŠµ (ì´í›„ ì½”ë“œëŠ” ë³€ê²½ ì—†ìŒ)
    trained_model, trained_tokenizer = train_model(
        train_dataset=final_dataset, tokenizer=base_tokenizer, eval_dataset=eval_dataset
    )
    
    # 3. ëª¨ë¸ ì €ì¥ ë° ì—…ë¡œë“œ
    save_and_upload_models(model=trained_model, tokenizer=trained_tokenizer)
    
    print("\nğŸ‰ ëª¨ë“  íŒŒì¸íŠœë‹ ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
