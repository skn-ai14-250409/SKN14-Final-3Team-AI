# train.py

# export HF_TOKEN=hf_your_hf_token
# export HF_USERNAME=your_hf_username
# export WANDB_API_KEY=your_wandb_api_key (optional, for wandb logging)

# pip install -r requirements.txt

# [1ë‹¨ê³„] í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸Œ-ëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig
from huggingface_hub import login
from liger_kernel.transformers import apply_liger_kernel_to_qwen2

def main():
    # --- 1. ë¡œê·¸ì¸ ---
    hf_token = os.environ.get('HF_TOKEN')
    if hf_token:
        login(token=hf_token)
        print("âœ… Hugging Face ë¡œê·¸ì¸ ì„±ê³µ!")
    else:
        raise ValueError("Hugging Face í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'export HF_TOKEN=...'ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

    # [2ë‹¨ê³„] ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_id = "Qwen/Qwen2.5-14B-Instruct"
    
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2"
    )
    print(f"--- ëª¨ë¸ '{model_id}' ë¡œë”© ì™„ë£Œ ---")

    # try: # RunPodì—ì„œ Liger ì„¤ì¹˜ í›„ ì£¼ì„ í•´ì œ
    #     apply_liger_kernel_to_qwen2()
    #     print("--- Liger-Kernel ì ìš© ì™„ë£Œ ---")
    # except ImportError:
    #     print("â—ï¸ Liger-Kernel ì ìš© ì‹¤íŒ¨. ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print("--- í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ ---")

    # [3ë‹¨ê³„] ë°ì´í„°ì…‹ ë¡œë“œ
    hf_username = os.environ.get('HF_USERNAME')
    if not hf_username:
        raise ValueError("HF_USERNAME í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
    
    dataset_repo_id = "rucipheryn/combined-dataset-30K-final"
    final_dataset = load_dataset(dataset_repo_id)
    print(f"--- ë°ì´í„°ì…‹ '{dataset_repo_id}' ë¡œë“œ ì™„ë£Œ ---")
    print(final_dataset)

    # [4ë‹¨ê³„] LoRA ì„¤ì •
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    lora_model = get_peft_model(model, lora_config)
    print("--- ëª¨ë¸ì— LoRA ì ìš© ì™„ë£Œ ---")
    lora_model.print_trainable_parameters()

    # [5ë‹¨ê³„] í›ˆë ¨ ê³„íš ìˆ˜ë¦½ (SFTConfig)
    
    report_to = "wandb" if os.environ.get('WANDB_API_KEY') else "none"
    if report_to == "wandb":
        os.environ['WANDB_PROJECT'] = "Qwen2.5_14B_LoRA_Finetuning"
        print("âœ… wandb ì—°ë™ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ğŸš€ ìµœì¢… ëª¨ë¸ì´ ì €ì¥ë  í—ˆê¹…í˜ì´ìŠ¤ ì €ì¥ì†Œ ì´ë¦„
    final_model_repo_id = f"{hf_username}/Qwen2.5-14B-KB-Finance-LoRA-ep2"

    sft_config = SFTConfig(
        dataset_text_field="text",
        max_seq_length=2048,
        output_dir=final_model_repo_id, # ë¡œì»¬ ì €ì¥ ê²½ë¡œì™€ í—ˆë¸Œ ì €ì¥ì†Œ ì´ë¦„ì„ ë™ì¼í•˜ê²Œ ì„¤ì •
        report_to=report_to,
        run_name=f"{final_model_repo_id}-run",
        num_train_epochs=2,
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        gradient_checkpointing=True,
        bf16=True,
        optim="paged_adamw_8bit",
        logging_strategy="steps",
        logging_steps=25,
        save_strategy="steps",
        save_steps=500,
        evaluation_strategy="steps",
        eval_steps=500,
        save_total_limit=2,
        push_to_hub=True, # í›ˆë ¨ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ í—ˆë¸Œì— ì—…ë¡œë“œ
    )

    # [6ë‹¨ê³„] íŠ¸ë ˆì´ë„ˆ ìƒì„±
    data_collator = DataCollatorForCompletionOnlyLM(
        tokenizer=tokenizer,
        instruction_template="<|im_start|>user",
        response_template="<|im_start|>assistant",
    )
    
    trainer = SFTTrainer(
        model=lora_model,
        tokenizer=tokenizer,
        args=sft_config,
        train_dataset=final_dataset["train"],
        eval_dataset=final_dataset["test"],
        data_collator=data_collator,
    )
    print("\n--- SFTTrainer ì¤€ë¹„ ì™„ë£Œ ---")

    # [7ë‹¨ê³„] í›ˆë ¨ ì‹œì‘
    print("\n--- íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    trainer.train()
    print("\n--- íŒŒì¸íŠœë‹ ì™„ë£Œ ---")

    # [8ë‹¨ê³„] ëª¨ë¸ ìµœì¢… ì €ì¥ ë° ì—…ë¡œë“œ
    # push_to_hub=True ì˜µì…˜ ë•ë¶„ì— trainer.train()ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ì—…ë¡œë“œëœë‹¤.
    print(f"\nâœ… ìµœì¢… ëª¨ë¸ì´ Hugging Face Hubì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"URL: https://huggingface.co/{final_model_repo_id}")


if __name__ == "__main__":
    main()