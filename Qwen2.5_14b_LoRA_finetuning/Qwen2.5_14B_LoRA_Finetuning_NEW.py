# í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì˜ˆì‹œ
# export HF_TOKEN=               # Hugging Face ì¸ì¦ í† í°(ì„ íƒ: CLI ë¡œê·¸ì¸ ì‹œ ìƒëµ ê°€ëŠ¥)
# export WANDB_API_KEY=       # wandb ë¡œê¹…ìš© API í‚¤(ì„ íƒ)
# export HF_USERNAME=rucipheryn                                       # Hugging Face ì‚¬ìš©ìëª…(ì„ íƒ: CLI ë¡œê·¸ì¸ ì‹œ ìƒëµ ê°€ëŠ¥)

import os
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM
from huggingface_hub import login
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

def setup_environment():
    """Hugging Face ë° wandb í™˜ê²½ì„ ì„¤ì •í•˜ê³  (hf_username, report_to)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    hf_token = os.environ.get("HF_TOKEN")
    wandb_api_key = os.environ.get("WANDB_API_KEY")

    # í† í°ì´ ìˆìœ¼ë©´ ë¡œê·¸ì¸, ì—†ìœ¼ë©´ ê¸°ì¡´ CLI ë¡œê·¸ì¸/ìºì‹œë¥¼ ì‚¬ìš©
    if hf_token:
        login(token=hf_token)
        print("Hugging Face ë¡œê·¸ì¸ ì„±ê³µ")
    else:
        print("HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•„, ê¸°ì¡´ CLI ë¡œê·¸ì¸/ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")

    # ìš”ì²­ì— ë”°ë¼ í•­ìƒ ì´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¡œ í‘¸ì‹œ
    hf_username = "rucipheryn"

    # wandb ì„¤ì •
    report_to = "none"
    if wandb_api_key:
        report_to = "wandb"
        os.environ["WANDB_PROJECT"] = "sk-networks-ai-camp-final"
        print("wandb ë¡œê¹… í™œì„±í™”")
    else:
        print("WANDB_API_KEY ë¯¸ì„¤ì •: wandb ë¡œê¹… ë¹„í™œì„±í™”(report_to=none)")

    return hf_username, report_to


def configure_h100_optimizations():
    """H100ì—ì„œ ê¶Œì¥ë˜ëŠ” ìˆ˜í•™ ì„¤ì •(TF32/ê³ ì •ë°€ matmul)ì„ í™œì„±í™”í•©ë‹ˆë‹¤."""
    try:
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = True
            if hasattr(torch, "set_float32_matmul_precision"):
                torch.set_float32_matmul_precision("high")
            print("H100/TF32 ìµœì í™” í™œì„±í™”")
        else:
            print("CUDA ë¯¸ì‚¬ìš©: H100 ìµœì í™” ìŠ¤í‚µ")
    except Exception as e:
        print(f"H100 ìµœì í™” ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")


def maybe_apply_liger():
    """í™˜ê²½ë³€ìˆ˜ USE_LIGER(1/true/yes/y)ì¼ ë•Œ Liger-Kernelì„ ì„ íƒì ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤."""
    flag = os.environ.get("USE_LIGER", "0").strip().lower()
    enabled = flag in ("1", "true", "yes", "y")
    if not enabled:
        print("USE_LIGER ë¹„í™œì„±: Liger-Kernel ì ìš© ìŠ¤í‚µ")
        return
    try:
        from liger_kernel.transformers import apply_liger_kernel_to_qwen2
        apply_liger_kernel_to_qwen2()
        print("Liger-Kernel ì ìš© ì™„ë£Œ")
    except Exception as e:
        print(f"Liger-Kernel ì ìš© ì‹¤íŒ¨: {e}")


def load_model_and_tokenizer(model_id: str):
    """ê¶Œì¥ ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        attn_implementation="sdpa",
        device_map="auto",
        use_cache=False,  # gradient_checkpointing=Trueì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë ¤ë©´ Falseë¡œ ì„¤ì •
    )
    print(f"--- ëª¨ë¸ '{model_id}' ë¡œë“œ ì™„ë£Œ ---")

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    print("--- í† í¬ë‚˜ì´ì € ì¤€ë¹„ ì™„ë£Œ ---")
    return model, tokenizer


def build_lora_config() -> LoraConfig:
    """Causal LMì„ ìœ„í•œ LoRA ì„¤ì •ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return LoraConfig(
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        task_type="CAUSAL_LM",
    )

def main():
    # ê¸°ë³¸ í™˜ê²½ ì„¤ì •
    hf_username, report_to = setup_environment()
    configure_h100_optimizations()
    MODEL_ID = "Qwen/Qwen2.5-14B-Instruct"
    DATASET_ID = f"{hf_username}/combined-dataset-30K-final-v2"
    OUTPUT_DIR = "Qwen2.5-14B-KB-Finance-LoRA-v2"
    FINAL_HUB_REPO_ID = f"{hf_username}/{OUTPUT_DIR}"

    # ëª¨ë¸/í† í¬ë‚˜ì´ì €/ë°ì´í„°ì…‹ ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(MODEL_ID)
    maybe_apply_liger()
    dataset = load_dataset(DATASET_ID)

    # --- ğŸ’¡ í•µì‹¬ ìˆ˜ì • ì‹œì‘ ğŸ’¡ ---
    # 1. ëª¨ë¸ ì¤€ë¹„: PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ ì‚¬ì „ ì²˜ë¦¬í•œë‹¤.
    #    (ì–‘ìí™”ë¥¼ ì•ˆ í•´ë„ ì´ í•¨ìˆ˜ë¥¼ ì“°ëŠ” ê²ƒì´ gradient_checkpointing ì•ˆì •ì„±ì— ë„ì›€ì´ ë¨)
    model = prepare_model_for_kbit_training(model)
    
    # 2. LoRA ì ìš©
    lora_model = get_peft_model(model, build_lora_config())
    # â›”ï¸ lora_model.train()ì€ ì—¬ê¸°ì„œ í˜¸ì¶œí•˜ì§€ ì•ŠëŠ”ë‹¤. Trainerê°€ ì•Œì•„ì„œ ì²˜ë¦¬í•˜ë„ë¡ ë‘”ë‹¤.
    # --- ğŸ’¡ í•µì‹¬ ìˆ˜ì • ë ğŸ’¡ ---

    print("\n--- LoRA ì–´ëŒ‘í„° ì ìš© ì™„ë£Œ ---")
    lora_model.print_trainable_parameters()

    # í•™ìŠµ ì„¤ì • êµ¬ì„±
    training_args = SFTConfig(
        output_dir=OUTPUT_DIR,
        hub_model_id=FINAL_HUB_REPO_ID,
        report_to=report_to,
        run_name=f"{OUTPUT_DIR}-run",
        num_train_epochs=2,
        learning_rate=2e-4,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        save_strategy="steps",
        save_steps=300,
        save_total_limit=2,
        logging_steps=25,
        evaluation_strategy="steps",
        eval_steps=300,
        optim="paged_adamw_8bit",
        bf16=True,
        max_seq_length=2048,
        lr_scheduler_type="cosine",
        warmup_ratio=0.03,
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        push_to_hub=True,
        hub_private_repo=False,
    )

    # íŠ¸ë ˆì´ë„ˆ êµ¬ì„±
    data_collator = DataCollatorForCompletionOnlyLM(
        tokenizer=tokenizer,
        instruction_template="<|im_start|>user",
        response_template="<|im_start|>assistant",
    )
    trainer = SFTTrainer(
        model=lora_model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        tokenizer=tokenizer,
        dataset_text_field="text",
        data_collator=data_collator,
    )
    print("\n--- SFTTrainer ì¤€ë¹„ ì™„ë£Œ ---")

    # í•™ìŠµ ì‹œì‘
    print("\n--- íŒŒì¸íŠœë‹ ì‹œì‘ ---")
    trainer.train()
    print("\n--- íŒŒì¸íŠœë‹ ì™„ë£Œ ---")
    print(f"\nëª¨ë¸ì´ Hugging Face Hubë¡œ í‘¸ì‹œë˜ì—ˆìŠµë‹ˆë‹¤: {FINAL_HUB_REPO_ID}")


if __name__ == "__main__":
    main()