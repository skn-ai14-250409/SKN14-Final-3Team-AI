# -*- coding: utf-8 -*-
"""batchAPI_KB_QA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQWoiPAFJ0UUzrlQD4VIFqrDsaruDYIt
"""

# (주의) Colab 전용 설치 명령 제거됨. 패키지는 환경에 미리 설치되어 있어야 합니다.

import os
import json
import jsonlines
import getpass
import tiktoken
from openai import OpenAI
# Colab 전용 모듈 제거: .py 스크립트에서는 불필요
import pdfplumber
from datetime import datetime

# OpenAI API 키를 안전하게 입력받습니다.
if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key: ")

# OpenAI 클라이언트를 초기화합니다.
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

print("✅ 라이브러리 임포트 및 API 키 설정이 완료되었습니다.")

"""--- 첫 번째 데이터 샘플 ---

`{'pdf': <pdfplumber.pdf.PDF object at 0x7b38f0a31730>, 'label': 0}`
Hugging Face datasets 라이브러리가 데이터를 로드하면서 PDF 파일을 열어주는 첫 단계를 미리 처리해준 상태
"""

from datasets import load_dataset

# Hugging Face Hub에서 데이터셋을 로드합니다.
# 만약 데이터셋이 비공개(private)라면, Hugging Face 로그인이 필요할 수 있습니다.
# 그럴 경우, from huggingface_hub import notebook_login; notebook_login() 코드를 먼저 실행해주세요.
try:
    dataset = load_dataset("sumilee/SKN14-Final-3Team-Data")
    print("✅ 데이터셋 로드에 성공했습니다!")
    print("\n--- 데이터셋 정보 ---")
    print(dataset)

    # 데이터셋의 첫 번째 샘플을 확인하여 구조를 파악합니다.
    print("\n--- 첫 번째 데이터 샘플 ---")
    # 'train' 스플릿이 있다고 가정합니다. 만약 스플릿 이름이 다르다면 수정이 필요합니다.
    sample = dataset['train'][0]
    print(sample)

except Exception as e:
    print(f"🚨 데이터셋 로드 중 오류가 발생했습니다: {e}")
    print("\n데이터셋 이름이 정확한지, 공개 상태인지 확인해주세요.")
    print("비공개 데이터셋의 경우, Colab 환경에 Hugging Face 로그인이 필요할 수 있습니다.")

from tqdm.auto import tqdm

def extract_text_from_pdf_object(pdf_obj):
    """pdfplumber.pdf.PDF 객체에서 텍스트 전체를 추출합니다."""
    full_text = ""
    # .pages는 pdf 객체의 모든 페이지 리스트입니다.
    for page in pdf_obj.pages:
        # 각 페이지에서 텍스트를 추출하여 full_text에 추가합니다.
        page_text = page.extract_text()
        if page_text:
            full_text += page_text + "\n"
    return full_text

def chunk_text(text, chunk_size=2000, chunk_overlap=200):
    """텍스트를 토큰 기준으로 지정된 크기와 겹침으로 나눕니다."""
    # gpt-4, gpt-3.5-turbo, gpt-4o 등 최신 모델에 사용되는 인코더입니다.
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)

    chunks = []
    # (chunk_size - chunk_overlap) 만큼 건너뛰면서 토큰을 자릅니다.
    for i in range(0, len(tokens), chunk_size - chunk_overlap):
        chunk_tokens = tokens[i:i + chunk_size]
        chunk_text = tokenizer.decode(chunk_tokens)
        chunks.append(chunk_text)
    return chunks

# 1. 모든 PDF에서 텍스트를 추출하고 청킹하여 하나의 리스트로 만듭니다.
all_text_chunks = []
print(f"📄 총 {len(dataset['train'])}개의 PDF 파일 처리를 시작합니다...")

# tqdm을 사용하여 진행 상황을 시각적으로 보여줍니다.
for item in tqdm(dataset['train'], desc="PDF 처리 중"):
    try:
        pdf_obj = item['pdf']
        full_text = extract_text_from_pdf_object(pdf_obj)
        text_chunks = chunk_text(full_text)
        all_text_chunks.extend(text_chunks)
    except Exception as e:
        # 오류가 발생한 PDF는 건너뛰고 계속 진행합니다.
        print(f"❗️ 특정 PDF 처리 중 오류 발생 (건너뜁니다): {e}")

print(f"✅ 모든 PDF 처리 완료! 총 {len(all_text_chunks)}개의 텍스트 청크가 생성되었습니다.")


# 2. 생성된 모든 청크에 대한 Batch API 입력 파일을 생성합니다.
batch_input_filename = "batch_input_all_pdfs.jsonl"
system_prompt = """
You are a helpful assistant specialized in finance for KB Financial Group.
Your task is to generate a high-quality question and a detailed answer based *only* on the provided context.
The question should be what a KB employee or a financial expert might ask.
The output format must be a single JSON object with two keys: "question" and "answer".
The entire response, including keys and values, must be in Korean.
"""
# 비용과 성능의 균형이 좋은 최신 모델을 사용합니다.
model_name = "gpt-4o-mini"

print(f"\n📝 총 {len(all_text_chunks)}개의 청크에 대한 Batch API 요청 파일 생성을 시작합니다...")

with jsonlines.open(batch_input_filename, mode='w') as writer:
    for i, chunk in enumerate(all_text_chunks):
        json_request = {
            "custom_id": f"request_{i+1}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": model_name,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Context:\n---\n{chunk}\n---\nPlease generate a question and answer pair based on the context above."}
                ],
                "response_format": {"type": "json_object"},
                "temperature": 0.5,
                "max_tokens": 1024
            }
        }
        writer.write(json_request)

print(f"✅ Batch API 입력 파일 '{batch_input_filename}'이 성공적으로 생성되었습니다.")

# 1. 파일 업로드
# 'batch_qa_generation_input.jsonl' 파일을 '읽기(rb)' 모드로 열어 OpenAI에 전송합니다.
print("📂 Batch 파일을 OpenAI에 업로드합니다...")
batch_file = client.files.create(
  file=open(batch_input_filename, "rb"),
  purpose="batch"
)
print(f"✅ 파일 업로드 완료! (File ID: {batch_file.id})")


# 2. Batch Job 생성
# 위에서 업로드한 파일의 ID를 사용하여 작업을 생성합니다.
print("\n🚀 Batch Job 생성을 요청합니다...")
batch_job = client.batches.create(
    input_file_id=batch_file.id,
    endpoint="/v1/chat/completions",
    # 24시간 안에 작업을 완료해달라는 의미입니다.
    completion_window="24h",
    metadata={
      "description": "sLLM QA Generation for KB Financial Group"
    }
)
print(f"✅ Batch Job 생성 완료! (Job ID: {batch_job.id})")
print("\n---")
print("ℹ️ 이제 OpenAI 서버에서 작업이 비동기적으로 처리됩니다.")
print("작업이 완료되기까지 몇 분 정도 소요될 수 있습니다. 잠시 후 다음 단계에서 결과를 확인해 보겠습니다.")

import json

try:
    # Cell 5에서 생성된 batch_job 변수의 ID를 사용합니다.
    job_id = batch_job.id
    retrieved_job = client.batches.retrieve(job_id)
    print(f"🔄 Batch Job (ID: {job_id})의 현재 상태: {retrieved_job.status}")

    # 작업이 완료되었는지 확인합니다.
    if retrieved_job.status == 'completed':
        print("\n✅ Batch Job이 성공적으로 완료되었습니다! 결과를 처리합니다...")

        output_file_id = retrieved_job.output_file_id
        result_content_bytes = client.files.content(output_file_id).read()
        results_str = result_content_bytes.decode('utf-8')

        final_results = []
        for line in results_str.strip().split('\n'):
            try:
                response_data = json.loads(line)
                content_str = response_data['response']['body']['choices'][0]['message']['content']
                qa_pair = json.loads(content_str)

                # Unsloth 스크립트에 바로 사용할 수 있는 형식으로 저장합니다.
                final_results.append({
                    "question": qa_pair.get("question", "질문 생성 실패"),
                    "answer_B": qa_pair.get("answer", "답변 생성 실패")
                })
            except (json.JSONDecodeError, KeyError, IndexError) as e:
                # 오류가 발생한 라인은 건너뜁니다.
                print(f"❗️결과 라인 처리 중 오류 발생 (건너뜁니다): {e}")

        if final_results:
            print(f"\n--- ✨ 총 {len(final_results)}개의 QA 데이터 생성 완료! ✨ ---")
            print("--- 최종 데이터 샘플 ---")
            print(json.dumps(final_results[0], indent=2, ensure_ascii=False))

            final_dataset_filename = 'kb_sllm_qa_dataset.jsonl'
            with open(final_dataset_filename, 'w', encoding='utf-8') as f:
                for item in final_results:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            print(f"\n✅ 최종 데이터셋이 '{final_dataset_filename}' 파일로 저장되었습니다.")

    elif retrieved_job.status in ['failed', 'expired', 'cancelled']:
        print(f"❌ 작업이 실패하거나 만료되었습니다. (상태: {retrieved_job.status})")
    else:
        # 아직 작업이 진행 중인 경우
        print("⏳ 작업이 아직 진행 중입니다. 잠시 후 이 셀을 다시 실행하여 확인해주세요.")

except NameError:
    print("❗️'batch_job' 변수를 찾을 수 없습니다. 이전 단계 코드를 먼저 실행해주세요.")
except Exception as e:
    print(f"🚨 예상치 못한 오류가 발생했습니다: {e}")



import json

file_path = "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl"

print(f"--- 파일 '{file_path}' 내용 (처음 10줄) ---")
try:
    with open(file_path, 'r', encoding='utf-8') as f:
        # 파일의 처음 10줄만 읽어서 출력합니다.
        for i, line in enumerate(f):
            if i >= 10:
                break
            try:
                # 각 줄은 JSON 형식이므로 파싱하여 예쁘게 출력합니다.
                json_obj = json.loads(line)
                print(json.dumps(json_obj, indent=2, ensure_ascii=False))
            except json.JSONDecodeError as e:
                print(f"⚠️ JSON 파싱 오류 발생: {e} - 줄 내용: {line.strip()}")
            print("-" * 20) # 줄 구분을 위한 구분선
except FileNotFoundError:
    print(f"🚨 오류: 파일을 찾을 수 없습니다. 경로를 확인해주세요: {file_path}")
except Exception as e:
    print(f"🚨 파일을 읽는 중 오류가 발생했습니다: {e}")



file_path = "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl"

try:
    with open(file_path, 'r', encoding='utf-8') as f:
        line_count = sum(1 for line in f)

    print(f"--- 파일 '{file_path}' 정보 ---")
    print(f"✅ 총 줄 수: {line_count} 줄")

except FileNotFoundError:
    print(f"🚨 오류: 파일을 찾을 수 없습니다. 경로를 확인해주세요: {file_path}")
except Exception as e:
    print(f"🚨 파일을 읽는 중 오류가 발생했습니다: {e}")

"""# Task
Create a Q&A dataset from the file "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl" and upload it to Hugging Face.

## 데이터 로드 및 q&a 추출

### Subtask:
Batch API 결과 파일(`batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl`)을 읽어와 각 줄에서 질문(`question`)과 답변(`answer`)을 추출합니다. 추출된 데이터는 Hugging Face `Dataset` 객체 생성에 적합한 리스트 형태로 준비합니다.

**Reasoning**:
Read the batch output file, extract the question and answer from each line, format them as a list of dictionaries with 'question' and 'answer_B' keys, and store them in the `qa_pairs` list.
"""

import json

file_path = "/content/batch_68b94c2d48d8819099c0c46a855b3587_output.jsonl"
qa_pairs = []

print(f"--- 파일 '{file_path}'에서 QA 데이터 추출 시작 ---")
try:
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            try:
                # 각 줄은 JSON 형식이므로 파싱합니다.
                response_data = json.loads(line)

                # response -> body -> choices[0] -> message -> content 경로의 문자열 추출
                content_str = response_data.get('response', {}).get('body', {}).get('choices', [{}])[0].get('message', {}).get('content')

                if content_str:
                    # content_str이 다시 JSON 형식으로 되어 있다고 가정하고 파싱합니다.
                    qa_pair = json.loads(content_str)

                    # 질문과 답변 추출 및 포맷 변경
                    question = qa_pair.get("question", "질문 생성 실패")
                    answer = qa_pair.get("answer", "답변 생성 실패")

                    # Unsloth 형식에 맞게 answer_B 키 사용
                    qa_pairs.append({
                        "question": question,
                        "answer_B": answer
                    })
                else:
                    print(f"⚠️ 줄 {i+1}: 'content' 필드를 찾을 수 없습니다. 해당 줄 건너뜁니다.")

            except json.JSONDecodeError as e:
                print(f"⚠️ 줄 {i+1}: JSON 파싱 오류 발생: {e} - 줄 내용: {line.strip()} - 해당 줄 건너뜁니다.")
            except KeyError as e:
                print(f"⚠️ 줄 {i+1}: 예상치 못한 키 오류 발생: {e} - 해당 줄 건너뜁니다.")
            except IndexError as e:
                print(f"⚠️ 줄 {i+1}: 예상치 못한 인덱스 오류 발생: {e} - 해당 줄 건너뜁니다.")
            except Exception as e:
                 print(f"⚠️ 줄 {i+1}: 예상치 못한 오류 발생: {e} - 해당 줄 건너뜁니다.")


    print(f"\n✅ 데이터 추출 완료! 총 {len(qa_pairs)}개의 QA 쌍을 추출했습니다.")

except FileNotFoundError:
    print(f"🚨 오류: 파일을 찾을 수 없습니다. 경로를 확인해주세요: {file_path}")
except Exception as e:
    print(f"🚨 파일을 읽는 중 오류가 발생했습니다: {e}")

"""## Hugging face 데이터셋 생성

### Subtask:
추출된 Q&A 리스트를 사용하여 `datasets` 라이브러리의 `Dataset` 객체를 생성합니다.

**Reasoning**:
Use the extracted Q&A list to create a Dataset object using the datasets library.
"""

from datasets import Dataset

# Create a Dataset object from the list of QA pairs
qa_dataset = Dataset.from_list(qa_pairs)

# Print information about the created dataset
print("--- 생성된 데이터셋 정보 ---")
print(qa_dataset)

print("\n--- 데이터셋 첫 번째 샘플 ---")
print(qa_dataset[0])

"""## 데이터셋 업로드 준비

### Subtask:
Hugging Face Hub에 데이터셋을 업로드하기 위해 인증 절차를 수행합니다.

**Reasoning**:
Authenticate with Hugging Face Hub to upload the dataset.
"""

from huggingface_hub import notebook_login

notebook_login()

"""## 데이터셋 업로드

### Subtask:
생성된 `Dataset` 객체를 Hugging Face Hub에 업로드합니다. 데이터셋 이름과 공개/비공개 설정을 지정합니다.

**Reasoning**:
Upload the created Dataset object to the Hugging Face Hub as instructed.

## 데이터셋 업로드

### Subtask:
생성된 `Dataset` 객체를 Hugging Face Hub에 업로드합니다. 데이터셋 이름과 공개/비공개 설정을 지정합니다.
"""

# Replace "your-username" with your actual Hugging Face username
repo_name = "sssssungjae/kb-sllm-qa-dataset"

print(f"Uploading dataset to Hugging Face Hub: {repo_name}")

# Use the push_to_hub method to upload the dataset
# Ensure you have authenticated with a token that has write access to "your-username"
qa_dataset.push_to_hub(
    repo_name,
    private=True, # Set to False if you want the dataset to be public
    commit_description="Initial upload of KB Financial Group QA dataset generated by sLLM"
)

print("✅ Dataset upload initiated. Check your Hugging Face profile for status.")
print(f"Please replace 'your-username' in the code with your actual Hugging Face username and re-run this cell.")
