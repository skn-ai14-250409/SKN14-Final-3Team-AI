# RAG 시스템 테스트 가이드

## 🧪 테스트 파일 개요

### 1. **comprehensive_rag_evaluator.py** (⭐ 메인 평가 도구)
**종합 RAG 성능 평가 시스템**
- **문서 검색 성능**: MRR, MAP, NDCG, Precision, Recall, F1
- **답변 생성 품질**: 정확성, 완전성, 관련성, 의미적 유사도
- **종합 성능 분석**: 등급 평가 및 개선 권장사항

```bash
# 종합 평가 실행
python tests/comprehensive_rag_evaluator.py
```

### 2. **test_rag_system.py** (기본 테스트)
**간단한 기능 테스트**
- 기본 검색 vs 카테고리별 검색 비교
- 응답 속도 측정
- 기본적인 결과 분석

```bash
# 기본 테스트 실행
python tests/test_rag_system.py
```

### 3. **quick_test.py** (빠른 검증)
**빠른 기능 확인**
- 서버 연결 테스트
- 간단한 질의응답 테스트
- 개발 중 빠른 검증용

```bash
# 빠른 테스트 실행
python tests/quick_test.py
```

### 4. **test_policy_dataset.py** (데이터셋 검증)
**강령 문서 기반 테스트**
- 특정 데이터셋 검증
- 예상 파일 발견 여부 확인
- 문서 검색 정확성 테스트

```bash
# 정책 데이터셋 테스트
python tests/test_policy_dataset.py
```

## 📊 평가 지표 설명

### 🔍 **문서 검색 성능 지표**

#### **Precision@K**
- **의미**: 상위 K개 검색 결과 중 관련 문서의 비율
- **계산**: (관련 문서 수) / K
- **범위**: 0.0 ~ 1.0 (높을수록 좋음)

#### **Recall@K** 
- **의미**: 전체 관련 문서 중 상위 K개에서 찾은 비율
- **계산**: (찾은 관련 문서 수) / (전체 관련 문서 수)
- **범위**: 0.0 ~ 1.0 (높을수록 좋음)

#### **F1@K**
- **의미**: Precision과 Recall의 조화평균
- **계산**: 2 × (Precision × Recall) / (Precision + Recall)
- **범위**: 0.0 ~ 1.0 (높을수록 좋음)

#### **MRR (Mean Reciprocal Rank)**
- **의미**: 첫 번째 관련 문서의 순위 역수 평균
- **계산**: 1 / (첫 관련 문서 순위)
- **범위**: 0.0 ~ 1.0 (높을수록 좋음)

#### **MAP (Mean Average Precision)**
- **의미**: 모든 관련 문서에 대한 Average Precision 평균
- **특징**: 순위를 고려한 정밀도 측정
- **범위**: 0.0 ~ 1.0 (높을수록 좋음)

#### **NDCG@K (Normalized Discounted Cumulative Gain)**
- **의미**: 순위를 고려한 정규화된 누적 이득
- **특징**: 상위 순위에 더 높은 가중치 부여
- **범위**: 0.0 ~ 1.0 (높을수록 좋음)

### 💬 **답변 생성 품질 지표**

#### **키워드 겹침 (Keyword Overlap)**
- **의미**: 예상 답변과 생성 답변 간 키워드 겹침 비율
- **계산**: (공통 키워드 수) / (예상 답변 키워드 수)

#### **의미적 유사도 (Semantic Similarity)**
- **의미**: 답변 간 의미적 유사성
- **계산**: Jaccard 유사도 기반

#### **완전성 (Completeness)**
- **의미**: 예상 답변의 구성 요소가 얼마나 포함되었는지
- **계산**: (포함된 구성 요소 수) / (전체 구성 요소 수)

#### **관련성 (Relevance)**
- **의미**: 질문과 답변 간 관련성
- **계산**: (질문-답변 공통 키워드) / (질문 키워드 수)

## 🎯 **사용법 가이드**

### **1. 전체 시스템 평가**
```bash
python tests/comprehensive_rag_evaluator.py
# 선택: 3. 전체 평가 (모든 케이스)
```

### **2. 빠른 성능 확인**
```bash
python tests/comprehensive_rag_evaluator.py
# 선택: 1. 빠른 평가 (5개 케이스)
```

### **3. 난이도별 평가**
```bash
python tests/comprehensive_rag_evaluator.py
# 선택: 4. 난이도별 평가
# 입력: easy/medium/hard
```

### **4. 개발 중 빠른 확인**
```bash
python tests/quick_test.py
```

## 📈 **성능 기준**

### **우수 (🌟)**
- 검색 성능: 0.8 이상
- 답변 품질: 0.8 이상

### **양호 (✅)**
- 검색 성능: 0.6 ~ 0.8
- 답변 품질: 0.6 ~ 0.8

### **보통 (⚠️)**
- 검색 성능: 0.4 ~ 0.6
- 답변 품질: 0.4 ~ 0.6

### **개선필요 (❌)**
- 검색 성능: 0.4 미만
- 답변 품질: 0.4 미만

## 🔧 **문제 해결**

### **낮은 Precision**
- 검색 결과에 관련 없는 문서가 많음
- **해결**: 메타데이터 필터링 강화, 임베딩 품질 개선

### **낮은 Recall**
- 관련 문서를 놓치고 있음
- **해결**: 검색 범위 확대, 키워드 매칭 개선

### **낮은 답변 품질**
- 생성된 답변이 부정확하거나 불완전
- **해결**: 프롬프트 개선, 더 많은 컨텍스트 제공

### **낮은 관련성**
- 질문과 답변이 맞지 않음
- **해결**: 검색 정확도 개선, 답변 생성 로직 점검

## 📝 **평가 결과 해석**

종합 평가 실행 후 다음과 같은 보고서가 생성됩니다:

```
📊 종합 RAG 성능 평가 보고서
=====================================
📈 전체 통계:
   총 테스트: 15개
   성공한 테스트: 14개
   성공률: 93.3%

🔍 문서 검색 성능:
   Precision@5: 0.742
   Recall@5: 0.681
   F1@5: 0.710
   MRR: 0.825
   MAP: 0.756
   NDCG@5: 0.789

💬 답변 생성 품질:
   키워드 겹침: 0.654
   의미적 유사도: 0.582
   완전성: 0.701
   관련성: 0.723

🎯 종합 성능 등급:
   검색 성능: ✅ 양호 (0.711)
   답변 품질: ✅ 양호 (0.665)
```

이 결과를 바탕으로 시스템 개선 방향을 결정할 수 있습니다!
