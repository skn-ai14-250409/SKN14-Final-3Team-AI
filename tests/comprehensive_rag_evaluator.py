#!/usr/bin/env python3
"""
ì¢…í•© RAG ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ (í†µí•© ë²„ì „)
- ë¬¸ì„œ ê²€ìƒ‰ ì„±ëŠ¥: MRR, MAP, NDCG, Precision, Recall, F1
- ë‹µë³€ ìƒì„± í’ˆì§ˆ: ì •í™•ì„±, ì™„ì „ì„±, ê´€ë ¨ì„±
- ë¹ ë¥¸ í…ŒìŠ¤íŠ¸, ê¸°ë³¸ í…ŒìŠ¤íŠ¸, ì¢…í•© í‰ê°€ í†µí•©
- ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„
"""
import requests
import time
import math
import re
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tests.rag_test_dataset import dataset, get_dataset_stats

BASE_URL = "http://localhost:8000/api/v1"

class RAGMetrics:
    """RAG í‰ê°€ ì§€í‘œ ê³„ì‚° í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_precision_at_k(retrieved_files: List[str], relevant_files: List[str], k: int = 5) -> float:
        """Precision@K ê³„ì‚°"""
        if not retrieved_files or k == 0:
            return 0.0
        
        retrieved_k = retrieved_files[:k]
        relevant_retrieved = sum(1 for file in retrieved_k if any(rel in file for rel in relevant_files))
        return relevant_retrieved / len(retrieved_k)
    
    @staticmethod
    def calculate_recall_at_k(retrieved_files: List[str], relevant_files: List[str], k: int = 5) -> float:
        """Recall@K ê³„ì‚°"""
        if not relevant_files:
            return 0.0
        
        retrieved_k = retrieved_files[:k]
        relevant_retrieved = sum(1 for rel_file in relevant_files 
                               if any(rel_file in ret_file for ret_file in retrieved_k))
        return relevant_retrieved / len(relevant_files)
    
    @staticmethod
    def calculate_f1_at_k(retrieved_files: List[str], relevant_files: List[str], k: int = 5) -> float:
        """F1@K ê³„ì‚°"""
        precision = RAGMetrics.calculate_precision_at_k(retrieved_files, relevant_files, k)
        recall = RAGMetrics.calculate_recall_at_k(retrieved_files, relevant_files, k)
        
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)
    
    @staticmethod
    def calculate_mrr(retrieved_files: List[str], relevant_files: List[str]) -> float:
        """Mean Reciprocal Rank ê³„ì‚°"""
        for i, retrieved_file in enumerate(retrieved_files, 1):
            if any(rel_file in retrieved_file for rel_file in relevant_files):
                return 1.0 / i
        return 0.0
    
    @staticmethod
    def calculate_ap(retrieved_files: List[str], relevant_files: List[str]) -> float:
        """Average Precision ê³„ì‚°"""
        if not relevant_files:
            return 0.0
        
        relevant_count = 0
        precision_sum = 0.0
        
        for i, retrieved_file in enumerate(retrieved_files, 1):
            if any(rel_file in retrieved_file for rel_file in relevant_files):
                relevant_count += 1
                precision_at_i = relevant_count / i
                precision_sum += precision_at_i
        
        return precision_sum / len(relevant_files) if relevant_files else 0.0
    
    @staticmethod
    def calculate_ndcg_at_k(retrieved_files: List[str], relevant_files: List[str], k: int = 5) -> float:
        """NDCG@K ê³„ì‚°"""
        if not relevant_files or k == 0:
            return 0.0
        
        # DCG ê³„ì‚°
        dcg = 0.0
        for i, retrieved_file in enumerate(retrieved_files[:k], 1):
            relevance = 1 if any(rel_file in retrieved_file for rel_file in relevant_files) else 0
            dcg += relevance / math.log2(i + 1)
        
        # IDCG ê³„ì‚° (ì´ìƒì ì¸ ìˆœì„œ)
        ideal_relevances = [1] * min(len(relevant_files), k) + [0] * max(0, k - len(relevant_files))
        idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevances))
        
        return dcg / idcg if idcg > 0 else 0.0

class AnswerQualityEvaluator:
    """ë‹µë³€ í’ˆì§ˆ í‰ê°€ í´ë˜ìŠ¤"""
    
    @staticmethod
    def calculate_keyword_overlap(generated_answer: str, expected_answer: str) -> float:
        """í‚¤ì›Œë“œ ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚°"""
        if not generated_answer or not expected_answer:
            return 0.0
        
        # í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        generated_keywords = set(re.findall(r'[ê°€-í£]{2,}', generated_answer.lower()))
        expected_keywords = set(re.findall(r'[ê°€-í£]{2,}', expected_answer.lower()))
        
        if not expected_keywords:
            return 0.0
        
        overlap = len(generated_keywords.intersection(expected_keywords))
        return overlap / len(expected_keywords)
    
    @staticmethod
    def calculate_semantic_similarity(generated_answer: str, expected_answer: str) -> float:
        """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        if not generated_answer or not expected_answer:
            return 0.0
        
        # í•µì‹¬ ê°œë… ì¶”ì¶œ
        generated_concepts = set(re.findall(r'[ê°€-í£]{3,}', generated_answer.lower()))
        expected_concepts = set(re.findall(r'[ê°€-í£]{3,}', expected_answer.lower()))
        
        if not expected_concepts:
            return 0.0
        
        # Jaccard ìœ ì‚¬ë„
        intersection = len(generated_concepts.intersection(expected_concepts))
        union = len(generated_concepts.union(expected_concepts))
        
        return intersection / union if union > 0 else 0.0
    
    @staticmethod
    def calculate_completeness(generated_answer: str, expected_answer: str) -> float:
        """ë‹µë³€ ì™„ì „ì„± ê³„ì‚°"""
        if not generated_answer or not expected_answer:
            return 0.0
        
        # ì˜ˆìƒ ë‹µë³€ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        expected_parts = expected_answer.split(',')
        covered_parts = 0
        
        for part in expected_parts:
            part_keywords = re.findall(r'[ê°€-í£]{2,}', part.strip())
            if any(keyword in generated_answer for keyword in part_keywords):
                covered_parts += 1
        
        return covered_parts / len(expected_parts) if expected_parts else 0.0
    
    @staticmethod
    def calculate_relevance(generated_answer: str, query: str) -> float:
        """ì§ˆë¬¸-ë‹µë³€ ê´€ë ¨ì„± ê³„ì‚°"""
        if not generated_answer or not query:
            return 0.0
        
        query_keywords = set(re.findall(r'[ê°€-í£]{2,}', query.lower()))
        answer_keywords = set(re.findall(r'[ê°€-í£]{2,}', generated_answer.lower()))
        
        if not query_keywords:
            return 0.0
        
        overlap = len(query_keywords.intersection(answer_keywords))
        return overlap / len(query_keywords)

class QuickTester:
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ (ê¸°ì¡´ quick_test.py ê¸°ëŠ¥)"""
    
    def __init__(self):
        self.base_url = BASE_URL
    
    def test_query(self, prompt: str, test_name: str):
        """ê°„ë‹¨í•œ ì§ˆì˜ í…ŒìŠ¤íŠ¸"""
        print(f"\nğŸ” {test_name}")
        print(f"ì§ˆì˜: '{prompt}'")
        print("-" * 50)
        
        start_time = time.time()
        try:
            response = requests.post(f"{self.base_url}/query_rag", 
                                   json={"prompt": prompt},
                                   headers={"Content-Type": "application/json"},
                                   timeout=30)
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                print(f"âœ… ì‘ë‹µì‹œê°„: {end_time - start_time:.3f}ì´ˆ")
                print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(data.get('sources', []))}")
                print(f"ğŸ’¬ ì‘ë‹µ: {data.get('response', '')[:100]}...")
                
                # ì†ŒìŠ¤ ì •ë³´ ì¶œë ¥
                sources = data.get('sources', [])
                if sources:
                    print(f"ğŸ“ ìƒìœ„ 3ê°œ ë¬¸ì„œ:")
                    for i, source in enumerate(sources[:3], 1):
                        print(f"   {i}. {source.get('file_name', 'Unknown')}")
                        if source.get('file_path'):
                            print(f"      ê²½ë¡œ: {source.get('file_path')}")
                        if source.get('main_category'):
                            print(f"      ë©”ì¸ì¹´í…Œê³ ë¦¬: {source.get('main_category')}")
                        if source.get('sub_category'):
                            print(f"      ì„œë¸Œì¹´í…Œê³ ë¦¬: {source.get('sub_category')}")
                        if source.get('page_number'):
                            print(f"      í˜ì´ì§€: {source.get('page_number')}")
                        if source.get('keywords'):
                            print(f"      í‚¤ì›Œë“œ: {source.get('keywords', [])}")
                        print()
            else:
                print(f"âŒ ì˜¤ë¥˜: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
    
    def run_quick_tests(self):
        """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ KB RAG ì‹œìŠ¤í…œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ì„œë²„ ìƒíƒœ í™•ì¸
        try:
            response = requests.get(f"{self.base_url}/healthcheck")
            if response.status_code == 200:
                print("âœ… ì„œë²„ ì—°ê²° ì„±ê³µ!")
            else:
                print("âŒ ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜")
                return
        except:
            print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”: python run_server.py --reload")
            return
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        test_cases = [
            ("ì‹ ìš©ëŒ€ì¶œ ê¸ˆë¦¬", "ì‹ ìš©ëŒ€ì¶œ ê´€ë ¨ ì§ˆì˜"),
            ("KB ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "ì£¼íƒëŒ€ì¶œ ê´€ë ¨ ì§ˆì˜"), 
            ("ê°œì¸ì •ë³´ë³´í˜¸ ì •ì±…", "ì •ì±… ê´€ë ¨ ì§ˆì˜"),
            ("ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²•", "ë²•ê·œ ê´€ë ¨ ì§ˆì˜")
        ]
        
        for prompt, test_name in test_cases:
            self.test_query(prompt, test_name)
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        print("\n" + "=" * 60)
        print("ğŸ¯ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("\nğŸ’¡ ë” ìì„¸í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ë³´ë ¤ë©´:")
        print("   curl http://localhost:8000/api/v1/vector_store_stats")

class BasicTester:
    """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤ (ê¸°ì¡´ test_rag_system.py ê¸°ëŠ¥)"""
    
    def __init__(self):
        self.base_url = BASE_URL
    
    def check_server(self):
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/healthcheck", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def test_basic_search(self, query: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{self.base_url}/query_rag",
                json={"prompt": query},
                timeout=30
            )
            
            end_time = time.time()
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "response_time": end_time - start_time,
                    "result_count": len(data.get("sources", [])),
                    "response": data.get("response", ""),
                    "sources": data.get("sources", [])
                }
            else:
                return {"success": False, "error": f"HTTP {response.status_code}"}
        
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def analyze_relevance(self, sources: List[Dict], query: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ë¶„ì„"""
        if not sources:
            return {"relevance_score": 0, "category_distribution": {}}
        
        query_keywords = query.lower().split()
        total_score = 0
        category_distribution = {}
        
        for source in sources:
            # ì¹´í…Œê³ ë¦¬ ë¶„í¬
            category = source.get("main_category", "unknown")
            category_distribution[category] = category_distribution.get(category, 0) + 1
            
            # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
            filename = source.get("file_name", "").lower()
            keywords = source.get("keywords", [])
            
            score = 0
            # íŒŒì¼ëª… ë§¤ì¹­
            score += sum(1 for kw in query_keywords if kw in filename)
            # í‚¤ì›Œë“œ ë§¤ì¹­  
            score += sum(1 for kw in query_keywords 
                        if any(k.lower().find(kw) >= 0 for k in keywords))
            
            total_score += score
        
        return {
            "relevance_score": total_score / len(sources),
            "category_distribution": category_distribution,
            "total_results": len(sources)
        }
    
    def run_basic_tests(self):
        """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸ§ª KB RAG ì‹œìŠ¤í…œ ê¸°ë³¸ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        # ì„œë²„ ì—°ê²° í™•ì¸
        if not self.check_server():
            print("âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("âœ… ì„œë²„ ì—°ê²° ì„±ê³µ!")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        test_cases = [
            {"query": "ì‹ ìš©ëŒ€ì¶œ ê¸ˆë¦¬", "description": "ìƒí’ˆ - ì‹ ìš©ëŒ€ì¶œ"},
            {"query": "KB ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "description": "ìƒí’ˆ - ì£¼íƒëŒ€ì¶œ"},
            {"query": "ê°œì¸ì •ë³´ë³´í˜¸ ì •ì±…", "description": "ì •ì±… - ê°œì¸ì •ë³´"},
            {"query": "ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²•", "description": "ë²•ê·œ - ì†Œë¹„ìë³´í˜¸"},
            {"query": "ìœ¤ë¦¬ê°•ë ¹", "description": "ì •ì±… - ìœ¤ë¦¬"}
        ]
        
        print(f"\nğŸ” {len(test_cases)}ê°œ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
        
        results = []
        
        for i, test_case in enumerate(test_cases, 1):
            query = test_case["query"]
            desc = test_case["description"]
            
            print(f"[{i}/{len(test_cases)}] {desc}")
            print(f"ì§ˆì˜: '{query}'")
            print("-" * 50)
            
            # ê¸°ë³¸ ê²€ìƒ‰
            basic_result = self.test_basic_search(query)
            if basic_result["success"]:
                basic_analysis = self.analyze_relevance(basic_result["sources"], query)
                print(f"ğŸ”¸ ê¸°ë³¸ ê²€ìƒ‰:")
                print(f"   ì‘ë‹µì‹œê°„: {basic_result['response_time']:.3f}ì´ˆ")
                print(f"   ê²°ê³¼ ìˆ˜: {basic_result['result_count']}ê°œ")
                print(f"   ê´€ë ¨ì„±: {basic_analysis['relevance_score']:.2f}")
                print(f"   ì¹´í…Œê³ ë¦¬: {basic_analysis['category_distribution']}")
                
                # ë¬¸ì„œ ì¶œì²˜ ì •ë³´ ì¶œë ¥
                if basic_result['sources']:
                    print(f"   ğŸ“š ìƒìœ„ ë¬¸ì„œ:")
                    for i, source in enumerate(basic_result['sources'][:3], 1):
                        print(f"      {i}. {source.get('file_name', 'Unknown')}")
                        if source.get('file_path'):
                            print(f"         ê²½ë¡œ: {source.get('file_path')}")
                        if source.get('main_category'):
                            print(f"         ì¹´í…Œê³ ë¦¬: {source.get('main_category')}")
                        if source.get('sub_category'):
                            print(f"         ì„œë¸Œì¹´í…Œê³ ë¦¬: {source.get('sub_category')}")
            else:
                print(f"ğŸ”¸ ê¸°ë³¸ ê²€ìƒ‰ ì‹¤íŒ¨: {basic_result['error']}")
            
            results.append(basic_result)
            print()
            time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
        
        # ì „ì²´ ê²°ê³¼ ìš”ì•½
        self.print_summary(results)
    
    def print_summary(self, results: List[Dict]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        successful_tests = sum(1 for r in results if r["success"])
        
        print(f"âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(results)}")
        
        if successful_tests > 0:
            # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
            avg_time = sum(r["response_time"] for r in results if r["success"]) / successful_tests
            print(f"\nâš¡ í‰ê·  ì‘ë‹µì‹œê°„: {avg_time:.3f}ì´ˆ")
        
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

class ComprehensiveRAGEvaluator:
    """ì¢…í•© RAG í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.base_url = BASE_URL
        self.metrics = RAGMetrics()
        self.answer_evaluator = AnswerQualityEvaluator()
        self.results = []
    
    def check_server(self) -> bool:
        """ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.base_url}/healthcheck", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def evaluate_single_query(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ì§ˆì˜ì— ëŒ€í•œ ì¢…í•© í‰ê°€"""
        test_id = test_case.get("id", "unknown")
        query = test_case["query"]
        expected_answer = test_case["expected_answer"]
        expected_files = test_case.get("expected_file", [])
        
        if isinstance(expected_files, str):
            expected_files = [expected_files]
        
        print(f"ğŸ” í‰ê°€ ì¤‘: {test_id}")
        print(f"ì§ˆì˜: {query}")
        
        try:
            # RAG ì‹œìŠ¤í…œì— ì§ˆì˜
            response = requests.post(
                f"{self.base_url}/query_rag",
                json={"prompt": query},
                timeout=30
            )
            
            if response.status_code != 200:
                return {
                    "test_id": test_id,
                    "success": False,
                    "error": f"HTTP {response.status_code}"
                }
            
            data = response.json()
            generated_answer = data.get("response", "")
            sources = data.get("sources", [])
            
            # ê²€ìƒ‰ëœ íŒŒì¼ëª… ì¶”ì¶œ
            retrieved_files = [source.get("file_name", "") for source in sources]
            
            # === ë¬¸ì„œ ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ ===
            retrieval_metrics = {
                "precision_at_3": self.metrics.calculate_precision_at_k(retrieved_files, expected_files, 3),
                "precision_at_5": self.metrics.calculate_precision_at_k(retrieved_files, expected_files, 5),
                "recall_at_3": self.metrics.calculate_recall_at_k(retrieved_files, expected_files, 3),
                "recall_at_5": self.metrics.calculate_recall_at_k(retrieved_files, expected_files, 5),
                "f1_at_3": self.metrics.calculate_f1_at_k(retrieved_files, expected_files, 3),
                "f1_at_5": self.metrics.calculate_f1_at_k(retrieved_files, expected_files, 5),
                "mrr": self.metrics.calculate_mrr(retrieved_files, expected_files),
                "map": self.metrics.calculate_ap(retrieved_files, expected_files),
                "ndcg_at_3": self.metrics.calculate_ndcg_at_k(retrieved_files, expected_files, 3),
                "ndcg_at_5": self.metrics.calculate_ndcg_at_k(retrieved_files, expected_files, 5)
            }
            
            # === ë‹µë³€ í’ˆì§ˆ í‰ê°€ ===
            answer_quality = {
                "keyword_overlap": self.answer_evaluator.calculate_keyword_overlap(generated_answer, expected_answer),
                "semantic_similarity": self.answer_evaluator.calculate_semantic_similarity(generated_answer, expected_answer),
                "completeness": self.answer_evaluator.calculate_completeness(generated_answer, expected_answer),
                "relevance": self.answer_evaluator.calculate_relevance(generated_answer, query)
            }
            
            # === ì „ì²´ ê²°ê³¼ ===
            result = {
                "test_id": test_id,
                "query": query,
                "success": True,
                "generated_answer": generated_answer,
                "expected_answer": expected_answer,
                "retrieved_files": retrieved_files,
                "expected_files": expected_files,
                "retrieval_metrics": retrieval_metrics,
                "answer_quality": answer_quality,
                "sources_count": len(sources)
            }
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"ğŸ“„ ê²€ìƒ‰ ì„±ëŠ¥:")
            print(f"   Precision@5: {retrieval_metrics['precision_at_5']:.3f}")
            print(f"   Recall@5: {retrieval_metrics['recall_at_5']:.3f}")
            print(f"   F1@5: {retrieval_metrics['f1_at_5']:.3f}")
            print(f"   MRR: {retrieval_metrics['mrr']:.3f}")
            print(f"   MAP: {retrieval_metrics['map']:.3f}")
            print(f"   NDCG@5: {retrieval_metrics['ndcg_at_5']:.3f}")
            
            print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œì²˜:")
            if retrieved_files:
                for i, file_info in enumerate(sources[:5], 1):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    file_name = file_info.get("file_name", "Unknown")
                    file_path = file_info.get("file_path", "")
                    main_category = file_info.get("main_category", "")
                    sub_category = file_info.get("sub_category", "")
                    page_number = file_info.get("page_number", "")
                    
                    print(f"   {i}. {file_name}")
                    if file_path:
                        print(f"      ê²½ë¡œ: {file_path}")
                    if main_category:
                        print(f"      ì¹´í…Œê³ ë¦¬: {main_category}")
                    if sub_category:
                        print(f"      ì„œë¸Œì¹´í…Œê³ ë¦¬: {sub_category}")
                    if page_number:
                        print(f"      í˜ì´ì§€: {page_number}")
                    print()
            else:
                print("   ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ")
            
            print(f"ğŸ’¬ ë‹µë³€ í’ˆì§ˆ:")
            print(f"   í‚¤ì›Œë“œ ê²¹ì¹¨: {answer_quality['keyword_overlap']:.3f}")
            print(f"   ì˜ë¯¸ì  ìœ ì‚¬ë„: {answer_quality['semantic_similarity']:.3f}")
            print(f"   ì™„ì „ì„±: {answer_quality['completeness']:.3f}")
            print(f"   ê´€ë ¨ì„±: {answer_quality['relevance']:.3f}")
            
            print(f"ğŸ“‹ ì˜ˆìƒ ì •ë‹µ:")
            print(f"   {expected_answer}")
            
            print(f"ğŸ“ ì˜ˆìƒ íŒŒì¼:")
            if expected_files:
                for i, expected_file in enumerate(expected_files, 1):
                    print(f"   {i}. {expected_file}")
            else:
                print("   ì˜ˆìƒ íŒŒì¼ ì—†ìŒ")
            
            print(f"ğŸ¯ ìƒì„±ëœ ë‹µë³€: {generated_answer[:100]}...")
            
            return result
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì˜¤ë¥˜: {e}")
            return {
                "test_id": test_id,
                "success": False,
                "error": str(e)
            }
    
    def run_comprehensive_evaluation(self, test_cases: List[Dict[str, Any]], max_tests: int = None):
        """ì¢…í•© í‰ê°€ ì‹¤í–‰"""
        if max_tests:
            test_cases = test_cases[:max_tests]
        
        print(f"ğŸ§ª ì¢…í•© RAG í‰ê°€ ì‹œì‘ ({len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸)")
        print("=" * 80)
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\n[{i}/{len(test_cases)}]")
            result = self.evaluate_single_query(test_case)
            self.results.append(result)
            print("-" * 60)
            time.sleep(1)  # API ë¶€í•˜ ë°©ì§€
    
    def calculate_aggregate_metrics(self) -> Dict[str, Any]:
        """ì „ì²´ í‰ê°€ ì§€í‘œ ì§‘ê³„"""
        if not self.results:
            return {}
        
        # HTTP ì„±ê³µí•œ í…ŒìŠ¤íŠ¸
        http_successful_results = [r for r in self.results if r.get("success", False)]
        
        if not http_successful_results:
            return {"error": "No successful evaluations"}
        
        # ì‹¤ì§ˆì ìœ¼ë¡œ ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•œ í…ŒìŠ¤íŠ¸ (ê²€ìƒ‰ ì„±ëŠ¥ì´ ì¼ì • ìˆ˜ì¤€ ì´ìƒ)
        meaningful_results = []
        no_answer_results = []
        
        for r in http_successful_results:
            # ê²€ìƒ‰ ì„±ëŠ¥ì´ ë‚®ê±°ë‚˜ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ì¸ ê²½ìš°
            if (r["retrieval_metrics"]["precision_at_5"] < 0.1 or 
                r["retrieval_metrics"]["recall_at_5"] < 0.1 or
                "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in r.get("generated_answer", "")):
                no_answer_results.append(r)
            else:
                meaningful_results.append(r)
        
        # ê²€ìƒ‰ ì„±ëŠ¥ í‰ê·  (ì˜ë¯¸ìˆëŠ” ê²°ê³¼ë§Œ)
        retrieval_avg = {}
        if meaningful_results:
            for metric in ["precision_at_3", "precision_at_5", "recall_at_3", "recall_at_5", 
                          "f1_at_3", "f1_at_5", "mrr", "map", "ndcg_at_3", "ndcg_at_5"]:
                values = [r["retrieval_metrics"][metric] for r in meaningful_results]
                retrieval_avg[metric] = sum(values) / len(values)
        else:
            retrieval_avg = {metric: 0.0 for metric in ["precision_at_3", "precision_at_5", "recall_at_3", "recall_at_5", 
                                                        "f1_at_3", "f1_at_5", "mrr", "map", "ndcg_at_3", "ndcg_at_5"]}
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê·  (ì˜ë¯¸ìˆëŠ” ê²°ê³¼ë§Œ)
        answer_avg = {}
        if meaningful_results:
            for metric in ["keyword_overlap", "semantic_similarity", "completeness", "relevance"]:
                values = [r["answer_quality"][metric] for r in meaningful_results]
                answer_avg[metric] = sum(values) / len(values)
        else:
            answer_avg = {metric: 0.0 for metric in ["keyword_overlap", "semantic_similarity", "completeness", "relevance"]}
        
        return {
            "total_tests": len(self.results),
            "http_successful_tests": len(http_successful_results),
            "meaningful_tests": len(meaningful_results),
            "no_answer_tests": len(no_answer_results),
            "http_success_rate": len(http_successful_results) / len(self.results),
            "meaningful_success_rate": len(meaningful_results) / len(self.results),
            "retrieval_performance": retrieval_avg,
            "answer_quality": answer_avg
        }
    
    def print_comprehensive_report(self):
        """ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "=" * 80)
        print("ğŸ“Š ì¢…í•© RAG ì„±ëŠ¥ í‰ê°€ ë³´ê³ ì„œ")
        print("=" * 80)
        
        aggregate = self.calculate_aggregate_metrics()
        
        if "error" in aggregate:
            print(f"âŒ {aggregate['error']}")
            return
        
        print(f"ğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"   ì´ í…ŒìŠ¤íŠ¸: {aggregate['total_tests']}ê°œ")
        print(f"   HTTP ì„±ê³µ: {aggregate['http_successful_tests']}ê°œ")
        print(f"   ì˜ë¯¸ìˆëŠ” ë‹µë³€: {aggregate['meaningful_tests']}ê°œ")
        print(f"   ë‹µë³€ ì—†ìŒ: {aggregate['no_answer_tests']}ê°œ")
        print(f"   HTTP ì„±ê³µë¥ : {aggregate['http_success_rate']*100:.1f}%")
        print(f"   ì˜ë¯¸ìˆëŠ” ë‹µë³€ë¥ : {aggregate['meaningful_success_rate']*100:.1f}%")
        
        print(f"\nğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì„±ëŠ¥:")
        retrieval = aggregate['retrieval_performance']
        print(f"   Precision@3: {retrieval['precision_at_3']:.3f}")
        print(f"   Precision@5: {retrieval['precision_at_5']:.3f}")
        print(f"   Recall@3: {retrieval['recall_at_3']:.3f}")
        print(f"   Recall@5: {retrieval['recall_at_5']:.3f}")
        print(f"   F1@3: {retrieval['f1_at_3']:.3f}")
        print(f"   F1@5: {retrieval['f1_at_5']:.3f}")
        print(f"   MRR: {retrieval['mrr']:.3f}")
        print(f"   MAP: {retrieval['map']:.3f}")
        print(f"   NDCG@3: {retrieval['ndcg_at_3']:.3f}")
        print(f"   NDCG@5: {retrieval['ndcg_at_5']:.3f}")
        
        print(f"\nğŸ’¬ ë‹µë³€ ìƒì„± í’ˆì§ˆ:")
        answer = aggregate['answer_quality']
        print(f"   í‚¤ì›Œë“œ ê²¹ì¹¨: {answer['keyword_overlap']:.3f}")
        print(f"   ì˜ë¯¸ì  ìœ ì‚¬ë„: {answer['semantic_similarity']:.3f}")
        print(f"   ì™„ì „ì„±: {answer['completeness']:.3f}")
        print(f"   ê´€ë ¨ì„±: {answer['relevance']:.3f}")
        
        # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        overall_retrieval = (retrieval['precision_at_5'] + retrieval['recall_at_5'] + retrieval['f1_at_5']) / 3
        overall_answer = (answer['keyword_overlap'] + answer['semantic_similarity'] + answer['completeness'] + answer['relevance']) / 4
        
        print(f"\nğŸ¯ ì¢…í•© ì„±ëŠ¥ ë“±ê¸‰:")
        print(f"   ê²€ìƒ‰ ì„±ëŠ¥: {self._get_performance_grade(overall_retrieval)} ({overall_retrieval:.3f})")
        print(f"   ë‹µë³€ í’ˆì§ˆ: {self._get_performance_grade(overall_answer)} ({overall_answer:.3f})")
        
        # ê°œì„  ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        if overall_retrieval < 0.5:
            print("   - ê²€ìƒ‰ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° í•„í„°ë§ì´ë‚˜ ì„ë² ë”© ëª¨ë¸ ê°œì„ ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        if overall_answer < 0.5:
            print("   - ë‹µë³€ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì´ë‚˜ LLM ëª¨ë¸ ê°œì„ ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        if retrieval['recall_at_5'] < 0.6:
            print("   - Recallì´ ë‚®ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ë²”ìœ„ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ í‚¤ì›Œë“œ ë§¤ì¹­ì„ ê°œì„ í•˜ì„¸ìš”.")
        if answer['completeness'] < 0.6:
            print("   - ë‹µë³€ ì™„ì „ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.")
    
    def _get_performance_grade(self, score: float) -> str:
        """ì„±ëŠ¥ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 0.8:
            return "ğŸŒŸ ìš°ìˆ˜"
        elif score >= 0.6:
            return "âœ… ì–‘í˜¸"
        elif score >= 0.4:
            return "âš ï¸ ë³´í†µ"
        else:
            return "âŒ ê°œì„ í•„ìš”"

def main():
    print("ğŸ§ª KB RAG ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ë„êµ¬")
    print("=" * 80)
    
    # ë°ì´í„°ì…‹ í†µê³„
    stats = get_dataset_stats()
    print(f"ğŸ“Š í‰ê°€ ë°ì´í„°ì…‹:")
    print(f"   ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {stats['total_cases']}ê°œ")
    print(f"   ë‚œì´ë„ë³„: {stats['by_difficulty']}")
    print(f"   ì¹´í…Œê³ ë¦¬ë³„: {stats['by_subcategory']}")
    
    # ì„œë²„ ì—°ê²° í™•ì¸
    try:
        response = requests.get(f"{BASE_URL}/healthcheck", timeout=5)
        if response.status_code != 200:
            print("\nâŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”: python run_server.py --reload")
            return
    except:
        print("\nâŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”: python run_server.py --reload")
        return
    
    print("\nâœ… ì„œë²„ ì—°ê²° ì„±ê³µ!")
    
    # í…ŒìŠ¤íŠ¸ ì˜µì…˜ ì„ íƒ
    print("\nğŸ® í…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("1. ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (4ê°œ ê¸°ë³¸ ì¼€ì´ìŠ¤)")
    print("2. ğŸ§ª ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (5ê°œ ìƒì„¸ ì¼€ì´ìŠ¤)")
    print("3. ğŸ“Š ì¢…í•© í‰ê°€ (ë°ì´í„°ì…‹ ê¸°ë°˜)")
    print("4. ğŸ” ë¹ ë¥¸ ì§ˆì˜ í…ŒìŠ¤íŠ¸ (ì§ì ‘ ì…ë ¥)")
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    if choice == "1":
        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
        quick_tester = QuickTester()
        quick_tester.run_quick_tests()
        
    elif choice == "2":
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        basic_tester = BasicTester()
        basic_tester.run_basic_tests()
        
    elif choice == "3":
        # ì¢…í•© í‰ê°€
        evaluator = ComprehensiveRAGEvaluator()
        
        # í‰ê°€ ì˜µì…˜ ì„ íƒ
        print("\nğŸ“Š í‰ê°€ ì˜µì…˜:")
        print("1. ë¹ ë¥¸ í‰ê°€ (5ê°œ ì¼€ì´ìŠ¤)")
        print("2. í‘œì¤€ í‰ê°€ (10ê°œ ì¼€ì´ìŠ¤)")
        print("3. ì „ì²´ í‰ê°€ (ëª¨ë“  ì¼€ì´ìŠ¤)")
        print("4. ë‚œì´ë„ë³„ í‰ê°€")
        
        eval_choice = input("\nì„ íƒ (1-4): ").strip()
        
        if eval_choice == "1":
            evaluator.run_comprehensive_evaluation(dataset[:5])
        elif eval_choice == "2":
            evaluator.run_comprehensive_evaluation(dataset[:10])
        elif eval_choice == "3":
            confirm = input("âš ï¸ ì „ì²´ í‰ê°€ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if confirm.lower() == 'y':
                evaluator.run_comprehensive_evaluation(dataset)
            else:
                print("í‰ê°€ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
        elif eval_choice == "4":
            difficulty = input("ë‚œì´ë„ ì„ íƒ (easy/medium/hard): ").strip()
            test_cases = [case for case in dataset if case.get("difficulty") == difficulty]
            if test_cases:
                evaluator.run_comprehensive_evaluation(test_cases)
            else:
                print(f"âŒ '{difficulty}' ë‚œì´ë„ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
            return
        
        # ì¢…í•© ë³´ê³ ì„œ ì¶œë ¥
        evaluator.print_comprehensive_report()
        
    elif choice == "4":
        # ë¹ ë¥¸ ì§ˆì˜ í…ŒìŠ¤íŠ¸
        print("\nğŸ” ë¹ ë¥¸ ì§ˆì˜ í…ŒìŠ¤íŠ¸")
        print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ì¦‰ì‹œ RAG ì‹œìŠ¤í…œì—ì„œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        while True:
            query = input("\nì§ˆë¬¸: ").strip()
            if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                break
            
            if not query:
                continue
            
            try:
                response = requests.post(
                    f"{BASE_URL}/query_rag",
                    json={"prompt": query},
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    print(f"\nğŸ’¬ ë‹µë³€: {data.get('response', '')}")
                    
                    sources = data.get('sources', [])
                    if sources:
                        print(f"\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
                        for i, source in enumerate(sources[:3], 1):
                            print(f"   {i}. {source.get('file_name', 'Unknown')}")
                            if source.get('file_path'):
                                print(f"      ê²½ë¡œ: {source.get('file_path')}")
                else:
                    print(f"âŒ ì˜¤ë¥˜: {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
    
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
