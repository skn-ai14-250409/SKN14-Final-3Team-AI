"""
ì‹¤í—˜ìš© LangGraph RAG ì›Œí¬í”Œë¡œìš°

ê¸°ì¡´ orchestrator.pyì™€ ë™ì¼í•œ ê¸°ëŠ¥ì„ ê·¸ë˜í”„ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„
ê¸°ì¡´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ ì‹œí—˜í•´ë³´ê¸° ìœ„í•œ íŒŒì¼
"""

from typing import Dict, List, Any, TypedDict, Annotated, Optional, Pattern
import logging
import re
import time
import uuid
import os
import yaml
from dataclasses import dataclass, field
from datetime import datetime
from operator import add

from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, AIMessage
from langchain_core.documents import Document
from pydantic import BaseModel, Field

from src.slm.slm import SLM
from src.rag.vector_store import VectorStore
from src.intent_router import IntentRouter
from src.langgraph.session_manager import session_manager, ConversationTurn, SessionContext
try:
    from src.config.keyword_mappings import (
        get_expansion_patterns,
        get_synonym_mappings,
        get_financial_terms,
        get_keyword_weights
    )
except ImportError:
    # ì„¤ì • íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    def get_expansion_patterns():
        return {}
    def get_synonym_mappings():
        return {}
    def get_financial_terms():
        return {}
    def get_keyword_weights():
        return {}
from src.constants import (
    NO_ANSWER_MSG,
    MAIN_LAW, MAIN_RULE, MAIN_PRODUCT,
    GENERAL_FAQ_CATEGORY,
    COMPANY_PRODUCTS_CATEGORY,
    COMPANY_RULES_CATEGORY,
    INDUSTRY_POLICY_CATEGORY,
)

logger = logging.getLogger(__name__)

# ê°„ë‹¨í•œ ë¡œê¹… í—¬í¼ í•¨ìˆ˜
def log_node_start(node_name: str, session_id: str = None):
    """ë…¸ë“œ ì‹œì‘ ë¡œê¹…"""
    logger.info(f"[GRAPH] {node_name} started - session_id: {session_id or 'unknown'}")

def log_node_complete(node_name: str, session_id: str = None):
    """ë…¸ë“œ ì™„ë£Œ ë¡œê¹…"""
    logger.info(f"[GRAPH] {node_name} completed - session_id: {session_id or 'unknown'}")

# ê³µí†µ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
class RAGUtils:
    """RAG ê´€ë ¨ ê³µí†µ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤"""
    
    # ê³µí†µ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
    STOP_WORDS = {
        "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì˜", "ë¡œ", "ìœ¼ë¡œ", "ë„", "ë§Œ", 
        "ë¶€í„°", "ê¹Œì§€", "ì—ì„œ", "ì—ê²Œ", "í•œí…Œ", "ì™€", "ê³¼", "ë­", "ìˆì–´", "ìˆë‚˜", 
        "ì•Œë ¤", "ì£¼ì„¸ìš”", "ì•ˆë‚´", "ì •ë³´", "ì–´ë–»ê²Œ", "ë¬´ì—‡", "ì–¸ì œ", "ì–´ë””", "ì™œ"
    }
    
    @staticmethod
    def extract_keywords_from_query(query: str) -> List[str]:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœì í™”ëœ ë²„ì „)"""
        try:
            # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
            words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', query)
            
            # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
            keywords = [word for word in words 
                       if len(word) > 1 and word not in RAGUtils.STOP_WORDS]
            
            return keywords
        except Exception as e:
            logger.error(f"[UTILS] Error extracting keywords from query: {e}")
            return []

    @staticmethod
    def extract_keywords_from_filename(filename: str) -> List[str]:
        """íŒŒì¼ëª…ì—ì„œ í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ (ìµœì í™”ëœ ë²„ì „)"""
        try:
            # íŒŒì¼ëª… ì •ë¦¬ (í™•ì¥ì ì œê±°, ì–¸ë”ìŠ¤ì½”ì–´/ê³µë°± ì²˜ë¦¬)
            clean_name = filename.replace(".pdf", "").replace("KB_", "").replace("_", " ")

            # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
            keywords = re.findall(r'[ê°€-í£a-zA-Z0-9]+', clean_name)

            # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ í•„í„°ë§
            keywords = [kw for kw in keywords 
                       if len(kw) > 1 and kw not in RAGUtils.STOP_WORDS]

            return keywords
        except Exception as e:
            logger.error(f"[UTILS] Error extracting keywords from filename: {e}")
            return []

    @staticmethod
    def calculate_keyword_match_score(query_keywords: List[str], file_keywords: List[str]) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (í†µí•©ëœ ë²„ì „)"""
        try:
            if not query_keywords or not file_keywords:
                return 0.0
            
            # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚° ì‚¬ìš©
            return RAGUtils.calculate_weighted_keyword_score(query_keywords, file_keywords)
        except Exception as e:
            logger.error(f"[UTILS] Error calculating keyword match score: {e}")
            return 0.0

    @staticmethod
    def extract_keywords_from_product_name(product_name: str) -> List[str]:
        """ìƒí’ˆëª…ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ë™ì  ë°©ì‹)"""
        keywords = []
        clean_name = product_name.replace("KB", "").strip()
        
        # 1. ê¸°ë³¸ ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£]+', clean_name)
        for word in words:
            if len(word) > 1:
                keywords.append(word)
        
        # 2. ë™ì  í‚¤ì›Œë“œ í™•ì¥ (í•˜ë“œì½”ë”© ëŒ€ì‹  íŒ¨í„´ ê¸°ë°˜)
        expanded_keywords = RAGUtils._expand_keywords_dynamically(keywords, product_name)
        keywords.extend(expanded_keywords)
        
        return list(set(keywords))
    
    @staticmethod
    def _expand_keywords_dynamically(base_keywords: List[str], product_name: str) -> List[str]:
        """ë™ì ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ í™•ì¥í•˜ëŠ” ë©”ì„œë“œ (ì„¤ì • íŒŒì¼ ê¸°ë°˜)"""
        expanded = []
        
        # ì„¤ì • íŒŒì¼ì—ì„œ íŒ¨í„´ ê°€ì ¸ì˜¤ê¸°
        expansion_patterns = get_expansion_patterns()
        synonym_mappings = get_synonym_mappings()
        
        # 1. íŒ¨í„´ ê¸°ë°˜ í‚¤ì›Œë“œ í™•ì¥
        for pattern, related_terms in expansion_patterns.items():
            if re.search(pattern, product_name):
                expanded.extend(related_terms)
        
        # 2. ìœ ì‚¬ì–´/ë™ì˜ì–´ í™•ì¥
        for keyword in base_keywords:
            if keyword in synonym_mappings:
                expanded.extend(synonym_mappings[keyword])
        
        # 3. ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ ì¶”ê°€
        financial_terms = get_financial_terms()
        for keyword in base_keywords:
            if keyword in financial_terms:
                expanded.extend(financial_terms[keyword])
        
        return expanded
    
    @staticmethod
    def calculate_weighted_keyword_score(query_keywords: List[str], file_keywords: List[str]) -> float:
        """ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        try:
            if not query_keywords or not file_keywords:
                return 0.0
            
            keyword_weights = get_keyword_weights()
            total_score = 0.0
            matched_count = 0
            
            for q_kw in query_keywords:
                # ì •í™•í•œ ë§¤ì¹­
                if q_kw in file_keywords:
                    weight = keyword_weights.get(q_kw, 0.5)  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ 0.5
                    total_score += weight * 1.0
                    matched_count += 1
                
                # ë¶€ë¶„ ë§¤ì¹­
                for f_kw in file_keywords:
                    if q_kw in f_kw or f_kw in q_kw:
                        weight = keyword_weights.get(q_kw, 0.3)  # ë¶€ë¶„ ë§¤ì¹­ì€ ë‚®ì€ ê°€ì¤‘ì¹˜
                        total_score += weight * 0.3
                        matched_count += 1
                        break  # ì¤‘ë³µ ê³„ì‚° ë°©ì§€
            
            # ì •ê·œí™” (ë§¤ì¹­ëœ í‚¤ì›Œë“œ ìˆ˜ë¡œ ë‚˜ëˆ„ê¸°)
            return total_score / len(query_keywords) if query_keywords else 0.0
            
        except Exception as e:
            logger.error(f"[UTILS] Error calculating weighted keyword score: {e}")
            return 0.0

    @staticmethod
    def normalize_retrieved(items) -> List[Document]:
        """ê²€ìƒ‰ ê²°ê³¼ ì •ê·œí™”"""
        norm = []
        for it in items or []:
            if isinstance(it, Document):
                norm.append(it)
            elif isinstance(it, str):
                norm.append(Document(page_content=it, metadata={"source_type": "raw_text"}))
            elif isinstance(it, dict):
                text = it.get("page_content") or it.get("text") or it.get("content") or ""
                meta = it.get("metadata") or {}
                norm.append(Document(page_content=text, metadata=meta))
            else:
                norm.append(Document(page_content=str(it), metadata={"source_type": "unknown"}))
        return norm

    @staticmethod
    def format_context(docs) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·"""
        lines = []
        for d in docs:
            meta = d.metadata or {}
            src = meta.get("relative_path") or meta.get("file_name") or meta.get("source_type") or "unknown_source"
            snippet = (d.page_content or "").strip()
            if not snippet:
                continue
            lines.append(f"[source: {src}]\n{snippet}")
        return "\n---\n".join(lines)

    @staticmethod
    def filter_by_relevance_score(docs: List[Document], query: str) -> List[Document]:
        """ê´€ë ¨ì„± í•„í„°ë§"""
        if not docs:
            return docs

        query_words = set(query.lower().split())
        scored_docs = []

        for doc in docs:
            content = (doc.page_content or "").lower()
            metadata = doc.metadata or {}

            score = 1.0

            # í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            content_words = set(content.split())
            keyword_overlap = len(query_words.intersection(content_words))
            score += keyword_overlap * 0.1

            # ë©”íƒ€ë°ì´í„° í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
            metadata_keywords = metadata.get("keywords", [])
            for keyword in metadata_keywords:
                if keyword.lower() in query.lower():
                    score += 0.2

            # íŒŒì¼ëª… ë§¤ì¹­ ë³´ë„ˆìŠ¤
            file_name = metadata.get("file_name", "").lower()
            for word in query_words:
                if word in file_name:
                    score += 0.3

            scored_docs.append((score, doc))

        scored_docs.sort(key=lambda x: x[0], reverse=True)
        min_score = 1.0
        filtered_docs = [doc for score, doc in scored_docs if score >= min_score]

        return filtered_docs[:5]

# --------------------- Guardrail YAML dataclasses ---------------------
@dataclass
class PolicySourceRef:
    file: str
    clause: Optional[str] = None

@dataclass
class PolicyRule:
    rule_id: str
    policy: str
    severity: str                 # "HIGH" | "MEDIUM" | "LOW"
    patterns: List[str]
    disclosures: List[str] = field(default_factory=list)
    fix_hint: str = ""
    sources: List[PolicySourceRef] = field(default_factory=list)
    compiled: List[Pattern] = field(default_factory=list)

@dataclass
class SoftFixRule:
    pattern: str
    replacement: str
    compiled: Optional[Pattern] = None

# LangGraph State ì •ì˜
class RAGState(TypedDict):
    """RAG ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    messages: Annotated[List[BaseMessage], add]
    query: str
    category: str
    product_name: str
    retrieved_docs: List[Document]
    context_text: str
    response: str
    sources: List[Dict[str, Any]]
    session_context: SessionContext  # ë©€í‹°í„´ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸
    conversation_history: List[ConversationTurn]  # ëŒ€í™” íˆìŠ¤í† ë¦¬
    turn_id: str  # í˜„ì¬ í„´ ID
    # guardrail ê²°ê³¼ (ìµœì†Œ)
    guardrail_decision: str
    violations: List[Dict[str, Any]]
    compliant_response: str

# Pydantic ëª¨ë¸: í•¨ìˆ˜ ë‚´ë¶€ ì¤‘ì²© ì •ì˜ë¥¼ ëª¨ë“ˆ ìˆ˜ì¤€ìœ¼ë¡œ ì´ë™
class ProductNameResponse(BaseModel):
    product_name: str = Field(
        ...,
        description="ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ KBê¸ˆìœµê·¸ë£¹ ìƒí’ˆëª…ë§Œ ì¶”ì¶œí•˜ì„¸ìš”. ìƒí’ˆëª…ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜í•˜ì„¸ìš”."
    )

class LangGraphRAGWorkflow:
    """LangGraph ê¸°ë°˜ ì‹¤í—˜ìš© RAG ì›Œí¬í”Œë¡œìš°"""

    def __init__(self):
        self.slm = SLM()
        self.vector_store = VectorStore()
        self.router = IntentRouter()
        # Guardrail ìë£Œêµ¬ì¡° (YAML ë¡œë“œ í›„ ì‚¬ìš©)
        self._policy_rules: List[PolicyRule] = []
        self._soft_fix_rules: List[SoftFixRule] = []
        self._glossary_terms: List[Dict[str, str]] = []
        self._glossary_regex_terms: List[Dict[str, str]] = []
        self._glossary_opts: Dict[str, Any] = {}
        self._glossary_regex_compiled: List[re.Pattern] = []

        self.workflow = self._build_workflow()
        # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ìºì‹œ
        self._filename_cache = None
        self._filename_index = None

        # ìµœì†Œ YAML ë¡œë“œ
        # ì‹¤ì œ guardrails í´ë”ì˜ policy_rules.yaml ê²½ë¡œë¡œ ìˆ˜ì •
        self._load_minimal_guardrail_yamls("src/langgraph/guardrails/policy_rules.yaml")

    def _build_workflow(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶• (ë©€í‹°í„´ ëŒ€í™” ì§€ì›)"""
        workflow = StateGraph(RAGState)

        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("session_init", self._session_init)  # ì„¸ì…˜ ì´ˆê¸°í™”
        workflow.add_node("context_analysis", self._context_analysis)  # ë§¥ë½ ë¶„ì„
        workflow.add_node("first_turn_preprocess", self._first_turn_preprocess)  # ì²« í„´ ì „ì²˜ë¦¬
        workflow.add_node("classify_intent", self._classify_intent)
        workflow.add_node("handle_general_faq", self._handle_general_faq)
        workflow.add_node("extract_product_name", self._extract_product_name)
        workflow.add_node("search_documents", self._search_documents)
        workflow.add_node("filter_relevance", self._filter_relevance)
        workflow.add_node("generate_response", self._generate_response)
        # ğŸ”¹ ìµœì¢… ê°€ë“œë ˆì¼ ë…¸ë“œ ì¶”ê°€
        workflow.add_node("guardrails", self._guardrails_slim_inline)
        workflow.add_node("save_conversation", self._save_conversation)  # ëŒ€í™” ì €ì¥

        # ì—£ì§€ ì¶”ê°€ (ë‹¨ìˆœí™”ëœ í”Œë¡œìš°)
        workflow.add_edge(START, "session_init")
        workflow.add_edge("session_init", "first_turn_preprocess")  # context_analysis ìš°íšŒ
        workflow.add_edge("first_turn_preprocess", "classify_intent")

        workflow.add_conditional_edges(
            "classify_intent",
            self._route_by_category,
            {
                "general_faq": "handle_general_faq",
                "rag_needed": "extract_product_name"
            }
        )

        workflow.add_edge("handle_general_faq", "save_conversation")
        workflow.add_edge("extract_product_name", "search_documents")
        workflow.add_edge("search_documents", "filter_relevance")
        workflow.add_edge("filter_relevance", "generate_response")
        # ğŸ”¹ generate_response â†’ guardrails â†’ save_conversation
        workflow.add_edge("generate_response", "guardrails")
        workflow.add_edge("guardrails", "save_conversation")
        workflow.add_edge("save_conversation", END)

        return workflow.compile()

    # ------------------------- Guardrail: YAML Loader -------------------------
    def _load_minimal_guardrail_yamls(self, policy_path: str = "config/policy_rules.yaml"):
        """policy_rules.yaml + glossary_terms.yaml ë¡œë“œí•´ì„œ
        _guardrails_slim_inlineì´ ë°”ë¡œ ì“¸ ìˆ˜ ìˆê²Œ ì…‹ì—…."""
        self._policy_rules = []
        self._soft_fix_rules = []
        self._glossary_terms = []
        self._glossary_regex_terms = []
        self._glossary_opts = {}
        self._glossary_regex_compiled = []

        if not os.path.exists(policy_path):
            logger.warning(f"[GUARD] policy file not found: {policy_path}")
            return

        with open(policy_path, "r", encoding="utf-8") as f:
            data = yaml.safe_load(f) or {}

        # rules
        for r in data.get("rules", []) or []:
            srcs = [PolicySourceRef(**s) for s in (r.get("sources") or [])]
            rule = PolicyRule(
                rule_id=r["rule_id"],
                policy=r.get("policy", "INTERNAL"),
                severity=r.get("severity", "MEDIUM").upper(),
                patterns=r.get("patterns", []),
                disclosures=r.get("disclosures", []) or [],
                fix_hint=r.get("fix_hint", "") or "",
                sources=srcs
            )
            rule.compiled = [re.compile(p, flags=re.IGNORECASE) for p in rule.patterns]
            self._policy_rules.append(rule)

        # soft_fixes
        for sf in data.get("soft_fixes", []) or []:
            s = SoftFixRule(pattern=sf["pattern"], replacement=sf["replacement"])
            s.compiled = re.compile(s.pattern, flags=re.IGNORECASE)
            self._soft_fix_rules.append(s)

        # glossary
        glossary_file = ((data.get("terminology") or {}).get("normalization") or {}).get("glossary_file")
        # ê²½ë¡œê°€ ìƒëŒ€ê²½ë¡œë©´ guardrails í´ë” ê¸°ì¤€ìœ¼ë¡œ ë³´ì •
        if glossary_file and not os.path.isabs(glossary_file):
            glossary_file = os.path.join(os.path.dirname(policy_path), os.path.basename(glossary_file))
        if glossary_file and os.path.exists(glossary_file):
            with open(glossary_file, "r", encoding="utf-8") as gf:
                g = yaml.safe_load(gf) or {}
            self._glossary_terms = g.get("terms") or []
            self._glossary_regex_terms = g.get("regex_terms") or []
            self._glossary_opts = g.get("options") or {}
            flags = re.IGNORECASE if self._glossary_opts.get("case_insensitive", True) else 0
            self._glossary_regex_compiled = [
                re.compile(item["pattern"], flags=flags) for item in (self._glossary_regex_terms or [])
            ]

        logger.info(f"[GUARD] loaded: rules={len(self._policy_rules)}, soft_fixes={len(self._soft_fix_rules)}, glossary_terms={len(self._glossary_terms)}")

    # ------------------------- Guardrail: Node (inline) -------------------------
    def _guardrails_slim_inline(self, state: RAGState) -> RAGState:
        """
        ìµœì¢… ê°€ë“œë ˆì¼(ì¸ë¼ì¸ ë²„ì „: í—¬í¼ í˜¸ì¶œ ì—†ìŒ)
          - ê·œì¹™(YAML: self._policy_rules)ìœ¼ë¡œ ì‘ë‹µ ê²€ì‚¬ â†’ HIGH ìˆìœ¼ë©´ BLOCK
          - ì†Œí”„íŠ¸ ì¹˜í™˜(self._soft_fix_rules) ì ìš©
          - ìš©ì–´ í‘œì¤€í™”(self._glossary_terms / self._glossary_regex_compiled) ì ìš©
          - ì¶œì²˜/ì •í™•ì„±/ëˆ„ë½/ê°•ì¡° ë“±ì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ
        """
        # (ì„ íƒ) ë¡œë”ë¥¼ ì´ë¯¸ í˜¸ì¶œí–ˆìœ¼ë¯€ë¡œ ì¶”ê°€ ë¦¬ë¡œë“œëŠ” ìƒëµ

        resp = (state.get("response") or "").strip()
        if not resp:
            return {
                **state,
                "guardrail_decision": "PASS",
                "violations": [],
                "compliant_response": state.get("response", "")
            }

        # 1) ê·œì¹™ ë§¤ì¹­
        violations = []
        for rule in (self._policy_rules or []):
            for pat in (rule.compiled or []):
                m = pat.search(resp)
                if not m:
                    continue
                violations.append({
                    "phase": "post",
                    "policy": getattr(rule, "policy", "INTERNAL"),
                    "rule_id": getattr(rule, "rule_id", "UNKNOWN"),
                    "severity": getattr(rule, "severity", "MEDIUM"),
                    "evidence": m.group(0),
                    "fix_hint": getattr(rule, "fix_hint", ""),
                    "sources": [
                        {"file": s.file, "clause": s.clause}
                        for s in (getattr(rule, "sources", []) or [])
                    ],
                })

        # HIGH ìœ„ë°˜ â†’ BLOCK
        if any(v.get("severity") == "HIGH" for v in violations):
            safe = (
                "ì£„ì†¡í•˜ì§€ë§Œ í•´ë‹¹ ì§ˆë¬¸ì€ ë‚´ë¶€ ê¸°ì¤€ìƒ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤."
            )
            return {
                **state,
                "guardrail_decision": "BLOCK",
                "violations": violations,
                "response": safe,
                "compliant_response": safe,
                "sources": state.get("sources", [])
            }

        # 2) ì†Œí”„íŠ¸ ì¹˜í™˜
        fixed = resp
        soft_changed = False
        for sf in (self._soft_fix_rules or []):
            try:
                compiled = getattr(sf, "compiled", None)
                replacement = getattr(sf, "replacement", "")
                if compiled and compiled.search(fixed):
                    fixed = compiled.sub(replacement, fixed)
                    soft_changed = True
            except Exception:
                continue

        # 3) ìš©ì–´ í‘œì¤€í™”
        # 3-1) terms: from â†’ to
        terms = self._glossary_terms or []
        if terms:
            ci = bool((self._glossary_opts or {}).get("case_insensitive", True))
            wb = bool((self._glossary_opts or {}).get("word_boundary", True))
            flags = re.IGNORECASE if ci else 0
            for t in terms:
                src = t.get("from")
                dst = t.get("to")
                if not src or not dst:
                    continue
                pat = re.escape(src)
                if wb:
                    pat = rf"\b{pat}\b"
                try:
                    fixed_new = re.sub(pat, dst, fixed, flags=flags)
                    if fixed_new != fixed:
                        fixed = fixed_new
                        soft_changed = True
                except Exception:
                    continue

        # 3-2) regex_terms: pattern â†’ replacement
        regex_compiled = self._glossary_regex_compiled or []
        regex_terms = self._glossary_regex_terms or []
        if regex_compiled and regex_terms:
            for idx, pat in enumerate(regex_compiled):
                try:
                    repl = ""
                    if idx < len(regex_terms):
                        repl = regex_terms[idx].get("replacement", "") or ""
                    fixed_new = pat.sub(repl, fixed)
                    if fixed_new != fixed:
                        fixed = fixed_new
                        soft_changed = True
                except Exception:
                    continue

        decision = "SOFT_FIX" if (soft_changed or violations) else "PASS"

        return {
            **state,
            "guardrail_decision": decision,
            "violations": violations,
            "response": fixed,
            "compliant_response": fixed,
        }

    # ------------------------- ê¸°ì¡´ ë¡œì§ -------------------------
    def _session_init(self, state: RAGState) -> RAGState:
        """ì„¸ì…˜ ì´ˆê¸°í™” ë° ê´€ë¦¬"""
        try:
            session_id = state.get("session_context", {}).session_id if state.get("session_context") else None
            log_node_start("session_init", session_id)
            
            if not session_id:
                # ìƒˆ ì„¸ì…˜ ìƒì„±
                session_context = session_manager.create_session()
                logger.info(f"[GRAPH] Created new session: {session_context.session_id}")
            else:
                # ê¸°ì¡´ ì„¸ì…˜ ì¡°íšŒ
                session_context = session_manager.get_session(session_id)
                if not session_context:
                    # ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
                    session_context = session_manager.create_session(session_id)
                    logger.info(f"[GRAPH] Recreated expired session: {session_id}")
                else:
                    logger.info(f"[GRAPH] Retrieved existing session: {session_id}")
            
            # í„´ ID ìƒì„±
            turn_id = f"turn_{int(time.time())}_{hash(str(time.time())) % 10000}"
            
            result = {
                **state,
                "session_context": session_context,
                "turn_id": turn_id,
                "conversation_history": session_manager.get_conversation_history(session_context.session_id, limit=5)
            }
            log_node_complete("session_init", session_context.session_id)
            return result
            
        except Exception as e:
            logger.error(f"[GRAPH] Session init failed: {e}")
            # í´ë°±: ê¸°ë³¸ ì„¸ì…˜ ìƒì„±
            session_context = session_manager.create_session()
            return {
                **state,
                "session_context": session_context,
                "turn_id": f"turn_{int(time.time())}",
                "conversation_history": []
            }

    def _context_analysis(self, state: RAGState) -> RAGState:
        """ëŒ€í™” ë§¥ë½ ë¶„ì„ ë° ë¼ìš°íŒ… ê²°ì •"""
        try:
            session_context = state.get("session_context")
            conversation_history = state.get("conversation_history", [])
            query = state.get("query", "")
            
            logger.info(f"[GRAPH] Context analysis - session_id: {session_context.session_id if session_context else 'None'}, conversation_history length: {len(conversation_history)}")
            
            if not session_context:
                logger.info(f"[GRAPH] No session context - routing to first_turn")
                return {**state, "context_route": "first_turn"}
            
            # ì²« í„´ì¸ì§€ í™•ì¸
            if not conversation_history:
                logger.info(f"[GRAPH] First turn detected for session: {session_context.session_id}")
                return {**state, "context_route": "first_turn"}
            else:
                logger.info(f"[GRAPH] Continue turn - conversation history exists: {len(conversation_history)} turns")
                logger.info(f"[GRAPH] Conversation history content: {[turn.turn_id for turn in conversation_history]}")
            
            # ì„ì‹œë¡œ í•­ìƒ first_turnìœ¼ë¡œ ë¼ìš°íŒ… (ë””ë²„ê¹…ìš©)
            logger.info(f"[GRAPH] FORCE ROUTING TO FIRST_TURN FOR DEBUGGING")
            return {**state, "context_route": "first_turn"}
            
            # ì´ì „ ëŒ€í™” ë§¥ë½ ë¶„ì„
            last_turn = conversation_history[-1]
            
            # ë§¥ë½ ê¸°ë°˜ ì˜ë„ ë¶„ì„
            context_intent = self._analyze_context_intent(query, last_turn, session_context)
            
            # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            session_manager.update_session(
                session_context.session_id,
                current_topic=self._extract_current_topic(query, last_turn),
                conversation_summary=session_manager.generate_conversation_summary(session_context.session_id)
            )
            
            logger.info(f"[GRAPH] Context analysis completed. Route: continue_turn, Intent: {context_intent}")
            return {
                **state,
                "context_route": "continue_turn",
                "context_intent": context_intent
            }
            
        except Exception as e:
            logger.error(f"[GRAPH] Context analysis failed: {e}")
            return {**state, "context_route": "first_turn"}

    def _analyze_context_intent(self, query: str, last_turn: ConversationTurn, session_context: SessionContext) -> str:
        """ë§¥ë½ ê¸°ë°˜ ì˜ë„ ë¶„ì„"""
        try:
            # ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê´€ì„± ë¶„ì„
            context_prompt = f"""
            ì´ì „ ëŒ€í™”:
            Q: {last_turn.user_query}
            A: {last_turn.ai_response[:200]}...
            
            í˜„ì¬ ì§ˆë¬¸: {query}
            
            í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ì–´ë–¤ ê´€ë ¨ì´ ìˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”:
            1. follow_up: ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ì„¸ë¶€ì‚¬í•­
            2. related: ê°™ì€ ì£¼ì œì˜ ìƒˆë¡œìš´ ì§ˆë¬¸
            3. new_topic: ì™„ì „íˆ ìƒˆë¡œìš´ ì£¼ì œ
            4. clarification: ì´ì „ ë‹µë³€ì— ëŒ€í•œ ëª…í™•í™” ìš”ì²­
            
            ë‹µë³€ í˜•ì‹: [ì˜ë„] - [ê°„ë‹¨í•œ ì„¤ëª…]
            """
            
            response = self.slm.generate_response(context_prompt)
            
            if "follow_up" in response.lower():
                return "follow_up"
            elif "related" in response.lower():
                return "related"
            elif "new_topic" in response.lower():
                return "new_topic"
            else:
                return "clarification"
                
        except Exception as e:
            logger.error(f"[GRAPH] Context intent analysis failed: {e}")
            return "related"

    def _extract_current_topic(self, query: str, last_turn: ConversationTurn) -> str:
        """í˜„ì¬ í† í”½ ì¶”ì¶œ"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í† í”½ ì¶”ì¶œ
            keywords = RAGUtils.extract_keywords_from_query(query)
            if keywords:
                return " ".join(keywords[:3])  # ìƒìœ„ 3ê°œ í‚¤ì›Œë“œ
            
            # ì´ì „ í† í”½ê³¼ì˜ ì—°ê´€ì„± í™•ì¸
            if last_turn.product_name:
                return last_turn.product_name
            
            return query[:50] + "..." if len(query) > 50 else query
            
        except Exception as e:
            logger.error(f"[GRAPH] Topic extraction failed: {e}")
            return query[:30] + "..." if len(query) > 30 else query

    def _route_by_context(self, state: RAGState) -> str:
        """ë§¥ë½ ê¸°ë°˜ ë¼ìš°íŒ…"""
        return state.get("context_route", "first_turn")

    def _save_conversation(self, state: RAGState) -> RAGState:
        """ëŒ€í™” í„´ ì €ì¥"""
        try:
            session_context = state.get("session_context")
            turn_id = state.get("turn_id")
            
            if not session_context or not turn_id:
                return state
            
            # ëŒ€í™” í„´ ìƒì„±
            conversation_turn = ConversationTurn(
                turn_id=turn_id,
                timestamp=datetime.now(),
                user_query=state.get("query", ""),
                ai_response=state.get("response", ""),
                category=state.get("category", ""),
                product_name=state.get("product_name", ""),
                sources=state.get("sources", []),
                session_context=session_context.to_dict()
            )
            
            # ì„¸ì…˜ ë§¤ë‹ˆì €ì— ì €ì¥
            session_manager.add_conversation_turn(session_context.session_id, conversation_turn)
            
            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            session_manager.add_message(session_context.session_id, HumanMessage(content=state.get("query", "")))
            session_manager.add_message(session_context.session_id, AIMessage(content=state.get("response", "")))
            
            logger.info(f"[GRAPH] Saved conversation turn: {turn_id}")
            return state
            
        except Exception as e:
            logger.error(f"[GRAPH] Save conversation failed: {e}")
            return state

    def _first_turn_preprocess(self, state: RAGState) -> RAGState:
        """ì„¸ì…˜ì˜ ì²« ì§ˆë¬¸ì— ëŒ€í•´ì„œë§Œ ì‹¤í–‰ë˜ëŠ” ì „ì²˜ë¦¬ ë…¸ë“œ(ê°•í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬).

        ë™ì‘:
        - ì„¸ì…˜ ì²« ì§ˆë¬¸ì¸ì§€ í™•ì¸ (conversation_history)
        - routerë¡œ intent íšë“(initial_intent) â€” ì•ˆì „í•œ ì˜ˆì™¸ ì²˜ë¦¬
        - SLMìœ¼ë¡œ ì±—ë´‡ ì„¸ì…˜ìš© ì§§ì€ ì œëª© ìƒì„±(initial_topic_summary)
          (SLM í˜¸ì¶œ ì‹¤íŒ¨ì— ëŒ€í•´ì„œë§Œ ìµœì†Œ í´ë°± ì²˜ë¦¬)
        """
        # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
        session_context = state.get("session_context")
        conversation_history = state.get("conversation_history", [])
        
        logger.info(f"[GRAPH] First turn preprocess - conversation_history length: {len(conversation_history)}")
        
        # ì²« í„´ì¸ì§€ í™•ì¸ (conversation_historyê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ í„´ì´ ì²« ë²ˆì§¸ì¸ ê²½ìš°)
        is_first_turn = not conversation_history or len(conversation_history) == 0
        
        if not is_first_turn:
            logger.info(f"[GRAPH] Skipping first turn preprocess - conversation already exists")
            return state

        query = state.get("query", "") or ""
        
        # ì œëª© ìƒì„±: LLM í˜¸ì¶œ (ìµœëŒ€ 15ì ì œí•œ)
        logger.info(f"[GRAPH] FIRST_TURN_PREPROCESS EXECUTING - Query: {query[:50]}...")
        
        system_message = SystemMessage(
            "ë‹¹ì‹ ì€ 'ì±—ë´‡ ì„¸ì…˜ ì œëª© ìƒì„±ê¸°'ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³  "
            "ì´ ì„¸ì…˜ì„ ëŒ€í‘œí•  ë§¤ìš° ê°„ê²°í•œ í•œ ì¤„ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”."
        )
        human_prompt = (
            f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
            "ì¶œë ¥: ì œëª© í…ìŠ¤íŠ¸ë§Œ í•œ ì¤„ë¡œ ì¶œë ¥\n"
            "- í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ (ìµœëŒ€ 15ì)\n"
            "- ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ ì œëª©ë§Œ ë°˜í™˜\n"
        )
        messages = [system_message, HumanMessage(human_prompt)]

        session_title = ""
        MAX_TITLE_CHARS = 15  # 15ìë¡œ ì œí•œ

        try:
            logger.info(f"[GRAPH] Generating title for query: {query[:50]}...")
            raw = (self.slm.invoke(messages) or "").strip()
            logger.info(f"[GRAPH] Raw title response: {raw[:100]}...")
            if raw:
                session_title = raw.splitlines()[0].strip()
                if len(session_title) > MAX_TITLE_CHARS:
                    session_title = session_title[:MAX_TITLE_CHARS].rstrip()
                logger.info(f"[GRAPH] Generated title: {session_title}")
            else:
                logger.warning(f"[GRAPH] Empty title response from SLM")
        except Exception as e:
            # ìµœì†Œ í´ë°±: intent ë˜ëŠ” ì§ˆë¬¸ì˜ ì•ë¶€ë¶„ ì‚¬ìš©
            logger.warning(f"[GRAPH] first_turn_preprocess: title generation failed: {e}")
            session_title = ""

        if not session_title:
            # LLM ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼ì¼ ë•Œ í´ë°±: intent ìš°ì„ , ì—†ìœ¼ë©´ ì§ˆë¬¸ ì•ë¶€ë¶„
            session_title = initial_intent or (query.strip().splitlines()[0][:MAX_TITLE_CHARS] or "ìƒˆë¡œìš´ ì§ˆë¬¸")
            logger.info(f"[GRAPH] Using fallback title: {session_title}")
        
        # ìµœì¢… ë³´ì¥: ì œëª©ì´ ì ˆëŒ€ ë¹„ì–´ìˆì§€ ì•Šë„ë¡
        if not session_title or session_title.strip() == "":
            session_title = "ìƒˆë¡œìš´ ì§ˆë¬¸"
            logger.info(f"[GRAPH] Final fallback title: {session_title}")
        
        # 15ì ì œí•œ ì ìš©
        if len(session_title) > MAX_TITLE_CHARS:
            session_title = session_title[:MAX_TITLE_CHARS].rstrip()
        
        
        # Router í˜¸ì¶œ - ì•ˆì „í•œ ì˜ˆì™¸ ì²˜ë¦¬
        try:
            initial_intent = self.router.route_prompt(query)
            if not initial_intent or not initial_intent.strip():
                logger.warning(f"[GRAPH] Router returned empty intent for query: {query[:100]}...")
                initial_intent = "unknown"
            else:
                initial_intent = initial_intent.strip()
        except Exception as e:
            logger.error(f"[GRAPH] Router failed for query '{query[:100]}...': {e}")
            initial_intent = "unknown"

        # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        if session_context:
            session_manager.update_session(
                session_context.session_id,
                initial_intent=initial_intent,
                session_title=session_title
            )
        
        logger.info(
            f"[GRAPH] First-turn preprocess done. session_id={session_context.session_id if session_context else 'unknown'}, "
            f"intent={initial_intent!r}, session_title={session_title!r}"
        )

        return {
            **state,
            "initial_intent": initial_intent,
            "initial_topic_summary": session_title,
        }

    def _classify_intent(self, state: RAGState) -> RAGState:
        """ì¸í…íŠ¸ ë¶„ë¥˜ ë…¸ë“œ - ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬ (ì œëª© ìƒì„± í¬í•¨)"""
        query = state["query"]
        session_context = state.get("session_context")
        conversation_history = state.get("conversation_history", [])
        
        try:
            category = self.router.route_prompt(query)
            if not category or not category.strip():
                logger.warning(f"[GRAPH] Router returned empty category for query: {query[:100]}...")
                category = "unknown"
            else:
                category = category.strip()
        except Exception as e:
            logger.error(f"[GRAPH] Intent classification failed for query '{query[:100]}...': {e}")
            category = "unknown"

        # ì²« í„´ì¸ ê²½ìš° ì œëª© ìƒì„±ì€ run_workflowì—ì„œ ì²˜ë¦¬
        session_title = ""

        logger.info(f"[GRAPH] Classified category: {category}")

        return {
            **state,
            "category": category,
            "initial_intent": category,
            "initial_topic_summary": session_title
        }

    def _route_by_category(self, state: RAGState) -> str:
        """ì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ë¼ìš°íŒ… ê²°ì •"""
        category = state["category"]

        if category == GENERAL_FAQ_CATEGORY:
            return "general_faq"
        elif category in [COMPANY_PRODUCTS_CATEGORY, COMPANY_RULES_CATEGORY, INDUSTRY_POLICY_CATEGORY]:
            return "rag_needed"
        else:
            return "rag_needed"  # ê¸°ë³¸ì ìœ¼ë¡œ RAG ì‚¬ìš©

    def _handle_general_faq(self, state: RAGState) -> RAGState:
        """ì¼ë°˜ FAQ ì²˜ë¦¬ ë…¸ë“œ - ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬"""
        query = state["query"]

        # ì¼ë°˜ FAQìš© ì‹œìŠ¤í…œ ë©”ì‹œì§€
        system_message = SystemMessage("""ë‹¹ì‹ ì€ KBê¸ˆìœµê·¸ë£¹ì˜ ê³ ê° ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1) ì¼ë°˜ì ì¸ ê¸ˆìœµ ìƒì‹ì´ë‚˜ KBê¸ˆìœµê·¸ë£¹ì˜ ê¸°ë³¸ ì •ë³´ì— ëŒ€í•´ ì¹œê·¼í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2) ë³µì¡í•œ ê¸ˆìœµ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
3) êµ¬ì²´ì ì¸ ìƒí’ˆ ì •ë³´ë‚˜ ê·œì •ì´ í•„ìš”í•œ ê²½ìš°, "ìƒì„¸í•œ ìƒë‹´ì„ ìœ„í•´ KBê¸ˆìœµê·¸ë£¹ì— ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
4) ê³ ê°ì˜ ìƒí™©ì— ë§ëŠ” ì¼ë°˜ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ë˜, ê°œì¸ ë§ì¶¤ ìƒë‹´ì€ ë³„ë„ ì•ˆë‚´í•˜ì„¸ìš”.
5) í•­ìƒ ì •ì¤‘í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì–´ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
6) ë‹µë³€ì€ 5ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.""")
        messages = [system_message, HumanMessage(query)]
        
        try:
            response = self.slm.invoke(messages)
            if not response or not response.strip():
                logger.warning(f"[GRAPH] SLM returned empty response for general FAQ")
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        except Exception as e:
            logger.error(f"[GRAPH] SLM failed for general FAQ: {e}")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        logger.info(f"[GRAPH] General FAQ response generated")

        return {
            **state,
            "messages": state.get("messages", []) + messages,
            "response": response,
            "sources": []
        }

    def _extract_product_name(self, state: RAGState) -> RAGState:
        """ìƒí’ˆëª… ì¶”ì¶œ ë…¸ë“œ"""
        query = state["query"]
        product_name = self._extract_product_name_from_question(query)

        logger.info(f"[GRAPH] Extracted product name: '{product_name}'")

        return {
            **state,
            "product_name": product_name
        }

    def _search_documents(self, state: RAGState) -> RAGState:
        """ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
        query = state["query"]
        product_name = state.get("product_name", "")
        category = state["category"]

        self.vector_store.get_index_ready()
        raw_retrieved = []

        # 1ì°¨: ì§ˆë¬¸ í‚¤ì›Œë“œë¡œ ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­ (ìµœìš°ì„ )
        try:
            query_keywords = RAGUtils.extract_keywords_from_query(query)
            exact_filename = self._find_exact_filename_match(query_keywords)

            if exact_filename:
                raw_retrieved = self.vector_store.similarity_search_by_filename(query, exact_filename)
                logger.info(f"[GRAPH] Exact filename match: {exact_filename}")
        except Exception as e:
            logger.warning(f"[GRAPH] Filename matching failed: {e}")
            # íŒŒì¼ëª… ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ê³„ì† ì§„í–‰

        # 2ì°¨: ìƒí’ˆëª… ê¸°ë°˜ ê²€ìƒ‰ (ì •í™•í•œ íŒŒì¼ëª… ë§¤ì¹­ì´ ì‹¤íŒ¨í•œ ê²½ìš°ë§Œ)
        if not raw_retrieved and product_name:
            try:
                # 1ì°¨: íŒŒì¼ëª… ì •í™• ë§¤ì¹­
                filename_with_underscores = product_name.replace(" ", "_") + ".pdf"
                raw_retrieved = self.vector_store.similarity_search_by_filename(query, filename_with_underscores)

                if not raw_retrieved:
                    # 2ì°¨: í‚¤ì›Œë“œ ê²€ìƒ‰
                    keywords = RAGUtils.extract_keywords_from_product_name(product_name)
                    raw_retrieved = self.vector_store.similarity_search_by_keywords(query, keywords)

                    if not raw_retrieved:
                        # 3ì°¨: ì¼ë°˜ ê²€ìƒ‰
                        raw_retrieved = self.vector_store.similarity_search(query)
            except Exception as e:
                logger.warning(f"[GRAPH] Product name search failed: {e}")
                raw_retrieved = self.vector_store.similarity_search(query)
        else:
            # 3ì°¨: ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ë˜ëŠ” ì¼ë°˜ ê²€ìƒ‰
            try:
                if category == COMPANY_PRODUCTS_CATEGORY:
                    raw_retrieved = self.vector_store.similarity_search_by_folder(query, MAIN_PRODUCT)
                elif category == COMPANY_RULES_CATEGORY:
                    raw_retrieved = self.vector_store.similarity_search_by_folder(query, MAIN_RULE)
                elif category == INDUSTRY_POLICY_CATEGORY:
                    raw_retrieved = self.vector_store.similarity_search_by_folder(query, MAIN_LAW)
                else:
                    raw_retrieved = self.vector_store.similarity_search(query)
            except Exception as e:
                logger.warning(f"[GRAPH] Category search failed: {e}")
                raw_retrieved = self.vector_store.similarity_search(query)

        # ë¬¸ì„œ ì •ê·œí™”
        retrieved_docs = RAGUtils.normalize_retrieved(raw_retrieved)

        logger.info(f"[GRAPH] Retrieved {len(retrieved_docs)} documents")

        return {
            **state,
            "retrieved_docs": retrieved_docs
        }

    def _find_exact_filename_match(self, query_keywords: List[str]) -> str:
        """ì§ˆë¬¸ í‚¤ì›Œë“œë¡œ ì •í™•í•œ íŒŒì¼ëª… ì°¾ê¸° (ë‹¨ìˆœí™”ëœ ë°©ì‹)"""
        try:
            # ìºì‹œëœ íŒŒì¼ëª… ì¸ë±ìŠ¤ ì‚¬ìš©
            if self._filename_index is None:
                self._build_filename_index()

            if not self._filename_index:
                return ""

            best_match = ""
            max_score = 0.0
            min_threshold = 0.3

            # ê°„ë‹¨í•œ ë§¤ì¹­ ë¡œì§
            for filename, file_keywords in self._filename_index.items():
                score = RAGUtils.calculate_keyword_match_score(query_keywords, file_keywords)
                if score > max_score and score >= min_threshold:
                    max_score = score
                    best_match = filename

            return best_match if max_score >= min_threshold else ""

        except Exception as e:
            logger.error(f"[GRAPH] Error in filename matching: {e}")
            return ""

    def _build_filename_index(self) -> None:
        """íŒŒì¼ëª… ì¸ë±ìŠ¤ êµ¬ì¶• (ë‹¨ìˆœí™”ëœ ë²„ì „)"""
        try:
            available_files = self.vector_store.get_available_files()
            if not available_files:
                self._filename_index = {}
                return

            self._filename_index = {
                filename: RAGUtils.extract_keywords_from_filename(filename)
                for filename in available_files
            }

        except Exception as e:
            logger.error(f"[GRAPH] Error building filename index: {e}")
            self._filename_index = {}

# ----------------------------------------------------------------------------------------------------------------------------

    def _filter_relevance(self, state: RAGState) -> RAGState:
        """ê´€ë ¨ì„± í•„í„°ë§ ë…¸ë“œ"""
        docs = state["retrieved_docs"]
        query = state["query"]

        filtered_docs = RAGUtils.filter_by_relevance_score(docs, query)
        context_text = RAGUtils.format_context(filtered_docs)

        logger.info(f"[GRAPH] Filtered to {len(filtered_docs)} relevant documents")

        return {
            **state,
            "retrieved_docs": filtered_docs,
            "context_text": context_text
        }

    def _generate_response(self, state: RAGState) -> RAGState:
        """ì‘ë‹µ ìƒì„± ë…¸ë“œ"""
        query = state["query"]
        context_text = state["context_text"]
        retrieved_docs = state["retrieved_docs"]
        category = state["category"]

        if not context_text.strip():
            return {
                **state,
                "response": NO_ANSWER_MSG,
                "sources": []
            }

        # ì¹´í…Œê³ ë¦¬ë³„ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±
        system_message = self._build_category_specific_system_message(category, context_text)
        messages = [system_message, HumanMessage(query)]

        response = self.slm.invoke(messages)

        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ - ìˆ˜ì •ëœ ë¶€ë¶„
        sources = []
        for doc in retrieved_docs:
            metadata = doc.metadata or {}
            # ì‹¤ì œ ë¬¸ì„œ ë‚´ìš©ë„ í¬í•¨
            source_info = dict(metadata)
            source_info['text'] = doc.page_content  # ì‹¤ì œ ë¬¸ì„œ ë‚´ìš© ì¶”ê°€
            source_info['page_content'] = doc.page_content  # í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€
            sources.append(source_info)

        logger.info(f"[GRAPH] Generated response with {len(sources)} sources")

        return {
            **state,
            "messages": state.get("messages", []) + messages,
            "response": response,
            "sources": sources
        }

    def run_workflow(self, query: str, session_id: str = None) -> Dict[str, Any]:
        """ë©€í‹°í„´ ëŒ€í™” ì§€ì› ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        try:
            # ê¸°ì¡´ ì„¸ì…˜ ì¡°íšŒ ë˜ëŠ” ìƒˆ ì„¸ì…˜ ìƒì„±
            if session_id:
                session_context = session_manager.get_session(session_id)
                if not session_context:
                    session_context = session_manager.create_session(session_id)
            else:
                session_context = session_manager.create_session()
            
            # ì´ˆê¸° ìƒíƒœ ì„¤ì • (ë©€í‹°í„´ ì§€ì›)
            initial_state = {
                "messages": [],
                "query": query,
                "category": "",
                "product_name": "",
                "retrieved_docs": [],
                "context_text": "",
                "response": "",
                "sources": [],
                "session_context": session_context,
                "conversation_history": [],
                "turn_id": "",
                "guardrail_decision": "",
                "violations": [],
                "compliant_response": "",
            }

            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = self.workflow.invoke(initial_state)

            # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            updated_session = final_state.get("session_context")
            if updated_session:
                session_manager.update_session(
                    updated_session.session_id,
                    last_activity=datetime.now()
                )

            # ì œëª© ìƒì„± (ì²« í„´ì¸ ê²½ìš°)
            conversation_history = session_manager.get_conversation_history(session_context.session_id)
            initial_topic_summary = final_state.get("initial_topic_summary", "")
            
            # LLMìœ¼ë¡œ ì œëª© ìƒì„± (ì²« í„´ì¸ ê²½ìš°)
            if not conversation_history and not initial_topic_summary:
                logger.info(f"[GRAPH] Generating title with LLM for first turn")
                
                try:
                    # LLMìœ¼ë¡œ ì œëª© ìƒì„±
                    system_message = SystemMessage(
                        "ë‹¹ì‹ ì€ 'ì±—ë´‡ ì„¸ì…˜ ì œëª© ìƒì„±ê¸°'ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³  "
                        "ì´ ì„¸ì…˜ì„ ëŒ€í‘œí•  ë§¤ìš° ê°„ê²°í•œ í•œ ì¤„ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”."
                    )
                    human_prompt = (
                        f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
                        "ì¶œë ¥: ì œëª© í…ìŠ¤íŠ¸ë§Œ í•œ ì¤„ë¡œ ì¶œë ¥\n"
                        "- í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ (ë°˜ë“œì‹œ 15ì ì´í•˜)\n"
                        "- ë¶ˆí•„ìš”í•œ ì„¤ëª… ì—†ì´ ì œëª©ë§Œ ë°˜í™˜\n"
                        "- ì˜ˆì‹œ: 'í–‡ì‚´ë¡  ë¬¸ì˜', 'ëŒ€ì¶œ ë¬¸ì˜', 'ìƒí’ˆ ì•ˆë‚´'\n"
                        "- ì¤‘ìš”: 15ìë¥¼ ì´ˆê³¼í•˜ë©´ ì•ˆë©ë‹ˆë‹¤!\n"
                    )
                    messages = [system_message, HumanMessage(human_prompt)]
                    
                    raw_title = (self.slm.invoke(messages) or "").strip()
                    if raw_title:
                        initial_topic_summary = raw_title.splitlines()[0].strip()
                        # 15ì ì œí•œ ê°•ì œ ì ìš©
                        if len(initial_topic_summary) > 15:
                            initial_topic_summary = initial_topic_summary[:15].rstrip()
                            logger.info(f"[GRAPH] Title truncated to 15 chars: {initial_topic_summary}")
                    else:
                        # LLM ì‹¤íŒ¨ì‹œ í´ë°±
                        initial_topic_summary = query[:15] if len(query) > 15 else query
                    
                    # ì„¸ì…˜ì— ì œëª© ì €ì¥
                    session_manager.update_session(
                        session_context.session_id,
                        session_title=initial_topic_summary
                    )
                    
                    logger.info(f"[GRAPH] LLM generated title: {initial_topic_summary}")
                    
                except Exception as e:
                    logger.error(f"[GRAPH] LLM title generation failed: {e}")
                    # í´ë°±: ì§ˆë¬¸ ì•ë¶€ë¶„ ì‚¬ìš©
                    initial_topic_summary = query[:15] if len(query) > 15 else query
                    session_manager.update_session(
                        session_context.session_id,
                        session_title=initial_topic_summary
                    )

            # ì‘ë‹µ êµ¬ì„±
            response_data = {
                "response": final_state["response"],
                "sources": final_state["sources"],
                "category": final_state.get("category", "unknown"),
                "product_name": final_state.get("product_name", ""),
                "session_info": {
                    "session_id": updated_session.session_id if updated_session else session_context.session_id,
                    "initial_intent": final_state.get("initial_intent", ""),
                    "initial_topic_summary": initial_topic_summary,
                    "conversation_mode": updated_session.conversation_mode if updated_session else "normal",
                    "current_topic": updated_session.current_topic if updated_session else "",
                    "active_product": updated_session.active_product if updated_session else "",
                },
                # í˜¸í™˜ì„± ìœ ì§€
                "initial_intent": final_state.get("initial_intent", ""),
                "initial_topic_summary": initial_topic_summary,
                "guardrail": {
                    "decision": final_state.get("guardrail_decision", ""),
                    "violations": final_state.get("violations", []),
                }
            }
            
            logger.info(f"[GRAPH] Workflow completed for session: {session_context.session_id}")
            return response_data
            
        except Exception as e:
            logger.error(f"[GRAPH] Workflow execution failed: {e}")
            return {
                "response": "ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "category": "error",
                "product_name": "",
                "session_info": {
                    "session_id": session_id or "error",
                    "initial_intent": "",
                    "initial_topic_summary": "",
                    "conversation_mode": "error",
                    "current_topic": "",
                    "active_product": "",
                },
                "initial_intent": "",
                "initial_topic_summary": "",
                "guardrail": {
                    "decision": "error",
                    "violations": [],
                }
            }

    # ê¸°ì¡´ orchestratorì˜ í—¬í¼ ë©”ì„œë“œë“¤ ë³µì‚¬
    # í”„ë¡¬í”„íŠ¸ ì¢€ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±.
    def _extract_product_name_from_question(self, question: str) -> str:
        """ì§ˆë¬¸ì—ì„œ ìƒí’ˆëª…ì„ ì¶”ì¶œí•˜ëŠ” ë©”ì„œë“œ (ê¸°ì¡´ orchestratorì™€ ë™ì¼)"""
        try:
            extraction_prompt = f"""
                ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì§ˆë¬¸ìì˜ ì˜ë„ì™€ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ KBê¸ˆìœµê·¸ë£¹ ìƒí’ˆëª…ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
                ì§ˆë¬¸: {question}

                ê·œì¹™:
                1) ì§ˆë¬¸ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ìƒí’ˆëª…ë§Œ ì¶”ì¶œ
                2) ì§ˆë¬¸ì˜ ë§¥ë½ê³¼ ê´€ë ¨ ì—†ëŠ” ìƒí’ˆëª…ì€ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ
                3) ìƒí’ˆëª…ì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
                4) ì˜ˆì‹œëŠ” ì°¸ê³ ìš©ì¼ ë¿, ì§ˆë¬¸ê³¼ ë¬´ê´€í•œ ìƒí’ˆëª… ì¶”ì¶œ ê¸ˆì§€
            """

            product_response = self.slm.get_structured_output(
                extraction_prompt,
                ProductNameResponse
            )
            return product_response.product_name.strip()
        except Exception as e:
            logger.error(f"[GRAPH] Failed to extract product name: {e}")
            return ""


    def _build_category_specific_system_message(self, category: str, context_text: str) -> SystemMessage:
        """ì¹´í…Œê³ ë¦¬ë³„ ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ê¸°ì¡´ orchestratorì™€ ë™ì¼)"""
        if category == COMPANY_PRODUCTS_CATEGORY:
            system_prompt = """ë‹¹ì‹ ì€ KBê¸ˆìœµê·¸ë£¹ì˜ ê¸ˆìœµìƒí’ˆ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1) ì œê³µëœ <ê²€ìƒ‰ëœ_ë¬¸ì„œ>ëŠ” KBê¸ˆìœµê·¸ë£¹ì˜ ê³µì‹ ìƒí’ˆ ì •ë³´ì…ë‹ˆë‹¤.
2) ìƒí’ˆì˜ íŠ¹ì§•, ì¡°ê±´, ê¸ˆë¦¬, í•œë„, ì‹ ì²­ë°©ë²• ë“±ì„ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.
3) ê³ ê°ì´ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”.
4) ìƒí’ˆ ë¹„êµë‚˜ ì¶”ì²œì´ í•„ìš”í•œ ê²½ìš°, ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
5) ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” "ì¶”ê°€ ìƒë‹´ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
6) ê°€ëŠ¥í•œ ê²½ìš° ê´€ë ¨ ìƒí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ë„ í•¨ê»˜ ì•ˆë‚´í•˜ì„¸ìš”.
7) ë‹µë³€ì€ 5ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

<ê²€ìƒ‰ëœ_ë¬¸ì„œ>
{context}
</ê²€ìƒ‰ëœ_ë¬¸ì„œ>"""

        elif category == COMPANY_RULES_CATEGORY:
            system_prompt = """ë‹¹ì‹ ì€ KBê¸ˆìœµê·¸ë£¹ì˜ ë‚´ë¶€ ê·œì • ë° ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1) ì œê³µëœ <ê²€ìƒ‰ëœ_ë¬¸ì„œ>ëŠ” KBê¸ˆìœµê·¸ë£¹ì˜ ê³µì‹ ë‚´ë¶€ ê·œì •ê³¼ ì •ì±…ì…ë‹ˆë‹¤.
2) ê·œì •ì˜ ëª©ì , ì ìš© ë²”ìœ„, ì„¸ë¶€ ì¡°ê±´, ì ˆì°¨ ë“±ì„ ì •í™•í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
3) ë³µì¡í•œ ê·œì •ì€ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
4) ê´€ë ¨ ë²•ë ¹ì´ë‚˜ ìƒìœ„ ê·œì •ê³¼ì˜ ê´€ê³„ë„ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”.
5) ê·œì • í•´ì„ì— ì• ë§¤í•¨ì´ ìˆì„ ê²½ìš°, ê°€ëŠ¥í•œ í•´ì„ì„ ëª¨ë‘ ì œì‹œí•˜ì„¸ìš”.
6) ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ì˜ˆì™¸ì‚¬í•­ì€ "ë³„ë„ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
7) ë‹µë³€ì€ 5ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

<ê²€ìƒ‰ëœ_ë¬¸ì„œ>
{context}
</ê²€ìƒ‰ëœ_ë¬¸ì„œ>"""

        elif category == INDUSTRY_POLICY_CATEGORY:
            system_prompt = """ë‹¹ì‹ ì€ ê¸ˆìœµì—…ê³„ ì •ì±… ë° ë²•ê·œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1) ì œê³µëœ <ê²€ìƒ‰ëœ_ë¬¸ì„œ>ëŠ” ê¸ˆìœµì—…ê³„ ê´€ë ¨ ë²•ë¥ , ì •ì±…, ê·œì œ ì •ë³´ì…ë‹ˆë‹¤.
2) ë²•ë ¹ì˜ ëª©ì , ì£¼ìš” ë‚´ìš©, ì ìš© ëŒ€ìƒ, ì‹œí–‰ ì‹œê¸° ë“±ì„ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
3) ê¸ˆìœµê¸°ê´€ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ì¤€ìˆ˜í•´ì•¼ í•  ì‚¬í•­ì„ êµ¬ì²´ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.
4) ê´€ë ¨ ë²•ë ¹ ê°„ì˜ ê´€ê³„ë‚˜ ê°œì • ì‚¬í•­ì´ ìˆë‹¤ë©´ í•¨ê»˜ ì„¤ëª…í•˜ì„¸ìš”.
5) ë²•ë ¹ í•´ì„ì´ ë³µì¡í•œ ê²½ìš°, í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë¨¼ì € ì œì‹œí•œ í›„ ì„¸ë¶€ì‚¬í•­ì„ ì„¤ëª…í•˜ì„¸ìš”.
6) ì‹¤ë¬´ ì ìš© ì‹œ ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì˜ˆì™¸ ì¡°ê±´ì´ ìˆë‹¤ë©´ ê°•ì¡°í•˜ì—¬ ì•ˆë‚´í•˜ì„¸ìš”.
7) ë‹µë³€ì€ 5ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

<ê²€ìƒ‰ëœ_ë¬¸ì„œ>
{context}
</ê²€ìƒ‰ëœ_ë¬¸ì„œ>"""
        else:
            # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€
            system_prompt = """ë‹¹ì‹ ì€ KBê¸ˆìœµê·¸ë£¹ì˜ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì§€ì¹¨:
1) ì œê³µëœ <ê²€ìƒ‰ëœ_ë¬¸ì„œ>ëŠ” KBê¸ˆìœµê·¸ë£¹ì˜ ê³µì‹ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ëœ ì •ë³´ì…ë‹ˆë‹¤.
2) ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
3) ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì•ˆë‚´í•˜ì„¸ìš”.
4) ë³µì¡í•œ ë‚´ìš©ì€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
5) ë‹µë³€ì€ 5ì¤„ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

<ê²€ìƒ‰ëœ_ë¬¸ì„œ>
{context}
</ê²€ìƒ‰ëœ_ë¬¸ì„œ>"""

        return SystemMessage(system_prompt.format(context=context_text))

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_langgraph_workflow = None

def get_langgraph_workflow() -> LangGraphRAGWorkflow:
    """LangGraph ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _langgraph_workflow
    if _langgraph_workflow is None:
        _langgraph_workflow = LangGraphRAGWorkflow()
    return _langgraph_workflow
