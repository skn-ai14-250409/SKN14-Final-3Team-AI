"""
LangGraph Nodes
==============
RAG ì›Œí¬í”Œë¡œìš°ì˜ ë…¸ë“œ í•¨ìˆ˜ë“¤
"""

import logging
import time
import re
from typing import Dict, List, Any
from functools import lru_cache

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.documents import Document

from .models import RAGState
from .session_manager import session_manager
from ..slm.slm import SLM
from .utils import (
    generate_session_title, 
    create_rag_response, 
    create_simple_response, 
    create_guardrail_response,
    extract_product_name,
    classify_product_subcategory,
    create_error_response,
    format_context,
    create_supervisor_prompt,
    get_prompt,
    get_error_message,
    get_cached_search_result,
    set_cached_search_result,
    DEFAULT_SEARCH_K,
)

# ========== Execution Path Tracking ==========

def track_execution_path(state: RAGState, node_name: str) -> RAGState:
    """ì‹¤í–‰ ê²½ë¡œë¥¼ ì¶”ì í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    execution_path = state.get("execution_path", [])
    execution_path.append(node_name)
    return {**state, "execution_path": execution_path}

def track_state_changes(state: RAGState, change_type: str, details: str = "") -> RAGState:
    """ìƒíƒœ ë³€ê²½ ì¶”ì """
    state_changes = state.get("state_changes", [])
    state_changes.append({
        "type": change_type,
        "details": details,
        "timestamp": time.time()
    })
    return {**state, "state_changes": state_changes}
    
from .utils import get_shared_slm, get_shared_vector_store

logger = logging.getLogger(__name__)

# ë¡œê¹… ë ˆë²¨ í™•ì¸ (ì„±ëŠ¥ ìµœì í™”ìš©)
DEBUG_MODE = logger.isEnabledFor(logging.DEBUG)
INFO_MODE = logger.isEnabledFor(logging.INFO)

# ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë¡œê¹… ì œì–´
def log_performance(operation: str, start_time: float, end_time: float, details: str = ""):
    """ì„±ëŠ¥ ë¡œê¹… í—¬í¼ í•¨ìˆ˜"""
    execution_time = end_time - start_time
    if execution_time > 1.0:  # 1ì´ˆ ì´ìƒì¸ ê²½ìš°ë§Œ ë¡œê¹…
        logger.info(f"â±ï¸ [PERFORMANCE] {operation}: {execution_time:.2f}ì´ˆ {details}")
    elif DEBUG_MODE:
        logger.debug(f"â±ï¸ [PERFORMANCE] {operation}: {execution_time:.2f}ì´ˆ {details}")

# ========== Utility Functions ==========

@lru_cache(maxsize=256)
def clean_markdown_formatting(text: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì œê±°í•˜ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìºì‹± ì ìš©)"""
    if not text:
        return text
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # **bold** -> bold
    text = re.sub(r'\*(.*?)\*', r'\1', text)      # *italic* -> italic
    text = re.sub(r'#+\s*', '', text)             # # headers -> headers
    text = re.sub(r'`(.*?)`', r'\1', text)        # `code` -> code
    text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)  # [text](url) -> text
    text = re.sub(r'^\s*[-*+]\s*', '', text, flags=re.MULTILINE)  # bullet points
    text = re.sub(r'^\s*\d+\.\s*', '', text, flags=re.MULTILINE)  # numbered lists
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\n\s*\n', '\n\n', text)  # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ ë‘ ê°œë¡œ ì œí•œ
    text = text.strip()
    
    return text

# ========== Node Functions ==========

def session_init_node(state: RAGState) -> RAGState:
    """ì„¸ì…˜ ì´ˆê¸°í™”"""
    if INFO_MODE:
        logger.info("[NODE] session_init_node ì‹¤í–‰ ì‹œì‘")
    try:
        # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
        state = track_execution_path(state, "session_init_node")
        
        # session_id ì²˜ë¦¬ (Djangoì—ì„œ ì „ë‹¬ë°›ì€ ê²½ìš° ë¬¸ìì—´, LangGraph ë‚´ë¶€ì—ì„œëŠ” SessionContext ê°ì²´)
        session_context_obj = state.get("session_context")
        if isinstance(session_context_obj, str):
            # Djangoì—ì„œ ì „ë‹¬ë°›ì€ session_id ë¬¸ìì—´
            session_id = session_context_obj
        elif hasattr(session_context_obj, 'session_id'):
            # SessionContext ê°ì²´
            session_id = session_context_obj.session_id
        else:
            # session_idê°€ ì—†ëŠ” ê²½ìš°
            session_id = None
        
        if not session_id:
            session_context = session_manager.create_session()
            logger.info(f"Created new session: {session_context.session_id}")
        else:
            session_context = session_manager.get_session(session_id)
            if not session_context:
                session_context = session_manager.create_session(session_id)
                logger.info(f"Recreated expired session: {session_id}")
        
        turn_id = f"turn_{int(time.time())}_{hash(str(time.time())) % 10000}"
        
        # Djangoì—ì„œ ì „ë‹¬ë°›ì€ conversation_historyê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì„¸ì…˜ ë§¤ë‹ˆì €ì—ì„œ ë¡œë“œ
        django_history = state.get("conversation_history", [])
        if django_history:
            logger.info(f"[SESSION_INIT] Using Django conversation history: {len(django_history)} messages")
            conversation_history = django_history
        else:
            conversation_history = session_manager.get_conversation_history(session_context.session_id, limit=5)
        
        return {
            **state,
            "session_context": session_context,
            "turn_id": turn_id,
            "conversation_history": conversation_history,
            # ê° í„´ë§ˆë‹¤ ì´ˆê¸°í™”í•´ì•¼ í•  í•„ë“œë“¤
            "product_name": "",
            "product_extraction_result": None,
            "category": "",
            "initial_intent": "",
            "current_topic": "",
            "active_product": "",
            "session_title": "",  # session_title ì´ˆê¸°í™”
            "response": "",  # response ì´ˆê¸°í™”
            "sources": [],  # sources ì´ˆê¸°í™”
            "retrieved_docs": [],  # retrieved_docs ì´ˆê¸°í™”
            "context_text": "",  # context_text ì´ˆê¸°í™”
            "ready_to_answer": False,  # ready_to_answer ì´ˆê¸°í™”
            "guardrail_decision": "",  # guardrail_decision ì´ˆê¸°í™”
            "violations": [],  # violations ì´ˆê¸°í™”
            "compliant_response": ""  # compliant_response ì´ˆê¸°í™”
        }
        
    except Exception as e:
        logger.error(f"Session init failed: {e}")
        session_context = session_manager.create_session()
        return {
            **state,
            "session_context": session_context,
            "turn_id": f"turn_{int(time.time())}",
            "conversation_history": []
        }

def supervisor_node(state: RAGState, llm=None, slm: SLM = None) -> RAGState:
    """ì¤‘ì•™ ê´€ë¦¬ì - íˆ´ ì„ íƒ"""
    logger.info("[NODE] supervisor_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "supervisor_node")
    
    # ìƒì„¸í•œ ì›Œí¬í”Œë¡œìš° ë¡œê·¸
    logger.info(f"ğŸ”„ [WORKFLOW] supervisor_node ì‹œì‘ - ì…ë ¥: {list(state.keys())}")
    logger.info(f"ğŸ”„ [WORKFLOW] supervisor_node - query: {state.get('query', '')[:50]}...")
    
    query = state.get("query", "")
    session_context = state.get("session_context")
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    # ì²« ëŒ€í™”ì¸ì§€ í™•ì¸ (conversation_historyê°€ ë¹„ì–´ìˆì„ ë•Œë§Œ)
    conversation_history = state.get("conversation_history", [])
    session_title = session_context.session_title if session_context else ""
    is_first_turn = not conversation_history or len(conversation_history) == 0
    
    # Djangoì—ì„œ ì „ë‹¬ë°›ì€ messagesê°€ ìˆìœ¼ë©´ ì´ë¥¼ í™œìš©í•˜ì—¬ ë§¥ë½ íŒŒì•…
    messages = state.get("messages", [])
    has_previous_context = messages and len(messages) > 1
    
    logger.info(f"[SUPERVISOR] Query: '{query}' | First turn: {is_first_turn} | Has previous context: {has_previous_context}")
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆëŠ” ê²½ìš° ì´ë¥¼ ê³ ë ¤í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    context_info = ""
    if has_previous_context:
        previous_messages = messages[:-1]  # ë§ˆì§€ë§‰ ë©”ì‹œì§€(í˜„ì¬ ì§ˆë¬¸) ì œì™¸
        context_parts = []
        for msg in previous_messages[-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ ê³ ë ¤
            if hasattr(msg, 'content'):
                role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                context_parts.append(f"{role}: {msg.content[:100]}...")
        
        if context_parts:
            context_info = f"\n\nì´ì „ ëŒ€í™” ë§¥ë½:\n" + "\n".join(context_parts)
            logger.info(f"[SUPERVISOR] Previous context: {context_info[:200]}...")

    # ì²« ë²ˆì§¸ í„´ì¼ ë•Œ: ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¼ìš°íŒ…
    if is_first_turn:
        logger.info("[SUPERVISOR] First turn: Using tools (excluding session_summary)")
        
        # ë„êµ¬ import
        from .tools import answer, rag_search, product_extraction, context_answer
        tool_functions = [answer, rag_search, product_extraction, context_answer]
        
        # ìŠˆí¼ë°”ì´ì € í”„ë¡¬í”„íŠ¸ ìƒì„±
        supervisor_prompt = create_supervisor_prompt(query=query)
        
        try:
            # LLMìœ¼ë¡œ íˆ´ ì„ íƒ
            result = slm.llm.bind_tools(tool_functions, tool_choice="required").invoke(supervisor_prompt)
            
            if hasattr(result, 'tool_calls') and result.tool_calls:
                tool_name = result.tool_calls[0]['name']
                logger.info(f"[SUPERVISOR] ì„ íƒëœ ë„êµ¬: {tool_name}")
                
                if tool_name == "rag_search":
                    return {
                        **state,
                        "needs_rag_search": True
                    }
                elif tool_name == "context_answer":
                    return {
                        **state,
                        "needs_context_answer": True
                    }
                elif tool_name == "product_extraction":
                    return {
                        **state,
                        "needs_product_extraction": True
                    }
                elif tool_name == "answer":
                    return {
                        **state,
                        "needs_answer": True
                    }
        except Exception as e:
            logger.error(f"Tool selection failed: {e}")
            return {
                **state,
                "needs_rag_search": True
            }
    else:
        # ë‘ ë²ˆì§¸ í„´ ì´í›„: ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìš°ì„  ê³ ë ¤
        logger.info("[SUPERVISOR] Multi-turn: Checking if context-based answer is possible")
        
        # ì´ì „ ëŒ€í™” ë§¥ë½ì´ ì¶©ë¶„í•œì§€ í™•ì¸
        if has_previous_context:
            # ìƒí’ˆëª…ì´ í¬í•¨ëœ ì§ˆë¬¸ì¸ì§€ ë¨¼ì € í™•ì¸ (ìƒí’ˆ ê²€ìƒ‰ ìš°ì„ )
            product_keywords = [
                "í–‡ì‚´ë¡ ", "ì§•ê²€ë‹¤ë¦¬ë¡ ", "ëª¨ì•„ë“œë¦¼ë¡ ", "ì‚¬ì—…ìëŒ€ì¶œ", "ì£¼íƒë‹´ë³´ëŒ€ì¶œ", "ì‹ ìš©ëŒ€ì¶œ",
                "ê¸‰ì—¬ì´ì²´ì‹ ìš©ëŒ€ì¶œ", "ì¥ê¸°ë¶„í• ìƒí™˜", "ì „í™˜ì œë„",
                "ì •ê¸°ì˜ˆê¸ˆ", "ì •ê¸°ì ê¸ˆ", "ììœ ì ê¸ˆ", "ì£¼íƒì²­ì•½", "ì—°ê¸ˆ",
                "ì‹ ìš©ì¹´ë“œ", "ì²´í¬ì¹´ë“œ", "ë³´í—˜", "í€ë“œ", "ì£¼ì‹", "ì±„ê¶Œ",
                "ìƒí’ˆ"
            ]
            
            has_product_name = any(keyword in query for keyword in product_keywords)
            
            if has_product_name:
                logger.info("[SUPERVISOR] Product name detected in multi-turn - setting flag for product extraction")
                return {
                    **state,
                    "needs_product_extraction": True,
                    "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                }
            
            # êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ëŠ” í‚¤ì›Œë“œë“¤
            specific_question_keywords = [
                "ìê²©", "ì¡°ê±´", "ìš”ê±´", "ì‹ ì²­", "ëŒ€ìƒ", "ìê²©ìš”ê±´", "ì‹ ì²­ìê²©",
                "ê¸ˆì•¡", "í•œë„", "ì´ì", "ê¸ˆë¦¬", "ê¸°ê°„", "ìƒí™˜", "ë‹´ë³´",
                "ì„œë¥˜", "ì œì¶œ", "í•„ìš”", "ì ˆì°¨", "ë°©ë²•", "ì ˆì°¨"
            ]
            
            # êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸
            is_specific_question = any(keyword in query for keyword in specific_question_keywords)
            
            if is_specific_question:
                logger.info("[SUPERVISOR] Specific question detected - setting flag for RAG search")
                # êµ¬ì²´ì ì¸ ì§ˆë¬¸ì€ RAG ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…í•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
                state = track_state_changes(state, "flag_set", "needs_rag_search=True")
                return {
                    **state,
                    "specific_question": True,
                    "needs_rag_search": True,
                    "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                }
            
            # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì´ ê°€ëŠ¥í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨ (prompts.yamlì—ì„œ ë¡œë“œ)
            context_analysis_prompt = get_prompt("context_analysis", 
                                               context_info=context_info, 
                                               query=query)
            
            try:
                analysis_result = slm.llm.invoke(context_analysis_prompt).content.strip().upper()
                if "YES" in analysis_result:
                    logger.info("[SUPERVISOR] Context-based answer is possible - setting flag for context answer")
                    # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ ë¼ìš°íŒ…í•˜ë„ë¡ í”Œë˜ê·¸ ì„¤ì •
                    return {
                        **state,
                        "context_based": True,
                        "needs_context_answer": True,
                        "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                    }
            except Exception as e:
                logger.warning(f"Context analysis failed: {e}, proceeding with normal tools")
        
        # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ë„êµ¬ ì‚¬ìš©
        logger.info("[SUPERVISOR] Multi-turn: Using normal tools")
        # ì¼ë°˜ ë„êµ¬ ì‚¬ìš© ì‹œì—ëŠ” supervisor_routerì—ì„œ ì²˜ë¦¬
        return {
            **state,
            "needs_normal_tools": True,
            "n_tool_calling": state.get("n_tool_calling", 0) + 1,
        }
    
    # ì˜ë„ íŒë‹¨ ì™„ë£Œ - supervisor_routerì—ì„œ ë¼ìš°íŒ… ì²˜ë¦¬
    logger.info("[SUPERVISOR] Intent analysis completed - routing to supervisor_router")
    return {
        **state,
        "n_tool_calling": state.get("n_tool_calling", 0) + 1,
    }

def supervisor_router(state: RAGState, slm: SLM = None) -> str:
    """ìŠˆí¼ë°”ì´ì € ë¼ìš°í„°"""
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "supervisor_router")
    
    logger.info("[ROUTER] supervisor_router ì‹¤í–‰ ì‹œì‘")
    logger.info(f"[ROUTER] ready_to_answer: {state.get('ready_to_answer')}")
    
    # ìƒì„¸í•œ ì›Œí¬í”Œë¡œìš° ë¡œê·¸
    logger.info(f"ğŸ”„ [WORKFLOW] supervisor_router ì‹œì‘ - í”Œë˜ê·¸: needs_rag_search={state.get('needs_rag_search')}, needs_context_answer={state.get('needs_context_answer')}")
    logger.info(f"ğŸ”„ [WORKFLOW] supervisor_router - redirect_to_rag: {state.get('redirect_to_rag')}")
    
    # messagesì—ì„œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ í™•ì¸
    messages = state.get("messages", [])
    if messages:
        last_message = messages[-1]
        logger.info(f"[ROUTER] Last message type: {type(last_message)}")
        logger.info(f"[ROUTER] Last message content: {getattr(last_message, 'content', 'No content')}")
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            logger.info(f"[ROUTER] Tool calls: {[call['name'] for call in last_message.tool_calls]}")
    
    # ready_to_answerê°€ Trueì´ë©´ ì¦‰ì‹œ answerë¡œ ë¼ìš°íŒ… (ë¬´í•œ ë£¨í”„ ë°©ì§€) - ìµœìš°ì„ ìˆœìœ„
    if state.get("ready_to_answer"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] ready_to_answer=True - ANSWERë¡œ ë¼ìš°íŒ… (ë¬´í•œ ë£¨í”„ ë°©ì§€)")
        return "answer"
    
    # supervisor_nodeì—ì„œ ì„¤ì •ëœ í”Œë˜ê·¸ë“¤ ì²˜ë¦¬
    if state.get("needs_rag_search"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] needs_rag_search=True - RAG_SEARCHë¡œ ë¼ìš°íŒ…")
        return "rag_search"
    
    if state.get("needs_context_answer"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] needs_context_answer=True - CONTEXT_ANSWERë¡œ ë¼ìš°íŒ…")
        return "context_answer"
    
    if state.get("needs_product_extraction"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] needs_product_extraction=True - PRODUCT_EXTRACTIONìœ¼ë¡œ ë¼ìš°íŒ…")
        return "product_extraction"
    
    if state.get("needs_answer"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] needs_answer=True - ANSWERë¡œ ë¼ìš°íŒ…")
        return "answer"
    
    # LLMì„ ì‚¬ìš©í•œ ì§€ëŠ¥í˜• ë¼ìš°íŒ… (í”„ë¡¬í”„íŠ¸ ê¸°ë°˜)
    query = state.get("query", "")
    
    if slm is None:
        slm = get_shared_slm()
    
    # ë¼ìš°íŒ… ê²°ì •ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (prompts.yamlì—ì„œ ë¡œë“œ)
    routing_prompt = get_prompt("main_routing", query=query)
    
    try:
        routing_decision = slm.invoke(routing_prompt).strip()
        logger.info(f"[ROUTER] LLM ë¼ìš°íŒ… ê²°ì •: {routing_decision}")
        
        if routing_decision == "PRODUCT_EXTRACTION":
            logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
            logger.info("[ROUTER] ìƒí’ˆ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€ - PRODUCT_EXTRACTIONìœ¼ë¡œ ë¼ìš°íŒ…")
            return "product_extraction"
        elif routing_decision == "CONTEXT_ANSWER":
            logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
            logger.info("[ROUTER] ë©€í‹°í„´ ëŒ€í™” ê°ì§€ - CONTEXT_ANSWERë¡œ ë¼ìš°íŒ…")
            return "context_answer"
        elif routing_decision == "RAG_SEARCH":
            logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
            logger.info("[ROUTER] ê·œì •/ì •ì±… ì§ˆë¬¸ ê°ì§€ - RAG_SEARCHë¡œ ë¼ìš°íŒ…")
            return "rag_search"
        else:
            logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
            logger.info("[ROUTER] ì¼ë°˜ FAQ ì§ˆë¬¸ ê°ì§€ - ANSWERë¡œ ë¼ìš°íŒ…")
            return "answer"
            
    except Exception as e:
        logger.error(f"[ROUTER] ë¼ìš°íŒ… ê²°ì • ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ RAG ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…
        logger.info("[ROUTER] ì˜¤ë¥˜ë¡œ ì¸í•´ ê¸°ë³¸ RAG_SEARCHë¡œ ë¼ìš°íŒ…")
        return "rag_search"
    
    


def product_extraction_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ìƒí’ˆëª… ì¶”ì¶œ ë…¸ë“œ"""
    logger.info("ğŸ·ï¸ [NODE] product_extraction_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "product_extraction_node")
    
    query = state.get("query", "")
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    try:
        extracted_product = extract_product_name(slm, query)
        sub_category = classify_product_subcategory(extracted_product)
        logger.info(f"Extracted product: '{extracted_product}', Sub category: '{sub_category}'")
        
        return {
            **state,
            "product_name": extracted_product,
            "product_extraction_result": {
                "product_name": extracted_product,
                "sub_category": sub_category,
                "confidence": 0.9,
                "reasoning": f"ìƒí’ˆëª… '{extracted_product}'ì„ ì¶”ì¶œí•˜ê³  '{sub_category}'ë¡œ ë¶„ë¥˜"
            },
            "ready_to_answer": False  # ì•„ì§ ë‹µë³€ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ
        }
        
    except Exception as e:
        logger.error(f"Product extraction failed: {e}")
        return {
            **state,
            "product_name": "ì¼ë°˜",
            "ready_to_answer": False
        }

def product_search_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ìƒí’ˆ ê²€ìƒ‰ ë…¸ë“œ"""
    logger.info("ğŸ” [NODE] product_search_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "product_search_node")
    
    query = state.get("query", "")
    product_name = state.get("product_name", "")
    
    # SLMê³¼ VectorStore ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    if slm is None:
        slm = get_shared_slm()
    vector_store = get_shared_vector_store()
    
    try:
        # ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        cached_docs = get_cached_search_result(query, product_name)
        if cached_docs is not None:
            logger.info(f"Using cached search results: {len(cached_docs)} documents")
            retrieved_docs = cached_docs
        else:
            vector_store.get_index_ready()
            
            if not product_name or product_name == "ì¼ë°˜":
                # ì¼ë°˜ ê²€ìƒ‰ (ì„±ëŠ¥ ìµœì í™”)
                start_search_time = time.time()
                retrieved_docs = vector_store.similarity_search(query, k=2)  # ìµœì†Œ ê²°ê³¼ë¡œ ì†ë„ ìµœëŒ€í™”
                end_search_time = time.time()
                logger.info(f"General search: {end_search_time - start_search_time:.2f}ì´ˆ, found {len(retrieved_docs)} documents")
            else:
                # ìƒí’ˆëª…ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (ìµœì í™”ëœ ë‹¨ì¼ ì‹œë„)
                filter_dict = {"keywords": {"$in": [product_name]}}
                
                try:
                    start_search_time = time.time()
                    retrieved_docs = vector_store.similarity_search(query, k=3, filter_dict=filter_dict)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                    end_search_time = time.time()
                    if retrieved_docs:
                        logger.info(f"Product search: {end_search_time - start_search_time:.2f}ì´ˆ, found {len(retrieved_docs)} documents using filter: {filter_dict}")
                    else:
                        # í•„í„°ë§ëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                        start_fallback_time = time.time()
                        retrieved_docs = vector_store.similarity_search(query, k=3)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                        end_fallback_time = time.time()
                        logger.info(f"No documents found for product '{product_name}', fallback search: {end_fallback_time - start_fallback_time:.2f}ì´ˆ, found {len(retrieved_docs)} documents")
                except Exception as e:
                    logger.warning(f"Filter failed: {filter_dict}, error: {e}")
                    # ì—ëŸ¬ ë°œìƒ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°± (ì„±ëŠ¥ ìµœì í™”)
                    start_fallback_time = time.time()
                    retrieved_docs = vector_store.similarity_search(query, k=3)  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
                    end_fallback_time = time.time()
                    logger.info(f"Error fallback search: {end_fallback_time - start_fallback_time:.2f}ì´ˆ, found {len(retrieved_docs)} documents")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            set_cached_search_result(query, product_name, retrieved_docs)
        
        # PDF ì •ë³´ ë¡œê¹…
        if retrieved_docs:
            logger.info("ğŸ“„ [PRODUCT SEARCH] ì‚¬ìš©ëœ PDF ë¬¸ì„œ ì •ë³´:")
            for i, doc in enumerate(retrieved_docs):
                metadata = doc.metadata
                file_name = metadata.get('file_name', 'Unknown')
                page_number = metadata.get('page_number', 'Unknown')
                logger.info(f"  ğŸ“‹ ë¬¸ì„œ {i+1}: {file_name} (í˜ì´ì§€: {page_number})")
        
        # ê³µí†µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
        start_llm_time = time.time()
        response, sources = create_rag_response(slm, query, retrieved_docs)
        end_llm_time = time.time()
        logger.info(f"[PRODUCT_SEARCH] LLM response generation: {end_llm_time - start_llm_time:.2f}ì´ˆ")
        
        return {
            **state,
            "response": response,
            "sources": sources,
            "retrieved_docs": retrieved_docs,
            "context_text": format_context(retrieved_docs) if retrieved_docs else "",
            "ready_to_answer": True
        }
        
    except Exception as e:
        logger.error(f"Product search failed: {e}")
        return {
            **state,
            **create_error_response("product_error")
        }

def session_summary_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ì„¸ì…˜ ìš”ì•½ ìƒì„± ë…¸ë“œ (ì²« ëŒ€í™”)"""
    start_time = time.time()
    logger.info("[SESSION_SUMMARY] Starting session title generation")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "session_summary_node")
    
    query = state.get("query", "")
    session_context = state.get("session_context")
    conversation_history = state.get("conversation_history", [])
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    # ì²« ëŒ€í™”ì¸ì§€ í™•ì¸
    conversation_history = state.get("conversation_history", [])
    is_first_turn = not conversation_history or len(conversation_history) == 0
    
    if is_first_turn:
        # ê³µí†µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œëª© ìƒì„±
        session_title = generate_session_title(query, slm)
        logger.info(f"[SESSION_SUMMARY] Generated title: '{session_title}'")

        # ì„¸ì…˜ì— ì œëª© ì €ì¥
        session_manager.update_session(
            session_context.session_id,
            session_title=session_title
        )
        
        logger.info(f"Session title saved to session: {session_context.session_id}")
        
        # ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        updated_session_context = session_manager.get_session(session_context.session_id)
        if updated_session_context:
            session_context = updated_session_context
        
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"Returning state with session_title: '{session_title}'")
        logger.info(f"ğŸ“ [NODE] session_summary_node ì™„ë£Œ - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
        return {
            **state,
            "session_title": session_title,
            "ready_to_answer": False,  # RAG ê²€ìƒ‰ì„ ìœ„í•´ Falseë¡œ ì„¤ì •
            "session_context": session_context,  # ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ í¬í•¨
            "response": ""  # RAG ê²€ìƒ‰ì—ì„œ ì‹¤ì œ ì‘ë‹µ ìƒì„±
        }
    else:
        # ì²« ëŒ€í™”ê°€ ì•„ë‹ˆë©´ ê¸°ì¡´ ì œëª© ì‚¬ìš©í•˜ê³  RAG ê²€ìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°
        existing_title = session_context.session_title if session_context else ""
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"ğŸ“ [NODE] session_summary_node ì™„ë£Œ (ê¸°ì¡´ ì œëª© ì‚¬ìš©) - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
        return {
            **state,
            "session_title": existing_title,
            "ready_to_answer": False,  # RAG ê²€ìƒ‰ì„ ìœ„í•´ Falseë¡œ ì„¤ì •
            "response": ""
        }
        

def rag_search_node(state: RAGState, slm: SLM = None, vector_store=None) -> RAGState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    start_time = time.time()
    logger.info("[RAG_SEARCH] Starting document search")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "rag_search_node")
    
    query = state.get("query", "")
    
    # í•µì‹¬ ë…¸ë“œ ìƒíƒœ ë³€ê²½ ì¶”ì 
    state = track_state_changes(state, "search", f"RAG search started for: {query[:50]}...")
    
    # SLMê³¼ VectorStore ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì¬ì‚¬ìš© ê°€ëŠ¥)
    if slm is None:
        slm = get_shared_slm()
    if vector_store is None:
        vector_store = get_shared_vector_store()
        vector_store.get_index_ready()  # í•œ ë²ˆë§Œ ì´ˆê¸°í™”
    
    try:
        # ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ê°„ì†Œí™”)
        messages = state.get("messages", [])
        enhanced_query = query
        
        # ìµœê·¼ 1ê°œ ë©”ì‹œì§€ë§Œ ê³ ë ¤í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
        if messages and len(messages) > 1:
            last_msg = messages[-3]  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ê³ ë ¤
            if hasattr(last_msg, 'content') and len(last_msg.content) < 100:
                # ë¬¸ìì—´ ì—°ê²° ìµœì í™”
                context_snippet = last_msg.content[:30]
                enhanced_query = f"{query} {context_snippet}"
                if DEBUG_MODE:
                    logger.debug(f"[RAG_SEARCH] Enhanced query with context: {enhanced_query[:100]}...")
        
        # ë¬¸ì„œ ê²€ìƒ‰ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ê²°ê³¼ ìˆ˜ ì œí•œ)
        start_search_time = time.time()
        retrieved_docs = vector_store.similarity_search(enhanced_query, k=3)  # ê´€ë ¨ ë¬¸ì„œ 3ê°œë¡œ ì¦ê°€
        end_search_time = time.time()
        log_performance("Vector search", start_search_time, end_search_time, f"found {len(retrieved_docs)} documents")
        
        # ê³µí†µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
        start_llm_time = time.time()
        response, sources = create_rag_response(slm, query, retrieved_docs)
        end_llm_time = time.time()
        log_performance("LLM response generation", start_llm_time, end_llm_time)
        
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"ğŸ“š [NODE] rag_search_node ì™„ë£Œ - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
        
        return {
            **state,
            "response": response,
            "sources": sources,
            "retrieved_docs": retrieved_docs,
            "context_text": format_context(retrieved_docs) if retrieved_docs else "",
            "ready_to_answer": True
        }
        
    except Exception as e:
        error_msg = str(e)
        if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
            logger.error(f"RAG search timeout: {e}")
            return {
                **state,
                **create_error_response("timeout_error")
            }
        else:
            logger.error(f"RAG search failed: {e}")
            return {
                **state,
                **create_error_response("search_error")
            }

def guardrail_check_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ê°€ë“œë ˆì¼ ê²€ì‚¬ ë…¸ë“œ"""
    logger.info("ğŸ›¡ï¸ [NODE] guardrail_check_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "guardrail_check_node")
    
    response = state.get("response", "")
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    try:
        # ì‘ë‹µ ê¸¸ì´ì— ë”°ë¥¸ ê°€ë“œë ˆì¼ ê²€ì‚¬ ìµœì í™”
        response_length = len(response)
        logger.info(f"ğŸ›¡ï¸ [GUARDRAIL] Response length: {response_length}ì")
        
        # ê°€ë“œë ˆì¼ ê²€ì‚¬ ìµœì í™” (ê°„ì†Œí™”ëœ ê²€ì‚¬)
        logger.info("ğŸ›¡ï¸ [GUARDRAIL] Starting optimized guardrail check...")
        start_time = time.time()
        
        # ê°„ì†Œí™”ëœ ê°€ë“œë ˆì¼ ê²€ì‚¬ (ê¸°ë³¸ ê²€ì‚¬ë§Œ)
        try:
            # ê¸°ë³¸ ì‘ë‹µ ê²€ì¦ (ë¹ ë¥¸ ì²´í¬)
            violations = []
            
            # 1. ê¸°ë³¸ ê¸¸ì´ ê²€ì‚¬ (ë¹ ë¦„)
            if len(response.strip()) < 10:
                violations.append("ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
            
            # 2. ê¸°ë³¸ ê¸ˆì§€ì–´ ê²€ì‚¬ (ë¹ ë¦„)
            forbidden_words = ["ì£„ì†¡", "ëª¨ë¥´ê² ", "ì•Œ ìˆ˜ ì—†", "í™•ì¸ ë¶ˆê°€"]
            if any(word in response for word in forbidden_words):
                violations.append("ë¶€ì ì ˆí•œ í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            
            # 3. ê¸°ë³¸ ì™„ì„±ë„ ê²€ì‚¬ (ë¹ ë¦„)
            if not response.endswith(('.', '!', '?', 'ë‹¤', 'ìš”', 'ë‹ˆë‹¤')):
                violations.append("ì‘ë‹µì´ ì™„ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # ìœ„ë°˜ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì•ˆì „í•œ ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´
            if violations:
                logger.warning(f"ğŸ›¡ï¸ [GUARDRAIL] Found {len(violations)} violations: {violations}")
                compliant_response = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤. ê´€ë ¨ ë¶€ì„œì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            else:
                compliant_response = response
                
        except Exception as e:
            logger.error(f"ğŸ›¡ï¸ [GUARDRAIL] Error: {e}")
            compliant_response = response
            violations = []
        
        end_time = time.time()
        logger.info(f"ğŸ›¡ï¸ [GUARDRAIL] Optimized guardrail check completed - ì‹¤í–‰ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        
        # ì‹¤í–‰ ê²½ë¡œ ë¡œê¹…
        execution_path = state.get("execution_path", [])
        path_str = " -> ".join(execution_path)
        logger.info(f"ğŸ›¡ï¸ [WORKFLOW] Execution path: {path_str}")
        
        result = {
            **state,
            "guardrail_decision": "COMPLIANT" if not violations else "VIOLATION",
            "violations": violations,
            "compliant_response": compliant_response,
            "response": compliant_response,
            "ready_to_answer": True
        }
        
        logger.info("ğŸ›¡ï¸ [NODE] guardrail_check_node ì™„ë£Œ")
        logger.info(f"ğŸ›¡ï¸ [NODE] ready_to_answer: {result.get('ready_to_answer')}")
        logger.info(f"ğŸ›¡ï¸ [NODE] response length: {len(result.get('response', ''))}")
        return result
        
    except Exception as e:
        logger.error(f"Guardrail check failed: {e}")
        result = {
            **state,
            "guardrail_decision": "ERROR",
            "violations": ["ê°€ë“œë ˆì¼ ê²€ì‚¬ ì˜¤ë¥˜"],
            "compliant_response": get_error_message("guardrail_error"),
            "response": get_error_message("guardrail_error"),
            "ready_to_answer": True
        }
        logger.info("ğŸ›¡ï¸ [NODE] guardrail_check_node ì™„ë£Œ (ì—ëŸ¬)")
        return result

def context_answer_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ë…¸ë“œ - ì´ì „ ëŒ€í™” ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€"""
    start_time = time.time()
    logger.info("[CONTEXT_ANSWER] Starting context-based answer generation")
    
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "context_answer_node")
    
    query = state.get("query", "")
    messages = state.get("messages", [])
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    if slm is None:
        slm = get_shared_slm()
    
    try:
        # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± (í† í° í•œê³„ ë°©ì§€ë¥¼ ìœ„í•´ ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ)
        previous_context = ""
        recent_messages = messages[:-1][-3:]  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
        for msg in recent_messages:
            if hasattr(msg, 'content'):
                role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                # ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ (í† í° í•œê³„ ë°©ì§€)
                content = msg.content[:200] + "..." if len(msg.content) > 200 else msg.content
                previous_context += f"{role}: {content}\n"
        
        # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ (prompts.yamlì—ì„œ ë¡œë“œ)
        context_prompt = get_prompt("context_answer", 
                                   previous_context=previous_context, 
                                   query=query)
        
        # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìƒì„± (ì„±ëŠ¥ ìµœì í™”)
        try:
            response = slm.llm.invoke(context_prompt).content
            response = clean_markdown_formatting(response)
        except Exception as e:
            logger.error(f"Context answer generation failed: {e}")
            response = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜ ê´€ë ¨ ë¶€ì„œì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."
        
        # ë‹µë³€ì´ ì¼ë°˜ì ì´ê±°ë‚˜ êµ¬ì²´ì ì¸ ì •ë³´ê°€ ë¶€ì¡±í•œì§€ í™•ì¸
        insufficient_info_keywords = [
            "ì¼ë°˜ì ìœ¼ë¡œ", "êµ¬ì²´ì ì¸ ì¡°ê±´ì€", "ìì„¸í•œ ì‚¬í•­ì€", "ë¬¸ì˜í•˜ì‹œê±°ë‚˜", 
            "í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤", "ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ", "í•´ë‹¹ ê¸ˆìœµê¸°ê´€ì—"
        ]
        
        needs_rag_search = any(keyword in response for keyword in insufficient_info_keywords)
        
        if needs_rag_search:
            logger.info("[CONTEXT_ANSWER] Insufficient information - redirecting to RAG search")
            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ìƒíƒœ ë³€ê²½ë§Œ ì¶”ì  (ì‹¤í–‰ ê²½ë¡œ ì¤‘ë³µ ë°©ì§€)
            state = track_state_changes(state, "redirect", "Insufficient context information")
            
            # ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ìƒí’ˆëª… ì¶”ì¶œ
            product_name = ""
            for msg in messages[:-1]:
                if hasattr(msg, 'content') and msg.__class__.__name__ == "AIMessage":
                    content = msg.content
                    # ìƒí’ˆëª… ì¶”ì¶œ ë¡œì§ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
                    if "ì‚¬ë¦½í•™êµêµì§ì›ìš°ëŒ€ëŒ€ì¶œ" in content:
                        product_name = "ì‚¬ë¦½í•™êµêµì§ì›ìš°ëŒ€ëŒ€ì¶œ"
                        break
                    elif "ë²„íŒ€ëª© ì „ì„¸ìê¸ˆëŒ€ì¶œ" in content:
                        product_name = "ë²„íŒ€ëª© ì „ì„¸ìê¸ˆëŒ€ì¶œ"
                        break
                    elif "í–‡ì‚´ë¡ " in content:
                        product_name = "í–‡ì‚´ë¡ "
                        break
            
            if product_name:
                logger.info(f"[CONTEXT_ANSWER] Redirecting to RAG search for product: {product_name}")
                # RAG ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±
                enhanced_query = f"{query} {product_name}"
                return {
                    **state,
                    "query": enhanced_query,
                    "messages": [AIMessage(content="RAG ê²€ìƒ‰", tool_calls=[{"name": "rag_search", "args": {"query": enhanced_query}, "id": f"call_{int(time.time())}_{hash(enhanced_query) % 10000}"}])],
                    "ready_to_answer": False,  # RAG ê²€ìƒ‰ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
                    "context_based": False,
                    "redirect_to_rag": True
                }
            else:
                # ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ì¼ë°˜ RAG ê²€ìƒ‰
                logger.info("[CONTEXT_ANSWER] No product name found - using general RAG search")
                return {
                    **state,
                    "messages": [AIMessage(content="RAG ê²€ìƒ‰", tool_calls=[{"name": "rag_search", "args": {"query": query}, "id": f"call_{int(time.time())}_{hash(query) % 10000}"}])],
                    "ready_to_answer": False,
                    "context_based": False,
                    "redirect_to_rag": True
                }
        
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"[CONTEXT_ANSWER] Context-based answer generated - execution time: {execution_time:.2f}s")
        
        return {
            **state,
            "response": response,
            "sources": [],  # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì´ë¯€ë¡œ ì†ŒìŠ¤ ì—†ìŒ
            "ready_to_answer": True,
            "context_based": True  # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì„ì„ í‘œì‹œ
        }
        
    except Exception as e:
        logger.error(f"Context answer generation failed: {e}")
        return {
            **state,
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
            "ready_to_answer": True,
            "context_based": False
        }

def answer_node(state: RAGState) -> RAGState:
    """ìµœì¢… ë‹µë³€ ë…¸ë“œ"""
    start_time = time.time()
    # ë¡œê¹… ìµœì†Œí™” (ì„±ëŠ¥ ê°œì„ )
    if DEBUG_MODE:
        logger.debug("ğŸ“ [NODE] ANSWER_NODE ì§„ì…")
    
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "answer_node")
    
    response = state.get("response", "")
    
    # í•µì‹¬ ë…¸ë“œ ìƒíƒœ ë³€ê²½ ì¶”ì 
    state = track_state_changes(state, "answer", f"Answer generation started - response length: {len(response) if response else 0}")
    logger.info(f"ğŸ“ [ANSWER] í˜„ì¬ ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
    
    # í˜„ì¬ ì§ˆë¬¸ì— ì§‘ì¤‘ (ì´ì „ ë§¥ë½ ì œí•œ)
    current_query = state.get("query", "")
    logger.info(f"[ANSWER] Current query: {current_query}")
    
    # ìƒí’ˆ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° ì´ì „ ë§¥ë½ ë¬´ì‹œ
    product_keywords = ["ë¡ ", "ëŒ€ì¶œ", "ì˜ˆê¸ˆ", "ì ê¸ˆ", "ì‹ ìš©ì¹´ë“œ", "ë³´í—˜", "í€ë“œ", "ì£¼ì‹", "ì±„ê¶Œ"]
    is_product_question = any(keyword in current_query for keyword in product_keywords)
    
    if is_product_question:
        logger.info("[ANSWER] ìƒí’ˆ ê´€ë ¨ ì§ˆë¬¸ - ì´ì „ ë§¥ë½ ë¬´ì‹œí•˜ê³  í˜„ì¬ ì§ˆë¬¸ì— ì§‘ì¤‘")
    else:
        # ì¼ë°˜ FAQ ì§ˆë¬¸ì¸ ê²½ìš°ì—ë§Œ ì´ì „ ë§¥ë½ ê³ ë ¤
        messages = state.get("messages", [])
        if messages and len(messages) > 1:
            previous_context = ""
            for msg in messages[:-1][-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ ê³ ë ¤ (ì œí•œ)
                if hasattr(msg, 'content'):
                    role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                    previous_context += f"{role}: {msg.content[:50]}...\n"  # ë” ì§§ê²Œ ì œí•œ
            
            if previous_context:
                logger.info(f"[ANSWER] Considering previous context: {previous_context[:100]}...")
    
    if not response or not response.strip():
        # ì¸ì‚¬ë§ ê°ì§€ ë° ê°„ë‹¨í•œ ì‘ë‹µ
        current_query = state.get("query", "").strip().lower()
        greeting_keywords = ["ì•ˆë…•", "hello", "hi", "í•˜ì´", "ë°˜ê°€", "ì²˜ìŒ", "ì‹œì‘"]
        
        if any(keyword in current_query for keyword in greeting_keywords):
            response = "ì•ˆë…•í•˜ì„¸ìš”! KBê¸ˆìœµê·¸ë£¹ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"
            logger.info("[ANSWER] Generated greeting response")
        else:
            # FAQ ë‹µë³€ ìƒì„± ì‹œë„
            try:
                slm = get_shared_slm()
                response = create_simple_response(slm, state.get("query", ""), "faq_system")
                response = clean_markdown_formatting(response)
                logger.info("[ANSWER] Generated FAQ response")
            except Exception as e:
                logger.error(f"FAQ response generation failed: {e}")
                # ê¸°ë³¸ ì‘ë‹µ ìƒì„± (RAG ê²€ìƒ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ)
                response = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜ ê´€ë ¨ ë¶€ì„œì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."
                logger.info("[ANSWER] Using default response")
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
    response = clean_markdown_formatting(response)
    
    # Djangoì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ LangGraphì—ì„œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
    logger.info("ğŸ“ [NODE] ëŒ€í™” í„´ ì €ì¥ì€ Djangoì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤")
    
    end_time = time.time()
    execution_time = end_time - start_time
    logger.info(f"âœ… [NODE] answer_node ì™„ë£Œ - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
    
    return {
        **state,
        "response": response,
        "ready_to_answer": True
    }

