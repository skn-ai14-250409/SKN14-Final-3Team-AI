"""
LangGraph Nodes
==============
RAG ì›Œí¬í”Œë¡œìš°ì˜ ë…¸ë“œ í•¨ìˆ˜ë“¤
"""

import logging
import time
import re
from typing import Dict, List, Any

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_core.documents import Document

from .models import RAGState
from .session_manager import session_manager
from ..slm.slm import SLM
from .utils import (
    generate_session_title, 
    create_rag_response, 
    create_simple_response, 
    create_guardrail_response,
    extract_product_name,
    classify_product_subcategory,
    create_error_response,
    format_context,
    create_supervisor_prompt,
    get_error_message,
    get_cached_search_result,
    set_cached_search_result,
    DEFAULT_SEARCH_K,
)

# ========== Execution Path Tracking ==========

def track_execution_path(state: RAGState, node_name: str) -> RAGState:
    """ì‹¤í–‰ ê²½ë¡œë¥¼ ì¶”ì í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    execution_path = state.get("execution_path", [])
    execution_path.append(node_name)
    return {**state, "execution_path": execution_path}
    
from .utils import get_shared_slm, get_shared_vector_store

logger = logging.getLogger(__name__)

# ========== Utility Functions ==========

def clean_markdown_formatting(text: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì œê±°í•˜ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    if not text:
        return text
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # **bold** -> bold
    text = re.sub(r'\*(.*?)\*', r'\1', text)      # *italic* -> italic
    text = re.sub(r'#+\s*', '', text)             # # headers -> headers
    text = re.sub(r'`(.*?)`', r'\1', text)        # `code` -> code
    text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)  # [text](url) -> text
    text = re.sub(r'^\s*[-*+]\s*', '', text, flags=re.MULTILINE)  # bullet points
    text = re.sub(r'^\s*\d+\.\s*', '', text, flags=re.MULTILINE)  # numbered lists
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\n\s*\n', '\n\n', text)  # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ ë‘ ê°œë¡œ ì œí•œ
    text = text.strip()
    
    return text

# ========== Node Functions ==========

def session_init_node(state: RAGState) -> RAGState:
    """ì„¸ì…˜ ì´ˆê¸°í™”"""
    logger.info("[NODE] session_init_node ì‹¤í–‰ ì‹œì‘")
    try:
        # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
        state = track_execution_path(state, "session_init_node")
        
        # session_id ì²˜ë¦¬ (Djangoì—ì„œ ì „ë‹¬ë°›ì€ ê²½ìš° ë¬¸ìì—´, LangGraph ë‚´ë¶€ì—ì„œëŠ” SessionContext ê°ì²´)
        session_context_obj = state.get("session_context")
        if isinstance(session_context_obj, str):
            # Djangoì—ì„œ ì „ë‹¬ë°›ì€ session_id ë¬¸ìì—´
            session_id = session_context_obj
        elif hasattr(session_context_obj, 'session_id'):
            # SessionContext ê°ì²´
            session_id = session_context_obj.session_id
        else:
            # session_idê°€ ì—†ëŠ” ê²½ìš°
            session_id = None
        
        if not session_id:
            session_context = session_manager.create_session()
            logger.info(f"Created new session: {session_context.session_id}")
        else:
            session_context = session_manager.get_session(session_id)
            if not session_context:
                session_context = session_manager.create_session(session_id)
                logger.info(f"Recreated expired session: {session_id}")
        
        turn_id = f"turn_{int(time.time())}_{hash(str(time.time())) % 10000}"
        
        # Djangoì—ì„œ ì „ë‹¬ë°›ì€ conversation_historyê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì„¸ì…˜ ë§¤ë‹ˆì €ì—ì„œ ë¡œë“œ
        django_history = state.get("conversation_history", [])
        if django_history:
            logger.info(f"[SESSION_INIT] Using Django conversation history: {len(django_history)} messages")
            conversation_history = django_history
        else:
            conversation_history = session_manager.get_conversation_history(session_context.session_id, limit=5)
        
        return {
            **state,
            "session_context": session_context,
            "turn_id": turn_id,
            "conversation_history": conversation_history,
            # ê° í„´ë§ˆë‹¤ ì´ˆê¸°í™”í•´ì•¼ í•  í•„ë“œë“¤
            "product_name": "",
            "product_extraction_result": None,
            "category": "",
            "initial_intent": "",
            "current_topic": "",
            "active_product": "",
            "session_title": "",  # session_title ì´ˆê¸°í™”
            "response": "",  # response ì´ˆê¸°í™”
            "sources": [],  # sources ì´ˆê¸°í™”
            "retrieved_docs": [],  # retrieved_docs ì´ˆê¸°í™”
            "context_text": "",  # context_text ì´ˆê¸°í™”
            "ready_to_answer": False,  # ready_to_answer ì´ˆê¸°í™”
            "guardrail_decision": "",  # guardrail_decision ì´ˆê¸°í™”
            "violations": [],  # violations ì´ˆê¸°í™”
            "compliant_response": ""  # compliant_response ì´ˆê¸°í™”
        }
        
    except Exception as e:
        logger.error(f"Session init failed: {e}")
        session_context = session_manager.create_session()
        return {
            **state,
            "session_context": session_context,
            "turn_id": f"turn_{int(time.time())}",
            "conversation_history": []
        }

def supervisor_node(state: RAGState, llm=None, slm: SLM = None) -> RAGState:
    """ì¤‘ì•™ ê´€ë¦¬ì - íˆ´ ì„ íƒ"""
    logger.info("[NODE] supervisor_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "supervisor_node")
    
    query = state.get("query", "")
    session_context = state.get("session_context")
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    # ì²« ëŒ€í™”ì¸ì§€ í™•ì¸ (conversation_historyê°€ ë¹„ì–´ìˆì„ ë•Œë§Œ)
    conversation_history = state.get("conversation_history", [])
    session_title = session_context.session_title if session_context else ""
    is_first_turn = not conversation_history or len(conversation_history) == 0
    
    # Djangoì—ì„œ ì „ë‹¬ë°›ì€ messagesê°€ ìˆìœ¼ë©´ ì´ë¥¼ í™œìš©í•˜ì—¬ ë§¥ë½ íŒŒì•…
    messages = state.get("messages", [])
    has_previous_context = messages and len(messages) > 1
    
    logger.info(f"[SUPERVISOR] Query: '{query}' | First turn: {is_first_turn} | Has previous context: {has_previous_context}")
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ì´ ìˆëŠ” ê²½ìš° ì´ë¥¼ ê³ ë ¤í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    context_info = ""
    if has_previous_context:
        previous_messages = messages[:-1]  # ë§ˆì§€ë§‰ ë©”ì‹œì§€(í˜„ì¬ ì§ˆë¬¸) ì œì™¸
        context_parts = []
        for msg in previous_messages[-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ ê³ ë ¤
            if hasattr(msg, 'content'):
                role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                context_parts.append(f"{role}: {msg.content[:100]}...")
        
        if context_parts:
            context_info = f"\n\nì´ì „ ëŒ€í™” ë§¥ë½:\n" + "\n".join(context_parts)
            logger.info(f"[SUPERVISOR] Previous context: {context_info[:200]}...")

    # ì²« ë²ˆì§¸ í„´ì¼ ë•Œ: session_summary ì œì™¸í•œ ë„êµ¬ë“¤ ì‚¬ìš© (ì´ë¯¸ SESSION_SUMMARY ë…¸ë“œì—ì„œ ì²˜ë¦¬ë¨)
    if is_first_turn:
        logger.info("[SUPERVISOR] First turn: Using tools (excluding session_summary)")
        from .tools import answer, rag_search, product_extraction
        tool_functions = [answer, rag_search, product_extraction]
    else:
        # ë‘ ë²ˆì§¸ í„´ ì´í›„: ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìš°ì„  ê³ ë ¤
        logger.info("[SUPERVISOR] Multi-turn: Checking if context-based answer is possible")
        
        # ì´ì „ ëŒ€í™” ë§¥ë½ì´ ì¶©ë¶„í•œì§€ í™•ì¸
        if has_previous_context:
            # êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ëŠ” í‚¤ì›Œë“œë“¤
            specific_question_keywords = [
                "ìê²©", "ì¡°ê±´", "ìš”ê±´", "ì‹ ì²­", "ëŒ€ìƒ", "ìê²©ìš”ê±´", "ì‹ ì²­ìê²©",
                "ê¸ˆì•¡", "í•œë„", "ì´ì", "ê¸ˆë¦¬", "ê¸°ê°„", "ìƒí™˜", "ë‹´ë³´",
                "ì„œë¥˜", "ì œì¶œ", "í•„ìš”", "ì ˆì°¨", "ë°©ë²•", "ì ˆì°¨"
            ]
            
            # êµ¬ì²´ì ì¸ ì§ˆë¬¸ì¸ì§€ í™•ì¸
            is_specific_question = any(keyword in query for keyword in specific_question_keywords)
            
            if is_specific_question:
                logger.info("[SUPERVISOR] Specific question detected - using RAG search")
                # êµ¬ì²´ì ì¸ ì§ˆë¬¸ì€ RAG ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…
                from .tools import rag_search
                return {
                    **state,
                    "messages": [AIMessage(content="RAG ê²€ìƒ‰", tool_calls=[{"name": "rag_search", "args": {"query": query}, "id": f"call_{int(time.time())}_{hash(query) % 10000}"}])],
                    "ready_to_answer": False,
                    "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                }
            
            # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì´ ê°€ëŠ¥í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨
            context_analysis_prompt = f"""
            ì´ì „ ëŒ€í™” ë§¥ë½:
            {context_info}

            í˜„ì¬ ì§ˆë¬¸: {query}

            ìœ„ ì´ì „ ëŒ€í™” ë‚´ìš©ë§Œìœ¼ë¡œ í˜„ì¬ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
            ë‹µë³€ ê°€ëŠ¥í•˜ë©´ "YES", ìƒˆë¡œìš´ ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•˜ë©´ "NO"ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
            """
            
            try:
                analysis_result = slm.llm.invoke(context_analysis_prompt).content.strip().upper()
                if "YES" in analysis_result:
                    logger.info("[SUPERVISOR] Context-based answer is possible - using context_answer")
                    # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ ì§ì ‘ ë¼ìš°íŒ…
                    return {
                        **state,
                        "messages": [AIMessage(content="ë§¥ë½ ê¸°ë°˜ ë‹µë³€")],
                        "ready_to_answer": False,  # context_answer_nodeë¡œ ë¼ìš°íŒ…
                        "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                        "context_based": True
                    }
            except Exception as e:
                logger.warning(f"Context analysis failed: {e}, proceeding with normal tools")
        
        # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ë„êµ¬ ì‚¬ìš©
        logger.info("[SUPERVISOR] Multi-turn: Using normal tools")
        from .tools import answer, rag_search, product_extraction, context_answer
        tool_functions = [answer, rag_search, product_extraction, context_answer]
    
    logger.info(f"Available tools: {[tool.name for tool in tool_functions]}")

    # ìŠˆí¼ë°”ì´ì € í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒˆë¡œìš´ 3ê°€ì§€ ë…¸ë“œ ì›Œí¬í”Œë¡œìš°ì— ë§ê²Œ)
    supervisor_prompt = create_supervisor_prompt(
        query=query + context_info  # ì´ì „ ëŒ€í™” ë§¥ë½ í¬í•¨
    )

    try:
        # LLMìœ¼ë¡œ íˆ´ ì„ íƒ
        result = slm.llm.bind_tools(tool_functions, tool_choice="required").invoke(supervisor_prompt)
        
        # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ stateì— ì €ì¥
        if hasattr(result, 'tool_calls') and result.tool_calls:
            tool_name = result.tool_calls[0]['name']
            tool_args = result.tool_calls[0]['args']
            
            logger.info(f"[SUPERVISOR] ========= ë„êµ¬ ì„ íƒ ì™„ë£Œ =========")
            logger.info(f"[SUPERVISOR] ì„ íƒëœ ë„êµ¬: {tool_name}")
            logger.info(f"[SUPERVISOR] ë„êµ¬ ì¸ì: {tool_args}")
            logger.info(f"[SUPERVISOR] ë‹¤ìŒ ë…¸ë“œ: {tool_name.upper()}_NODE")
            
            # ë„êµ¬ ì‹¤í–‰ - ìƒˆë¡œìš´ ì›Œí¬í”Œë¡œìš°ì— ë§ê²Œ ìˆ˜ì •
            if tool_name == "answer":
                logger.info("[SUPERVISOR] answer ë„êµ¬ ì‹¤í–‰ - FAQ ë‹µë³€ ìƒì„±")
                # ì§ì ‘ ë‹µë³€ ìƒì„±
                response = create_simple_response(slm, query, "faq_system")
                response = clean_markdown_formatting(response)
                return {
                    **state,
                    "messages": [result],
                    "response": response,
                    "ready_to_answer": True,
                    "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                }
            elif tool_name == "rag_search":
                logger.info("[SUPERVISOR] rag_search ë„êµ¬ ì‹¤í–‰ - RAG ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…")
                # RAG ê²€ìƒ‰ìœ¼ë¡œ ë¼ìš°íŒ…
                return {
                    **state,
                    "messages": [result],
                    "ready_to_answer": False,  # rag_search ë…¸ë“œë¡œ ê°€ì•¼ í•¨
                    "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                }
            elif tool_name == "product_extraction":
                logger.info("[SUPERVISOR] product_extraction ë„êµ¬ ì‹¤í–‰ - ìƒí’ˆ ì¶”ì¶œë¡œ ë¼ìš°íŒ…")
                # product_extraction ë…¸ë“œë¡œ ë¼ìš°íŒ…
                return {
                    **state,
                    "messages": [result],
                    "ready_to_answer": False,  # product_extraction ë…¸ë“œë¡œ ê°€ì•¼ í•¨
                    "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                }
            elif tool_name == "context_answer":
                logger.info("[SUPERVISOR] context_answer ë„êµ¬ ì‹¤í–‰ - ë§¥ë½ ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ ë¼ìš°íŒ…")
                # context_answer ë…¸ë“œë¡œ ë¼ìš°íŒ…
                return {
                    **state,
                    "messages": [result],
                    "ready_to_answer": False,  # context_answer ë…¸ë“œë¡œ ê°€ì•¼ í•¨
                    "n_tool_calling": state.get("n_tool_calling", 0) + 1,
                }
        
        return {
            **state,
            "messages": [result],
            "n_tool_calling": state.get("n_tool_calling", 0) + 1,
        }
        
    except Exception as e:
        logger.error(f"Supervisor failed: {e}")
        return {
            **state,
            "messages": [AIMessage(content="ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")],
            "ready_to_answer": True
        }

def supervisor_router(state: RAGState) -> str:
    """ìŠˆí¼ë°”ì´ì € ë¼ìš°í„°"""
    logger.info("[ROUTER] supervisor_router ì‹¤í–‰ ì‹œì‘")
    logger.info(f"[ROUTER] State keys: {list(state.keys())}")
    logger.info(f"[ROUTER] ready_to_answer: {state.get('ready_to_answer')}")
    
    # messagesì—ì„œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ í™•ì¸
    messages = state.get("messages", [])
    if messages:
        last_message = messages[-1]
        logger.info(f"[ROUTER] Last message type: {type(last_message)}")
        logger.info(f"[ROUTER] Last message content: {getattr(last_message, 'content', 'No content')}")
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            logger.info(f"[ROUTER] Tool calls: {[call['name'] for call in last_message.tool_calls]}")
    
    # RAG ê²€ìƒ‰ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ì¸ì§€ í™•ì¸
    if state.get("redirect_to_rag"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] redirect_to_rag=True - RAG_SEARCHë¡œ ë¼ìš°íŒ…")
        return "rag_search"
    
    # messagesì—ì„œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë„êµ¬ í˜¸ì¶œ í™•ì¸ (redirect_to_rag ì²˜ë¦¬ í›„)
    if messages:
        last_message = messages[-1]
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            tool_name = last_message.tool_calls[0]['name']
            if tool_name == "rag_search":
                logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
                logger.info("[ROUTER] ì„ íƒëœ ë„êµ¬: rag_search")
                logger.info("[ROUTER] ë‹¤ìŒ ë…¸ë“œ: RAG_SEARCH")
                return "rag_search"
    
    # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì¸ì§€ í™•ì¸
    if state.get("context_based"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] context_based=True - CONTEXT_ANSWERë¡œ ë¼ìš°íŒ…")
        return "context_answer"
    
    if state.get("ready_to_answer"):
        logger.info("[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
        logger.info("[ROUTER] ready_to_answer=True - ANSWERë¡œ ë¼ìš°íŒ…")
        return "answer"
    
    # ì²« ë²ˆì§¸ í„´ì¸ì§€ í™•ì¸
    conversation_history = state.get("conversation_history", [])
    is_first_turn = not conversation_history or len(conversation_history) == 0
    
    logger.info(f"[ROUTER] conversation_history: {conversation_history}")
    logger.info(f"[ROUTER] is_first_turn: {is_first_turn}")
    
    # messagesì—ì„œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ ë„êµ¬ í˜¸ì¶œ í™•ì¸ (rag_search ì œì™¸)
    if messages:
        last_message = messages[-1]
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            tool_name = last_message.tool_calls[0]['name']
            if tool_name != "rag_search":  # rag_searchëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬ë¨
                logger.info(f"[ROUTER] ========= ë¼ìš°íŒ… ê²°ì • =========")
                logger.info(f"[ROUTER] ì„ íƒëœ ë„êµ¬: {tool_name}")
                logger.info(f"[ROUTER] ë‹¤ìŒ ë…¸ë“œ: {tool_name.upper()}_NODE")
                return tool_name
    
    # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ RAG_SEARCHë¡œ
    logger.info("[ROUTER] No tool calls found - defaulting to RAG_SEARCH")
    return "rag_search"


def product_extraction_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ìƒí’ˆëª… ì¶”ì¶œ ë…¸ë“œ"""
    logger.info("ğŸ·ï¸ [NODE] product_extraction_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "product_extraction_node")
    
    query = state.get("query", "")
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    try:
        extracted_product = extract_product_name(slm, query)
        sub_category = classify_product_subcategory(extracted_product)
        logger.info(f"Extracted product: '{extracted_product}', Sub category: '{sub_category}'")
        
        return {
            **state,
            "product_name": extracted_product,
            "product_extraction_result": {
                "product_name": extracted_product,
                "sub_category": sub_category,
                "confidence": 0.9,
                "reasoning": f"ìƒí’ˆëª… '{extracted_product}'ì„ ì¶”ì¶œí•˜ê³  '{sub_category}'ë¡œ ë¶„ë¥˜"
            },
            "ready_to_answer": False  # ì•„ì§ ë‹µë³€ ì¤€ë¹„ë˜ì§€ ì•ŠìŒ
        }
        
    except Exception as e:
        logger.error(f"Product extraction failed: {e}")
        return {
            **state,
            "product_name": "ì¼ë°˜",
            "ready_to_answer": False
        }

def product_search_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ìƒí’ˆ ê²€ìƒ‰ ë…¸ë“œ"""
    logger.info("ğŸ” [NODE] product_search_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "product_search_node")
    
    query = state.get("query", "")
    product_name = state.get("product_name", "")
    
    # SLMê³¼ VectorStore ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    if slm is None:
        slm = get_shared_slm()
    vector_store = get_shared_vector_store()
    
    try:
        # ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        cached_docs = get_cached_search_result(query, product_name)
        if cached_docs is not None:
            logger.info(f"Using cached search results: {len(cached_docs)} documents")
            retrieved_docs = cached_docs
        else:
            vector_store.get_index_ready()
            
            if not product_name or product_name == "ì¼ë°˜":
                # ì¼ë°˜ ê²€ìƒ‰
                retrieved_docs = vector_store.similarity_search(query, k=DEFAULT_SEARCH_K)
                logger.info(f"General search found {len(retrieved_docs)} documents")
            else:
                # ìƒí’ˆëª…ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° í•„í„°ë§ (ìµœì í™”ëœ ë‹¨ì¼ ì‹œë„)
                filter_dict = {"keywords": {"$in": [product_name]}}
                
                try:
                    retrieved_docs = vector_store.similarity_search(query, k=DEFAULT_SEARCH_K, filter_dict=filter_dict)
                    if retrieved_docs:
                        logger.info(f"Found {len(retrieved_docs)} documents using filter: {filter_dict}")
                    else:
                        # í•„í„°ë§ëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                        retrieved_docs = vector_store.similarity_search(query, k=DEFAULT_SEARCH_K)
                        logger.info(f"No documents found for product '{product_name}', using general search")
                        logger.info(f"General search found {len(retrieved_docs)} documents")
                except Exception as e:
                    logger.warning(f"Filter failed: {filter_dict}, error: {e}")
                    # ì—ëŸ¬ ë°œìƒ ì‹œ ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±
                    retrieved_docs = vector_store.similarity_search(query, k=DEFAULT_SEARCH_K)
                    logger.info(f"Fallback to general search found {len(retrieved_docs)} documents")
            
            # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            set_cached_search_result(query, product_name, retrieved_docs)
        
        # PDF ì •ë³´ ë¡œê¹…
        if retrieved_docs:
            logger.info("ğŸ“„ [PRODUCT SEARCH] ì‚¬ìš©ëœ PDF ë¬¸ì„œ ì •ë³´:")
            for i, doc in enumerate(retrieved_docs):
                metadata = doc.metadata
                file_name = metadata.get('file_name', 'Unknown')
                page_number = metadata.get('page_number', 'Unknown')
                logger.info(f"  ğŸ“‹ ë¬¸ì„œ {i+1}: {file_name} (í˜ì´ì§€: {page_number})")
        
        # ê³µí†µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response, sources = create_rag_response(slm, query, retrieved_docs)
        
        return {
            **state,
            "response": response,
            "sources": sources,
            "retrieved_docs": retrieved_docs,
            "context_text": format_context(retrieved_docs) if retrieved_docs else "",
            "ready_to_answer": True
        }
        
    except Exception as e:
        logger.error(f"Product search failed: {e}")
        return {
            **state,
            **create_error_response("product_error")
        }

def session_summary_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ì„¸ì…˜ ìš”ì•½ ìƒì„± ë…¸ë“œ (ì²« ëŒ€í™”)"""
    import time
    start_time = time.time()
    logger.info("[SESSION_SUMMARY] Starting session title generation")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "session_summary_node")
    
    query = state.get("query", "")
    session_context = state.get("session_context")
    conversation_history = state.get("conversation_history", [])
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    # ì²« ëŒ€í™”ì¸ì§€ í™•ì¸
    conversation_history = state.get("conversation_history", [])
    is_first_turn = not conversation_history or len(conversation_history) == 0
    
    if is_first_turn:
        # ê³µí†µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì œëª© ìƒì„±
        session_title = generate_session_title(query, slm)
        logger.info(f"[SESSION_SUMMARY] Generated title: '{session_title}'")

        # ì„¸ì…˜ì— ì œëª© ì €ì¥
        session_manager.update_session(
            session_context.session_id,
            session_title=session_title
        )
        
        logger.info(f"Session title saved to session: {session_context.session_id}")
        
        # ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        updated_session_context = session_manager.get_session(session_context.session_id)
        if updated_session_context:
            session_context = updated_session_context
        
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"Returning state with session_title: '{session_title}'")
        logger.info(f"ğŸ“ [NODE] session_summary_node ì™„ë£Œ - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
        return {
            **state,
            "session_title": session_title,
            "ready_to_answer": False,  # RAG ê²€ìƒ‰ì„ ìœ„í•´ Falseë¡œ ì„¤ì •
            "session_context": session_context,  # ì—…ë°ì´íŠ¸ëœ ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ í¬í•¨
            "response": ""  # RAG ê²€ìƒ‰ì—ì„œ ì‹¤ì œ ì‘ë‹µ ìƒì„±
        }
    else:
        # ì²« ëŒ€í™”ê°€ ì•„ë‹ˆë©´ ê¸°ì¡´ ì œëª© ì‚¬ìš©í•˜ê³  RAG ê²€ìƒ‰ìœ¼ë¡œ ë„˜ì–´ê°
        existing_title = session_context.session_title if session_context else ""
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"ğŸ“ [NODE] session_summary_node ì™„ë£Œ (ê¸°ì¡´ ì œëª© ì‚¬ìš©) - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
        return {
            **state,
            "session_title": existing_title,
            "ready_to_answer": False,  # RAG ê²€ìƒ‰ì„ ìœ„í•´ Falseë¡œ ì„¤ì •
            "response": ""
        }
        

def rag_search_node(state: RAGState, slm: SLM = None, vector_store=None) -> RAGState:
    """RAG ê²€ìƒ‰ ë…¸ë“œ"""
    import time
    start_time = time.time()
    logger.info("[RAG_SEARCH] Starting document search")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "rag_search_node")
    
    query = state.get("query", "")
    
    # SLMê³¼ VectorStore ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì¬ì‚¬ìš© ê°€ëŠ¥)
    if slm is None:
        slm = get_shared_slm()
    if vector_store is None:
        vector_store = get_shared_vector_store()
        vector_store.get_index_ready()  # í•œ ë²ˆë§Œ ì´ˆê¸°í™”
    
    try:
        # ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        messages = state.get("messages", [])
        enhanced_query = query
        
        if messages and len(messages) > 1:
            # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê²€ìƒ‰ ì¿¼ë¦¬ì— í¬í•¨
            previous_context = ""
            for msg in messages[:-1][-2:]:  # ìµœê·¼ 2ê°œ ë©”ì‹œì§€ë§Œ ê³ ë ¤
                if hasattr(msg, 'content'):
                    previous_context += f" {msg.content[:50]}"
            
            if previous_context:
                enhanced_query = f"{query} {previous_context.strip()}"
                logger.info(f"[RAG_SEARCH] Enhanced query with context: {enhanced_query[:100]}...")
        
        # ë¬¸ì„œ ê²€ìƒ‰ (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ê²°ê³¼ ìˆ˜ ì œí•œ)
        retrieved_docs = vector_store.similarity_search(enhanced_query, k=3)# ë” ì ì€ ê²°ê³¼ë¡œ ì†ë„ í–¥ìƒ
        logger.info(f"[RAG_SEARCH] Found {len(retrieved_docs)} documents")
        
        # ê³µí†µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response, sources = create_rag_response(slm, query, retrieved_docs)
        
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"ğŸ“š [NODE] rag_search_node ì™„ë£Œ - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
        
        return {
            **state,
            "response": response,
            "sources": sources,
            "retrieved_docs": retrieved_docs,
            "context_text": format_context(retrieved_docs) if retrieved_docs else "",
            "ready_to_answer": True
        }
        
    except Exception as e:
        logger.error(f"RAG search failed: {e}")
        return {
            **state,
            **create_error_response("search_error")
        }

def guardrail_check_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ê°€ë“œë ˆì¼ ê²€ì‚¬ ë…¸ë“œ"""
    logger.info("ğŸ›¡ï¸ [NODE] guardrail_check_node ì‹¤í–‰ ì‹œì‘")
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "guardrail_check_node")
    
    response = state.get("response", "")
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if slm is None:
        slm = get_shared_slm()
    
    try:
        # ê³µí†µ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ë“œë ˆì¼ ê²€ì‚¬
        logger.info("ğŸ›¡ï¸ [GUARDRAIL] Starting guardrail check...")
        compliant_response, violations = create_guardrail_response(slm, response)
        logger.info("ğŸ›¡ï¸ [GUARDRAIL] Guardrail check completed")
        
        # ì‹¤í–‰ ê²½ë¡œ ë¡œê¹…
        execution_path = state.get("execution_path", [])
        path_str = " -> ".join(execution_path)
        logger.info(f"ğŸ›¡ï¸ [WORKFLOW] Execution path: {path_str}")
        
        result = {
            **state,
            "guardrail_decision": "COMPLIANT" if not violations else "VIOLATION",
            "violations": violations,
            "compliant_response": compliant_response,
            "response": compliant_response,
            "ready_to_answer": True
        }
        
        logger.info("ğŸ›¡ï¸ [NODE] guardrail_check_node ì™„ë£Œ")
        logger.info(f"ğŸ›¡ï¸ [NODE] ready_to_answer: {result.get('ready_to_answer')}")
        logger.info(f"ğŸ›¡ï¸ [NODE] response length: {len(result.get('response', ''))}")
        return result
        
    except Exception as e:
        logger.error(f"Guardrail check failed: {e}")
        result = {
            **state,
            "guardrail_decision": "ERROR",
            "violations": ["ê°€ë“œë ˆì¼ ê²€ì‚¬ ì˜¤ë¥˜"],
            "compliant_response": get_error_message("guardrail_error"),
            "response": get_error_message("guardrail_error"),
            "ready_to_answer": True
        }
        logger.info("ğŸ›¡ï¸ [NODE] guardrail_check_node ì™„ë£Œ (ì—ëŸ¬)")
        return result

def context_answer_node(state: RAGState, slm: SLM = None) -> RAGState:
    """ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ë…¸ë“œ - ì´ì „ ëŒ€í™” ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€"""
    import time
    start_time = time.time()
    logger.info("[CONTEXT_ANSWER] Starting context-based answer generation")
    
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "context_answer_node")
    
    query = state.get("query", "")
    messages = state.get("messages", [])
    
    # SLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    if slm is None:
        slm = get_shared_slm()
    
    try:
        # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±
        previous_context = ""
        for msg in messages[:-1]:  # í˜„ì¬ ì§ˆë¬¸ ì œì™¸
            if hasattr(msg, 'content'):
                role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                previous_context += f"{role}: {msg.content}\n"
        
        # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        context_prompt = f"""
        ì´ì „ ëŒ€í™” ë‚´ìš©:
        {previous_context}

        í˜„ì¬ ì§ˆë¬¸: {query}

        ìœ„ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
        ì´ì „ ëŒ€í™”ì—ì„œ ì´ë¯¸ ì œê³µëœ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ìƒˆë¡œìš´ ê²€ìƒ‰ì´ë‚˜ ë¬¸ì„œ ì°¸ì¡° ì—†ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ë§Œìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œ ê²½ìš°ì—ë§Œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        
        # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        response = slm.llm.invoke(context_prompt).content
        response = clean_markdown_formatting(response)
        
        # ë‹µë³€ì´ ì¼ë°˜ì ì´ê±°ë‚˜ êµ¬ì²´ì ì¸ ì •ë³´ê°€ ë¶€ì¡±í•œì§€ í™•ì¸
        insufficient_info_keywords = [
            "ì¼ë°˜ì ìœ¼ë¡œ", "êµ¬ì²´ì ì¸ ì¡°ê±´ì€", "ìì„¸í•œ ì‚¬í•­ì€", "ë¬¸ì˜í•˜ì‹œê±°ë‚˜", 
            "í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤", "ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ", "í•´ë‹¹ ê¸ˆìœµê¸°ê´€ì—"
        ]
        
        needs_rag_search = any(keyword in response for keyword in insufficient_info_keywords)
        
        if needs_rag_search:
            logger.info("[CONTEXT_ANSWER] Insufficient information - redirecting to RAG search")
            # ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ìƒí’ˆëª… ì¶”ì¶œ
            product_name = ""
            for msg in messages[:-1]:
                if hasattr(msg, 'content') and msg.__class__.__name__ == "AIMessage":
                    content = msg.content
                    # ìƒí’ˆëª… ì¶”ì¶œ ë¡œì§ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
                    if "ì‚¬ë¦½í•™êµêµì§ì›ìš°ëŒ€ëŒ€ì¶œ" in content:
                        product_name = "ì‚¬ë¦½í•™êµêµì§ì›ìš°ëŒ€ëŒ€ì¶œ"
                        break
                    elif "ë²„íŒ€ëª© ì „ì„¸ìê¸ˆëŒ€ì¶œ" in content:
                        product_name = "ë²„íŒ€ëª© ì „ì„¸ìê¸ˆëŒ€ì¶œ"
                        break
                    elif "í–‡ì‚´ë¡ " in content:
                        product_name = "í–‡ì‚´ë¡ "
                        break
            
            if product_name:
                logger.info(f"[CONTEXT_ANSWER] Redirecting to RAG search for product: {product_name}")
                # RAG ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±
                enhanced_query = f"{query} {product_name}"
                return {
                    **state,
                    "query": enhanced_query,
                    "messages": [AIMessage(content="RAG ê²€ìƒ‰", tool_calls=[{"name": "rag_search", "args": {"query": enhanced_query}, "id": f"call_{int(time.time())}_{hash(enhanced_query) % 10000}"}])],
                    "ready_to_answer": False,  # RAG ê²€ìƒ‰ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
                    "context_based": False,
                    "redirect_to_rag": True
                }
            else:
                # ìƒí’ˆëª…ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ì¼ë°˜ RAG ê²€ìƒ‰
                logger.info("[CONTEXT_ANSWER] No product name found - using general RAG search")
                return {
                    **state,
                    "messages": [AIMessage(content="RAG ê²€ìƒ‰", tool_calls=[{"name": "rag_search", "args": {"query": query}, "id": f"call_{int(time.time())}_{hash(query) % 10000}"}])],
                    "ready_to_answer": False,
                    "context_based": False,
                    "redirect_to_rag": True
                }
        
        end_time = time.time()
        execution_time = end_time - start_time
        logger.info(f"[CONTEXT_ANSWER] Context-based answer generated - execution time: {execution_time:.2f}s")
        
        return {
            **state,
            "response": response,
            "sources": [],  # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì´ë¯€ë¡œ ì†ŒìŠ¤ ì—†ìŒ
            "ready_to_answer": True,
            "context_based": True  # ë§¥ë½ ê¸°ë°˜ ë‹µë³€ì„ì„ í‘œì‹œ
        }
        
    except Exception as e:
        logger.error(f"Context answer generation failed: {e}")
        return {
            **state,
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "sources": [],
            "ready_to_answer": True,
            "context_based": False
        }

def answer_node(state: RAGState) -> RAGState:
    """ìµœì¢… ë‹µë³€ ë…¸ë“œ"""
    import time
    from datetime import datetime
    start_time = time.time()
    # ë¡œê¹… ìµœì†Œí™” (ì„±ëŠ¥ ê°œì„ )
    logger.debug("ğŸ“ [NODE] ANSWER_NODE ì§„ì…")
    
    # ì‹¤í–‰ ê²½ë¡œ ì¶”ì 
    state = track_execution_path(state, "answer_node")
    
    response = state.get("response", "")
    logger.info(f"ğŸ“ [ANSWER] í˜„ì¬ ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
    
    # ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ì‘ë‹µ ê°œì„ 
    messages = state.get("messages", [])
    if messages and len(messages) > 1:
        # ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‘ë‹µ ê°œì„ 
        previous_context = ""
        for msg in messages[:-1][-2:]:  # ìµœê·¼ 2ê°œ ë©”ì‹œì§€ë§Œ ê³ ë ¤
            if hasattr(msg, 'content'):
                role = "ì‚¬ìš©ì" if msg.__class__.__name__ == "HumanMessage" else "AI"
                previous_context += f"{role}: {msg.content[:100]}...\n"
        
        if previous_context:
            logger.info(f"[ANSWER] Considering previous context: {previous_context[:200]}...")
            # ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ì‘ë‹µ ê°œì„  ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
    
    if not response or not response.strip():
        # ê¸°ë³¸ ì‘ë‹µ ìƒì„± (RAG ê²€ìƒ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ)
        response = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜ ê´€ë ¨ ë¶€ì„œì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."
        logger.info("[ANSWER] Using default response")
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ì œê±°
    response = clean_markdown_formatting(response)
    
    # Djangoì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ LangGraphì—ì„œëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
    logger.info("ğŸ“ [NODE] ëŒ€í™” í„´ ì €ì¥ì€ Djangoì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤")
    
    end_time = time.time()
    execution_time = end_time - start_time
    logger.info(f"âœ… [NODE] answer_node ì™„ë£Œ - ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
    
    return {
        **state,
        "response": response,
        "ready_to_answer": True
    }

