# chat_cli.py

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import os
from dotenv import load_dotenv

# --- 1. í™˜ê²½ ì„¤ì • ë° .env íŒŒì¼ ë¡œë“œ ---
# load_dotenv() í•¨ìˆ˜ê°€ .env íŒŒì¼ì„ ì°¾ì•„ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ìë™ìœ¼ë¡œ ë¡œë“œí•´ì¤€ë‹¤.
load_dotenv()

# ğŸ’¡ .env ë˜ëŠ” exportë¡œ ì„¤ì •ëœ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.
#    get() ë©”ì†Œë“œì˜ ë‘ ë²ˆì§¸ ì¸ì(ê¸°ë³¸ê°’)ëŠ” ì´ì œ ì‚­ì œí•´ë„ ì•ˆì „í•˜ë‹¤.
HF_TOKEN = os.environ.get("HF_TOKEN")
HF_USERNAME = os.environ.get("HF_USERNAME")
assert HF_USERNAME, "í™˜ê²½ë³€ìˆ˜ HF_USERNAMEì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤ (.env í™•ì¸)."
MODEL_ID = f"{HF_USERNAME}/Qwen2.5-14B-KB-Finance-LoRA-v2-Merged"

# --- 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---
def load_model():
    """
    íŒŒì¸íŠœë‹ë˜ê³  ë³‘í•©ëœ ìµœì¢… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜.
    RunPodì˜ GPU í™˜ê²½ì„ ê°€ì •í•œë‹¤.
    """
    print(f"'{MODEL_ID}' ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

    # ì–‘ìí™” ì—†ì´, bfloat16ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID, torch_dtype=torch.bfloat16, device_map="auto", token=HF_TOKEN, trust_remote_code=True
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN, trust_remote_code=True)


    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    print("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!")
    return model, tokenizer

# --- 3. ë©”ì¸ ëŒ€í™” ë£¨í”„ ---
def main():
    model, tokenizer = load_model()

    print("\n=============================================")
    print("  KB ê¸ˆìœµ ì±—ë´‡ í„°ë¯¸ë„ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    print("  (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ë˜ëŠ” 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”)")
    print("=============================================")

    while True:
        # 1. ì‚¬ìš©ìë¡œë¶€í„° ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
        user_input = input("\nğŸ‘¤ ë‹¹ì‹ : ")
        if user_input.lower() in ["exit", "quit"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 2. Qwen2 í…œí”Œë¦¿ì— ë§ì¶° í”„ë¡¬í”„íŠ¸ ìƒì„±
        messages = [
            {"role": "system", "content": "KB ê¸ˆìœµ ë„ë©”ì¸ ë³´ì¡°ì—­. ì¶”ì¸¡ ê¸ˆì§€, ê·¼ê±° ìš°ì„ . ë°˜ë§ë¡œ ê°„ê²°í•˜ê²Œ."},
            {"role": "user", "content": user_input},
            ]
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # 3. ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True).to(model.device)

        # 4. ë‹µë³€ ìƒì„±
        # GenerationConfigë¥¼ ì‚¬ìš©í•´ ìƒì„± íŒŒë¼ë¯¸í„°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬
        generation_config = GenerationConfig(
            max_new_tokens=1024,
            temperature=0.7,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
        )

        print("\nğŸ¤– KB ì±—ë´‡ì´ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
        outputs = model.generate(**inputs, generation_config=generation_config)

        # 5. ê²°ê³¼ ë””ì½”ë”© ë° ì¶œë ¥
        response_text = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # í”„ë¡¬í”„íŠ¸ë¥¼ ì œì™¸í•œ ìˆœìˆ˜ ë‹µë³€ë§Œ ì¶”ì¶œ
        try:
            assistant_response = response_text.split("<|im_start|>assistant", 1)[-1]
            assistant_response = assistant_response.split("<|im_end|>", 1)[0].strip()
            print(f"\nğŸ¤– KB ì±—ë´‡: {assistant_response}")
        except:
            print(f"\nâ—ï¸ ë‹µë³€ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ì¶œë ¥:\n{response_text}")


# --- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    # Hugging Face í† í°ì´ ìˆëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸
    if not HF_TOKEN:
        print("ê²½ê³ : .env íŒŒì¼ì´ë‚˜ í™˜ê²½ ë³€ìˆ˜ì— HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•  ìˆ˜ë„ ìˆë‹¤.
        # exit()
    main()